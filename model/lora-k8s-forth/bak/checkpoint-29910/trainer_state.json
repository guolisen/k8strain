{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 9.97,
  "eval_steps": 500,
  "global_step": 29910,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.016666666666666666,
      "grad_norm": 6.034632205963135,
      "learning_rate": 1.1111111111111112e-05,
      "loss": 3.1697,
      "step": 50
    },
    {
      "epoch": 0.03333333333333333,
      "grad_norm": 5.154573440551758,
      "learning_rate": 2.2222222222222223e-05,
      "loss": 2.4867,
      "step": 100
    },
    {
      "epoch": 0.05,
      "grad_norm": 5.440109729766846,
      "learning_rate": 3.3333333333333335e-05,
      "loss": 1.8121,
      "step": 150
    },
    {
      "epoch": 0.06666666666666667,
      "grad_norm": 3.160090684890747,
      "learning_rate": 4.4444444444444447e-05,
      "loss": 1.559,
      "step": 200
    },
    {
      "epoch": 0.08333333333333333,
      "grad_norm": 1.6614370346069336,
      "learning_rate": 5.555555555555556e-05,
      "loss": 1.4866,
      "step": 250
    },
    {
      "epoch": 0.1,
      "grad_norm": 1.5564707517623901,
      "learning_rate": 6.666666666666667e-05,
      "loss": 1.4346,
      "step": 300
    },
    {
      "epoch": 0.11666666666666667,
      "grad_norm": 0.9920642375946045,
      "learning_rate": 7.777777777777778e-05,
      "loss": 1.4717,
      "step": 350
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 1.3894377946853638,
      "learning_rate": 8.888888888888889e-05,
      "loss": 1.4784,
      "step": 400
    },
    {
      "epoch": 0.15,
      "grad_norm": 1.1750980615615845,
      "learning_rate": 0.0001,
      "loss": 1.4513,
      "step": 450
    },
    {
      "epoch": 0.16666666666666666,
      "grad_norm": 0.9808857440948486,
      "learning_rate": 0.00011111111111111112,
      "loss": 1.4207,
      "step": 500
    },
    {
      "epoch": 0.18333333333333332,
      "grad_norm": 0.8464698791503906,
      "learning_rate": 0.00012222222222222224,
      "loss": 1.378,
      "step": 550
    },
    {
      "epoch": 0.2,
      "grad_norm": 1.037108063697815,
      "learning_rate": 0.00013333333333333334,
      "loss": 1.3967,
      "step": 600
    },
    {
      "epoch": 0.21666666666666667,
      "grad_norm": 2.0790984630584717,
      "learning_rate": 0.00014444444444444444,
      "loss": 1.4676,
      "step": 650
    },
    {
      "epoch": 0.23333333333333334,
      "grad_norm": 0.8947954177856445,
      "learning_rate": 0.00015555555555555556,
      "loss": 1.3677,
      "step": 700
    },
    {
      "epoch": 0.25,
      "grad_norm": 1.4205303192138672,
      "learning_rate": 0.0001666666666666667,
      "loss": 1.358,
      "step": 750
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 1.0384674072265625,
      "learning_rate": 0.00017777777777777779,
      "loss": 1.3011,
      "step": 800
    },
    {
      "epoch": 0.2833333333333333,
      "grad_norm": 0.95660400390625,
      "learning_rate": 0.00018888888888888888,
      "loss": 1.3006,
      "step": 850
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.7582662105560303,
      "learning_rate": 0.0002,
      "loss": 1.355,
      "step": 900
    },
    {
      "epoch": 0.31666666666666665,
      "grad_norm": 0.8865571618080139,
      "learning_rate": 0.00019965635738831617,
      "loss": 1.2882,
      "step": 950
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 0.8626538515090942,
      "learning_rate": 0.00019931271477663232,
      "loss": 1.4451,
      "step": 1000
    },
    {
      "epoch": 0.35,
      "grad_norm": 0.7808882594108582,
      "learning_rate": 0.00019896907216494845,
      "loss": 1.389,
      "step": 1050
    },
    {
      "epoch": 0.36666666666666664,
      "grad_norm": 0.7904962301254272,
      "learning_rate": 0.0001986254295532646,
      "loss": 1.4207,
      "step": 1100
    },
    {
      "epoch": 0.38333333333333336,
      "grad_norm": 0.7824050188064575,
      "learning_rate": 0.00019828178694158077,
      "loss": 1.298,
      "step": 1150
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.9412658214569092,
      "learning_rate": 0.00019793814432989693,
      "loss": 1.3927,
      "step": 1200
    },
    {
      "epoch": 0.4166666666666667,
      "grad_norm": 1.057042121887207,
      "learning_rate": 0.00019759450171821308,
      "loss": 1.3778,
      "step": 1250
    },
    {
      "epoch": 0.43333333333333335,
      "grad_norm": 0.844883143901825,
      "learning_rate": 0.00019725085910652924,
      "loss": 1.4342,
      "step": 1300
    },
    {
      "epoch": 0.45,
      "grad_norm": 0.7048695683479309,
      "learning_rate": 0.00019690721649484537,
      "loss": 1.2618,
      "step": 1350
    },
    {
      "epoch": 0.4666666666666667,
      "grad_norm": 0.7119460701942444,
      "learning_rate": 0.0001965635738831615,
      "loss": 1.3423,
      "step": 1400
    },
    {
      "epoch": 0.48333333333333334,
      "grad_norm": 0.7435970902442932,
      "learning_rate": 0.00019621993127147766,
      "loss": 1.28,
      "step": 1450
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.5936318039894104,
      "learning_rate": 0.00019587628865979381,
      "loss": 1.3318,
      "step": 1500
    },
    {
      "epoch": 0.5166666666666667,
      "grad_norm": 0.6121538877487183,
      "learning_rate": 0.00019553264604810997,
      "loss": 1.3048,
      "step": 1550
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 0.8717366456985474,
      "learning_rate": 0.00019518900343642613,
      "loss": 1.298,
      "step": 1600
    },
    {
      "epoch": 0.55,
      "grad_norm": 0.6505681872367859,
      "learning_rate": 0.00019484536082474229,
      "loss": 1.2105,
      "step": 1650
    },
    {
      "epoch": 0.5666666666666667,
      "grad_norm": 0.6116631031036377,
      "learning_rate": 0.00019450171821305842,
      "loss": 1.3287,
      "step": 1700
    },
    {
      "epoch": 0.5833333333333334,
      "grad_norm": 0.6175141930580139,
      "learning_rate": 0.00019415807560137457,
      "loss": 1.3058,
      "step": 1750
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.5305272936820984,
      "learning_rate": 0.00019381443298969073,
      "loss": 1.2886,
      "step": 1800
    },
    {
      "epoch": 0.6166666666666667,
      "grad_norm": 0.8329936265945435,
      "learning_rate": 0.0001934707903780069,
      "loss": 1.3513,
      "step": 1850
    },
    {
      "epoch": 0.6333333333333333,
      "grad_norm": 0.7213583588600159,
      "learning_rate": 0.00019312714776632305,
      "loss": 1.2915,
      "step": 1900
    },
    {
      "epoch": 0.65,
      "grad_norm": 0.5964334011077881,
      "learning_rate": 0.00019278350515463918,
      "loss": 1.3319,
      "step": 1950
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 0.6902828812599182,
      "learning_rate": 0.00019243986254295533,
      "loss": 1.2694,
      "step": 2000
    },
    {
      "epoch": 0.6833333333333333,
      "grad_norm": 0.8016732931137085,
      "learning_rate": 0.0001920962199312715,
      "loss": 1.3162,
      "step": 2050
    },
    {
      "epoch": 0.7,
      "grad_norm": 0.6699756383895874,
      "learning_rate": 0.00019175257731958765,
      "loss": 1.3405,
      "step": 2100
    },
    {
      "epoch": 0.7166666666666667,
      "grad_norm": 0.5580851435661316,
      "learning_rate": 0.0001914089347079038,
      "loss": 1.3773,
      "step": 2150
    },
    {
      "epoch": 0.7333333333333333,
      "grad_norm": 0.6817197799682617,
      "learning_rate": 0.00019106529209621996,
      "loss": 1.3333,
      "step": 2200
    },
    {
      "epoch": 0.75,
      "grad_norm": 1.0993871688842773,
      "learning_rate": 0.0001907216494845361,
      "loss": 1.3621,
      "step": 2250
    },
    {
      "epoch": 0.7666666666666667,
      "grad_norm": 0.6982540488243103,
      "learning_rate": 0.00019037800687285222,
      "loss": 1.3745,
      "step": 2300
    },
    {
      "epoch": 0.7833333333333333,
      "grad_norm": 0.6761738657951355,
      "learning_rate": 0.00019003436426116838,
      "loss": 1.4542,
      "step": 2350
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.7217954397201538,
      "learning_rate": 0.00018969072164948454,
      "loss": 1.327,
      "step": 2400
    },
    {
      "epoch": 0.8166666666666667,
      "grad_norm": 0.5575506091117859,
      "learning_rate": 0.0001893470790378007,
      "loss": 1.3684,
      "step": 2450
    },
    {
      "epoch": 0.8333333333333334,
      "grad_norm": 1.0270781517028809,
      "learning_rate": 0.00018900343642611685,
      "loss": 1.2819,
      "step": 2500
    },
    {
      "epoch": 0.85,
      "grad_norm": 0.9721574783325195,
      "learning_rate": 0.000188659793814433,
      "loss": 1.3575,
      "step": 2550
    },
    {
      "epoch": 0.8666666666666667,
      "grad_norm": 0.9285989999771118,
      "learning_rate": 0.00018831615120274914,
      "loss": 1.4325,
      "step": 2600
    },
    {
      "epoch": 0.8833333333333333,
      "grad_norm": 1.7726469039916992,
      "learning_rate": 0.0001879725085910653,
      "loss": 1.2731,
      "step": 2650
    },
    {
      "epoch": 0.9,
      "grad_norm": 1.179112195968628,
      "learning_rate": 0.00018762886597938145,
      "loss": 1.3664,
      "step": 2700
    },
    {
      "epoch": 0.9166666666666666,
      "grad_norm": 0.5892927646636963,
      "learning_rate": 0.0001872852233676976,
      "loss": 1.373,
      "step": 2750
    },
    {
      "epoch": 0.9333333333333333,
      "grad_norm": 0.7930667400360107,
      "learning_rate": 0.00018694158075601377,
      "loss": 1.4165,
      "step": 2800
    },
    {
      "epoch": 0.95,
      "grad_norm": 0.934227466583252,
      "learning_rate": 0.0001865979381443299,
      "loss": 1.2346,
      "step": 2850
    },
    {
      "epoch": 0.9666666666666667,
      "grad_norm": 0.8188766837120056,
      "learning_rate": 0.00018625429553264605,
      "loss": 1.3603,
      "step": 2900
    },
    {
      "epoch": 0.9833333333333333,
      "grad_norm": 0.7878722548484802,
      "learning_rate": 0.0001859106529209622,
      "loss": 1.3633,
      "step": 2950
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.5443913340568542,
      "learning_rate": 0.00018556701030927837,
      "loss": 1.1966,
      "step": 3000
    },
    {
      "epoch": 1.0166666666666666,
      "grad_norm": 0.7565566897392273,
      "learning_rate": 0.00018522336769759452,
      "loss": 1.2913,
      "step": 3050
    },
    {
      "epoch": 1.0333333333333334,
      "grad_norm": 0.6120283007621765,
      "learning_rate": 0.00018487972508591068,
      "loss": 1.2804,
      "step": 3100
    },
    {
      "epoch": 1.05,
      "grad_norm": 0.741524875164032,
      "learning_rate": 0.0001845360824742268,
      "loss": 1.2603,
      "step": 3150
    },
    {
      "epoch": 1.0666666666666667,
      "grad_norm": 0.84932941198349,
      "learning_rate": 0.00018419243986254294,
      "loss": 1.183,
      "step": 3200
    },
    {
      "epoch": 1.0833333333333333,
      "grad_norm": 0.6580337285995483,
      "learning_rate": 0.0001838487972508591,
      "loss": 1.3285,
      "step": 3250
    },
    {
      "epoch": 1.1,
      "grad_norm": 0.5612415671348572,
      "learning_rate": 0.00018350515463917526,
      "loss": 1.2321,
      "step": 3300
    },
    {
      "epoch": 1.1166666666666667,
      "grad_norm": 0.5239695310592651,
      "learning_rate": 0.00018316151202749141,
      "loss": 1.3072,
      "step": 3350
    },
    {
      "epoch": 1.1333333333333333,
      "grad_norm": 0.5599063038825989,
      "learning_rate": 0.00018281786941580757,
      "loss": 1.2403,
      "step": 3400
    },
    {
      "epoch": 1.15,
      "grad_norm": 0.5131039023399353,
      "learning_rate": 0.00018247422680412373,
      "loss": 1.2755,
      "step": 3450
    },
    {
      "epoch": 1.1666666666666667,
      "grad_norm": 0.6144182085990906,
      "learning_rate": 0.00018213058419243986,
      "loss": 1.372,
      "step": 3500
    },
    {
      "epoch": 1.1833333333333333,
      "grad_norm": 0.5329729914665222,
      "learning_rate": 0.00018178694158075602,
      "loss": 1.2667,
      "step": 3550
    },
    {
      "epoch": 1.2,
      "grad_norm": 0.579426646232605,
      "learning_rate": 0.00018144329896907217,
      "loss": 1.2665,
      "step": 3600
    },
    {
      "epoch": 1.2166666666666668,
      "grad_norm": 0.5890432596206665,
      "learning_rate": 0.00018109965635738833,
      "loss": 1.3523,
      "step": 3650
    },
    {
      "epoch": 1.2333333333333334,
      "grad_norm": 1.018305778503418,
      "learning_rate": 0.0001807560137457045,
      "loss": 1.35,
      "step": 3700
    },
    {
      "epoch": 1.25,
      "grad_norm": 0.4090264141559601,
      "learning_rate": 0.00018041237113402062,
      "loss": 1.2904,
      "step": 3750
    },
    {
      "epoch": 1.2666666666666666,
      "grad_norm": 0.4827949106693268,
      "learning_rate": 0.00018006872852233677,
      "loss": 1.222,
      "step": 3800
    },
    {
      "epoch": 1.2833333333333332,
      "grad_norm": 0.83797287940979,
      "learning_rate": 0.00017972508591065293,
      "loss": 1.39,
      "step": 3850
    },
    {
      "epoch": 1.3,
      "grad_norm": 0.9143689870834351,
      "learning_rate": 0.0001793814432989691,
      "loss": 1.3034,
      "step": 3900
    },
    {
      "epoch": 1.3166666666666667,
      "grad_norm": 0.6203644871711731,
      "learning_rate": 0.00017903780068728525,
      "loss": 1.3188,
      "step": 3950
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 0.678196370601654,
      "learning_rate": 0.0001786941580756014,
      "loss": 1.3228,
      "step": 4000
    },
    {
      "epoch": 1.35,
      "grad_norm": 1.2113006114959717,
      "learning_rate": 0.00017835051546391753,
      "loss": 1.3581,
      "step": 4050
    },
    {
      "epoch": 1.3666666666666667,
      "grad_norm": 0.6509129405021667,
      "learning_rate": 0.00017800687285223366,
      "loss": 1.1598,
      "step": 4100
    },
    {
      "epoch": 1.3833333333333333,
      "grad_norm": 0.6990793347358704,
      "learning_rate": 0.00017766323024054982,
      "loss": 1.2574,
      "step": 4150
    },
    {
      "epoch": 1.4,
      "grad_norm": 0.7037894129753113,
      "learning_rate": 0.00017731958762886598,
      "loss": 1.2758,
      "step": 4200
    },
    {
      "epoch": 1.4166666666666667,
      "grad_norm": 0.88969486951828,
      "learning_rate": 0.00017697594501718214,
      "loss": 1.3672,
      "step": 4250
    },
    {
      "epoch": 1.4333333333333333,
      "grad_norm": 1.2827427387237549,
      "learning_rate": 0.0001766323024054983,
      "loss": 1.3033,
      "step": 4300
    },
    {
      "epoch": 1.45,
      "grad_norm": 0.8234273195266724,
      "learning_rate": 0.00017628865979381445,
      "loss": 1.268,
      "step": 4350
    },
    {
      "epoch": 1.4666666666666668,
      "grad_norm": 0.6469606161117554,
      "learning_rate": 0.00017594501718213058,
      "loss": 1.3853,
      "step": 4400
    },
    {
      "epoch": 1.4833333333333334,
      "grad_norm": 0.5849502086639404,
      "learning_rate": 0.00017560137457044674,
      "loss": 1.1961,
      "step": 4450
    },
    {
      "epoch": 1.5,
      "grad_norm": 0.5518989562988281,
      "learning_rate": 0.0001752577319587629,
      "loss": 1.1747,
      "step": 4500
    },
    {
      "epoch": 1.5166666666666666,
      "grad_norm": 0.5460448265075684,
      "learning_rate": 0.00017491408934707905,
      "loss": 1.2696,
      "step": 4550
    },
    {
      "epoch": 1.5333333333333332,
      "grad_norm": 0.675975501537323,
      "learning_rate": 0.0001745704467353952,
      "loss": 1.2349,
      "step": 4600
    },
    {
      "epoch": 1.55,
      "grad_norm": 0.7094246745109558,
      "learning_rate": 0.00017422680412371134,
      "loss": 1.1951,
      "step": 4650
    },
    {
      "epoch": 1.5666666666666667,
      "grad_norm": 0.6908568143844604,
      "learning_rate": 0.0001738831615120275,
      "loss": 1.2922,
      "step": 4700
    },
    {
      "epoch": 1.5833333333333335,
      "grad_norm": 0.5452812910079956,
      "learning_rate": 0.00017353951890034365,
      "loss": 1.3634,
      "step": 4750
    },
    {
      "epoch": 1.6,
      "grad_norm": 0.6734644174575806,
      "learning_rate": 0.0001731958762886598,
      "loss": 1.3298,
      "step": 4800
    },
    {
      "epoch": 1.6166666666666667,
      "grad_norm": 0.9414399266242981,
      "learning_rate": 0.00017285223367697597,
      "loss": 1.2772,
      "step": 4850
    },
    {
      "epoch": 1.6333333333333333,
      "grad_norm": 0.6197199821472168,
      "learning_rate": 0.00017250859106529212,
      "loss": 1.2342,
      "step": 4900
    },
    {
      "epoch": 1.65,
      "grad_norm": 0.5116356611251831,
      "learning_rate": 0.00017216494845360825,
      "loss": 1.22,
      "step": 4950
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 0.6782267689704895,
      "learning_rate": 0.00017182130584192438,
      "loss": 1.3039,
      "step": 5000
    },
    {
      "epoch": 1.6833333333333333,
      "grad_norm": 0.7473208904266357,
      "learning_rate": 0.00017147766323024054,
      "loss": 1.2559,
      "step": 5050
    },
    {
      "epoch": 1.7,
      "grad_norm": 0.6109886765480042,
      "learning_rate": 0.0001711340206185567,
      "loss": 1.2541,
      "step": 5100
    },
    {
      "epoch": 1.7166666666666668,
      "grad_norm": 0.585942804813385,
      "learning_rate": 0.00017079037800687286,
      "loss": 1.2947,
      "step": 5150
    },
    {
      "epoch": 1.7333333333333334,
      "grad_norm": 1.1237646341323853,
      "learning_rate": 0.000170446735395189,
      "loss": 1.3613,
      "step": 5200
    },
    {
      "epoch": 1.75,
      "grad_norm": 0.6035114526748657,
      "learning_rate": 0.00017010309278350517,
      "loss": 1.3296,
      "step": 5250
    },
    {
      "epoch": 1.7666666666666666,
      "grad_norm": 0.46353572607040405,
      "learning_rate": 0.0001697594501718213,
      "loss": 1.3492,
      "step": 5300
    },
    {
      "epoch": 1.7833333333333332,
      "grad_norm": 0.46474528312683105,
      "learning_rate": 0.00016941580756013746,
      "loss": 1.223,
      "step": 5350
    },
    {
      "epoch": 1.8,
      "grad_norm": 0.5225105285644531,
      "learning_rate": 0.00016907216494845361,
      "loss": 1.2731,
      "step": 5400
    },
    {
      "epoch": 1.8166666666666667,
      "grad_norm": 0.44771745800971985,
      "learning_rate": 0.00016872852233676977,
      "loss": 1.1744,
      "step": 5450
    },
    {
      "epoch": 1.8333333333333335,
      "grad_norm": 0.4808097779750824,
      "learning_rate": 0.00016838487972508593,
      "loss": 1.219,
      "step": 5500
    },
    {
      "epoch": 1.85,
      "grad_norm": 0.6982908844947815,
      "learning_rate": 0.00016804123711340206,
      "loss": 1.2854,
      "step": 5550
    },
    {
      "epoch": 1.8666666666666667,
      "grad_norm": 0.541642963886261,
      "learning_rate": 0.00016769759450171822,
      "loss": 1.2336,
      "step": 5600
    },
    {
      "epoch": 1.8833333333333333,
      "grad_norm": 0.7528989911079407,
      "learning_rate": 0.00016735395189003437,
      "loss": 1.3727,
      "step": 5650
    },
    {
      "epoch": 1.9,
      "grad_norm": 0.5674157738685608,
      "learning_rate": 0.00016701030927835053,
      "loss": 1.3344,
      "step": 5700
    },
    {
      "epoch": 1.9166666666666665,
      "grad_norm": 0.5858086943626404,
      "learning_rate": 0.0001666666666666667,
      "loss": 1.3006,
      "step": 5750
    },
    {
      "epoch": 1.9333333333333333,
      "grad_norm": 0.7806682586669922,
      "learning_rate": 0.00016632302405498285,
      "loss": 1.2361,
      "step": 5800
    },
    {
      "epoch": 1.95,
      "grad_norm": 0.6237480044364929,
      "learning_rate": 0.00016597938144329898,
      "loss": 1.3451,
      "step": 5850
    },
    {
      "epoch": 1.9666666666666668,
      "grad_norm": 0.6422592997550964,
      "learning_rate": 0.00016563573883161513,
      "loss": 1.2656,
      "step": 5900
    },
    {
      "epoch": 1.9833333333333334,
      "grad_norm": 0.4929872751235962,
      "learning_rate": 0.00016529209621993126,
      "loss": 1.3107,
      "step": 5950
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.4354602098464966,
      "learning_rate": 0.00016494845360824742,
      "loss": 1.2556,
      "step": 6000
    },
    {
      "epoch": 2.0166666666666666,
      "grad_norm": 0.7167348861694336,
      "learning_rate": 0.00016460481099656358,
      "loss": 1.1534,
      "step": 6050
    },
    {
      "epoch": 2.033333333333333,
      "grad_norm": 0.7123018503189087,
      "learning_rate": 0.00016426116838487973,
      "loss": 1.1909,
      "step": 6100
    },
    {
      "epoch": 2.05,
      "grad_norm": 0.8153208494186401,
      "learning_rate": 0.0001639175257731959,
      "loss": 1.1926,
      "step": 6150
    },
    {
      "epoch": 2.066666666666667,
      "grad_norm": 0.5038204193115234,
      "learning_rate": 0.00016357388316151202,
      "loss": 1.1823,
      "step": 6200
    },
    {
      "epoch": 2.0833333333333335,
      "grad_norm": 0.8663319945335388,
      "learning_rate": 0.00016323024054982818,
      "loss": 1.2259,
      "step": 6250
    },
    {
      "epoch": 2.1,
      "grad_norm": 0.669266402721405,
      "learning_rate": 0.00016288659793814434,
      "loss": 1.2849,
      "step": 6300
    },
    {
      "epoch": 2.1166666666666667,
      "grad_norm": 0.8865059018135071,
      "learning_rate": 0.0001625429553264605,
      "loss": 1.2884,
      "step": 6350
    },
    {
      "epoch": 2.1333333333333333,
      "grad_norm": 0.8247556686401367,
      "learning_rate": 0.00016219931271477665,
      "loss": 1.2107,
      "step": 6400
    },
    {
      "epoch": 2.15,
      "grad_norm": 0.6957985758781433,
      "learning_rate": 0.00016185567010309278,
      "loss": 1.1129,
      "step": 6450
    },
    {
      "epoch": 2.1666666666666665,
      "grad_norm": 1.7432329654693604,
      "learning_rate": 0.00016151202749140894,
      "loss": 1.2371,
      "step": 6500
    },
    {
      "epoch": 2.183333333333333,
      "grad_norm": 0.7521037459373474,
      "learning_rate": 0.0001611683848797251,
      "loss": 1.1378,
      "step": 6550
    },
    {
      "epoch": 2.2,
      "grad_norm": 0.8273115158081055,
      "learning_rate": 0.00016082474226804125,
      "loss": 1.2234,
      "step": 6600
    },
    {
      "epoch": 2.216666666666667,
      "grad_norm": 0.7179253697395325,
      "learning_rate": 0.0001604810996563574,
      "loss": 1.2355,
      "step": 6650
    },
    {
      "epoch": 2.2333333333333334,
      "grad_norm": 0.8935246467590332,
      "learning_rate": 0.00016013745704467357,
      "loss": 1.1494,
      "step": 6700
    },
    {
      "epoch": 2.25,
      "grad_norm": 0.6201605200767517,
      "learning_rate": 0.0001597938144329897,
      "loss": 1.2296,
      "step": 6750
    },
    {
      "epoch": 2.2666666666666666,
      "grad_norm": 0.5908774137496948,
      "learning_rate": 0.00015945017182130585,
      "loss": 1.27,
      "step": 6800
    },
    {
      "epoch": 2.283333333333333,
      "grad_norm": 0.5920626521110535,
      "learning_rate": 0.00015910652920962198,
      "loss": 1.1386,
      "step": 6850
    },
    {
      "epoch": 2.3,
      "grad_norm": 0.6913658380508423,
      "learning_rate": 0.00015876288659793814,
      "loss": 1.2125,
      "step": 6900
    },
    {
      "epoch": 2.3166666666666664,
      "grad_norm": 0.5607749223709106,
      "learning_rate": 0.0001584192439862543,
      "loss": 1.1825,
      "step": 6950
    },
    {
      "epoch": 2.3333333333333335,
      "grad_norm": 0.7316296100616455,
      "learning_rate": 0.00015807560137457046,
      "loss": 1.2207,
      "step": 7000
    },
    {
      "epoch": 2.35,
      "grad_norm": 0.6697998642921448,
      "learning_rate": 0.0001577319587628866,
      "loss": 1.1831,
      "step": 7050
    },
    {
      "epoch": 2.3666666666666667,
      "grad_norm": 0.46644121408462524,
      "learning_rate": 0.00015738831615120274,
      "loss": 1.1826,
      "step": 7100
    },
    {
      "epoch": 2.3833333333333333,
      "grad_norm": 0.8851874470710754,
      "learning_rate": 0.0001570446735395189,
      "loss": 1.2109,
      "step": 7150
    },
    {
      "epoch": 2.4,
      "grad_norm": 0.6283709406852722,
      "learning_rate": 0.00015670103092783506,
      "loss": 1.2178,
      "step": 7200
    },
    {
      "epoch": 2.4166666666666665,
      "grad_norm": 0.5172827839851379,
      "learning_rate": 0.00015635738831615121,
      "loss": 1.2717,
      "step": 7250
    },
    {
      "epoch": 2.4333333333333336,
      "grad_norm": 0.6698195934295654,
      "learning_rate": 0.00015601374570446737,
      "loss": 1.2077,
      "step": 7300
    },
    {
      "epoch": 2.45,
      "grad_norm": 0.7838262319564819,
      "learning_rate": 0.0001556701030927835,
      "loss": 1.2211,
      "step": 7350
    },
    {
      "epoch": 2.466666666666667,
      "grad_norm": 0.5634123086929321,
      "learning_rate": 0.00015532646048109966,
      "loss": 1.194,
      "step": 7400
    },
    {
      "epoch": 2.4833333333333334,
      "grad_norm": 0.9593711495399475,
      "learning_rate": 0.00015498281786941582,
      "loss": 1.2591,
      "step": 7450
    },
    {
      "epoch": 2.5,
      "grad_norm": 0.8125361800193787,
      "learning_rate": 0.00015463917525773197,
      "loss": 1.3056,
      "step": 7500
    },
    {
      "epoch": 2.5166666666666666,
      "grad_norm": 0.5078768730163574,
      "learning_rate": 0.00015429553264604813,
      "loss": 1.1944,
      "step": 7550
    },
    {
      "epoch": 2.533333333333333,
      "grad_norm": 0.5614415407180786,
      "learning_rate": 0.0001539518900343643,
      "loss": 1.3546,
      "step": 7600
    },
    {
      "epoch": 2.55,
      "grad_norm": 0.939806342124939,
      "learning_rate": 0.00015360824742268042,
      "loss": 1.2816,
      "step": 7650
    },
    {
      "epoch": 2.5666666666666664,
      "grad_norm": 0.6847313046455383,
      "learning_rate": 0.00015326460481099657,
      "loss": 1.3253,
      "step": 7700
    },
    {
      "epoch": 2.5833333333333335,
      "grad_norm": 0.7380647659301758,
      "learning_rate": 0.0001529209621993127,
      "loss": 1.229,
      "step": 7750
    },
    {
      "epoch": 2.6,
      "grad_norm": 0.55921471118927,
      "learning_rate": 0.00015257731958762886,
      "loss": 1.2679,
      "step": 7800
    },
    {
      "epoch": 2.6166666666666667,
      "grad_norm": 0.5151671171188354,
      "learning_rate": 0.00015223367697594502,
      "loss": 1.2314,
      "step": 7850
    },
    {
      "epoch": 2.6333333333333333,
      "grad_norm": 0.3873840868473053,
      "learning_rate": 0.00015189003436426118,
      "loss": 1.1273,
      "step": 7900
    },
    {
      "epoch": 2.65,
      "grad_norm": 0.8343899250030518,
      "learning_rate": 0.00015154639175257733,
      "loss": 1.2341,
      "step": 7950
    },
    {
      "epoch": 2.6666666666666665,
      "grad_norm": 0.5549861192703247,
      "learning_rate": 0.00015120274914089346,
      "loss": 1.3255,
      "step": 8000
    },
    {
      "epoch": 2.6833333333333336,
      "grad_norm": 0.7191237807273865,
      "learning_rate": 0.00015085910652920962,
      "loss": 1.3242,
      "step": 8050
    },
    {
      "epoch": 2.7,
      "grad_norm": 0.7853273153305054,
      "learning_rate": 0.00015051546391752578,
      "loss": 1.3265,
      "step": 8100
    },
    {
      "epoch": 2.716666666666667,
      "grad_norm": 0.6109648942947388,
      "learning_rate": 0.00015017182130584194,
      "loss": 1.1678,
      "step": 8150
    },
    {
      "epoch": 2.7333333333333334,
      "grad_norm": 0.5469188094139099,
      "learning_rate": 0.0001498281786941581,
      "loss": 1.1917,
      "step": 8200
    },
    {
      "epoch": 2.75,
      "grad_norm": 0.5989205241203308,
      "learning_rate": 0.00014948453608247422,
      "loss": 1.2178,
      "step": 8250
    },
    {
      "epoch": 2.7666666666666666,
      "grad_norm": 0.5792564749717712,
      "learning_rate": 0.00014914089347079038,
      "loss": 1.2399,
      "step": 8300
    },
    {
      "epoch": 2.783333333333333,
      "grad_norm": 0.6717056632041931,
      "learning_rate": 0.00014879725085910654,
      "loss": 1.1889,
      "step": 8350
    },
    {
      "epoch": 2.8,
      "grad_norm": 0.5392754077911377,
      "learning_rate": 0.0001484536082474227,
      "loss": 1.1971,
      "step": 8400
    },
    {
      "epoch": 2.8166666666666664,
      "grad_norm": 0.9009340405464172,
      "learning_rate": 0.00014810996563573885,
      "loss": 1.2867,
      "step": 8450
    },
    {
      "epoch": 2.8333333333333335,
      "grad_norm": 0.805875301361084,
      "learning_rate": 0.000147766323024055,
      "loss": 1.2631,
      "step": 8500
    },
    {
      "epoch": 2.85,
      "grad_norm": 0.49231454730033875,
      "learning_rate": 0.00014742268041237114,
      "loss": 1.2659,
      "step": 8550
    },
    {
      "epoch": 2.8666666666666667,
      "grad_norm": 0.47572728991508484,
      "learning_rate": 0.0001470790378006873,
      "loss": 1.2806,
      "step": 8600
    },
    {
      "epoch": 2.8833333333333333,
      "grad_norm": 0.4458601474761963,
      "learning_rate": 0.00014673539518900343,
      "loss": 1.235,
      "step": 8650
    },
    {
      "epoch": 2.9,
      "grad_norm": 0.4752271771430969,
      "learning_rate": 0.00014639175257731958,
      "loss": 1.1716,
      "step": 8700
    },
    {
      "epoch": 2.9166666666666665,
      "grad_norm": 0.7757300138473511,
      "learning_rate": 0.00014604810996563574,
      "loss": 1.2706,
      "step": 8750
    },
    {
      "epoch": 2.9333333333333336,
      "grad_norm": 1.3189328908920288,
      "learning_rate": 0.0001457044673539519,
      "loss": 1.1922,
      "step": 8800
    },
    {
      "epoch": 2.95,
      "grad_norm": 0.7642238140106201,
      "learning_rate": 0.00014536082474226805,
      "loss": 1.2638,
      "step": 8850
    },
    {
      "epoch": 2.966666666666667,
      "grad_norm": 0.6204886436462402,
      "learning_rate": 0.00014501718213058418,
      "loss": 1.1832,
      "step": 8900
    },
    {
      "epoch": 2.9833333333333334,
      "grad_norm": 0.5461443662643433,
      "learning_rate": 0.00014467353951890034,
      "loss": 1.2487,
      "step": 8950
    },
    {
      "epoch": 3.0,
      "grad_norm": 0.6285306811332703,
      "learning_rate": 0.0001443298969072165,
      "loss": 1.2563,
      "step": 9000
    },
    {
      "epoch": 3.0166666666666666,
      "grad_norm": 0.8330346941947937,
      "learning_rate": 0.00014398625429553266,
      "loss": 1.1368,
      "step": 9050
    },
    {
      "epoch": 3.033333333333333,
      "grad_norm": 0.5629140138626099,
      "learning_rate": 0.00014364261168384881,
      "loss": 1.1923,
      "step": 9100
    },
    {
      "epoch": 3.05,
      "grad_norm": 0.6934713125228882,
      "learning_rate": 0.00014329896907216494,
      "loss": 1.1015,
      "step": 9150
    },
    {
      "epoch": 3.066666666666667,
      "grad_norm": 0.6333319544792175,
      "learning_rate": 0.0001429553264604811,
      "loss": 1.0868,
      "step": 9200
    },
    {
      "epoch": 3.0833333333333335,
      "grad_norm": 0.5893462300300598,
      "learning_rate": 0.00014261168384879726,
      "loss": 1.1002,
      "step": 9250
    },
    {
      "epoch": 3.1,
      "grad_norm": 1.1958461999893188,
      "learning_rate": 0.00014226804123711342,
      "loss": 1.1706,
      "step": 9300
    },
    {
      "epoch": 3.1166666666666667,
      "grad_norm": 0.5737650394439697,
      "learning_rate": 0.00014192439862542957,
      "loss": 1.2047,
      "step": 9350
    },
    {
      "epoch": 3.1333333333333333,
      "grad_norm": 0.59295654296875,
      "learning_rate": 0.00014158075601374573,
      "loss": 1.2635,
      "step": 9400
    },
    {
      "epoch": 3.15,
      "grad_norm": 0.6686267852783203,
      "learning_rate": 0.00014123711340206186,
      "loss": 1.1544,
      "step": 9450
    },
    {
      "epoch": 3.1666666666666665,
      "grad_norm": 0.5861908197402954,
      "learning_rate": 0.00014089347079037802,
      "loss": 1.1593,
      "step": 9500
    },
    {
      "epoch": 3.183333333333333,
      "grad_norm": 0.5233091711997986,
      "learning_rate": 0.00014054982817869415,
      "loss": 1.142,
      "step": 9550
    },
    {
      "epoch": 3.2,
      "grad_norm": 0.6590291857719421,
      "learning_rate": 0.0001402061855670103,
      "loss": 1.1876,
      "step": 9600
    },
    {
      "epoch": 3.216666666666667,
      "grad_norm": 0.6201801300048828,
      "learning_rate": 0.00013986254295532646,
      "loss": 1.2354,
      "step": 9650
    },
    {
      "epoch": 3.2333333333333334,
      "grad_norm": 0.5609785318374634,
      "learning_rate": 0.00013951890034364262,
      "loss": 1.148,
      "step": 9700
    },
    {
      "epoch": 3.25,
      "grad_norm": 1.01478111743927,
      "learning_rate": 0.00013917525773195878,
      "loss": 1.2287,
      "step": 9750
    },
    {
      "epoch": 3.2666666666666666,
      "grad_norm": 0.7681980133056641,
      "learning_rate": 0.0001388316151202749,
      "loss": 1.1092,
      "step": 9800
    },
    {
      "epoch": 3.283333333333333,
      "grad_norm": 0.8341172337532043,
      "learning_rate": 0.00013848797250859106,
      "loss": 1.2099,
      "step": 9850
    },
    {
      "epoch": 3.3,
      "grad_norm": 0.5373563766479492,
      "learning_rate": 0.00013814432989690722,
      "loss": 1.135,
      "step": 9900
    },
    {
      "epoch": 3.3166666666666664,
      "grad_norm": 0.6503771543502808,
      "learning_rate": 0.00013780068728522338,
      "loss": 1.2162,
      "step": 9950
    },
    {
      "epoch": 3.3333333333333335,
      "grad_norm": 0.555709183216095,
      "learning_rate": 0.00013745704467353953,
      "loss": 1.2034,
      "step": 10000
    },
    {
      "epoch": 3.35,
      "grad_norm": 0.5131813883781433,
      "learning_rate": 0.00013711340206185566,
      "loss": 1.216,
      "step": 10050
    },
    {
      "epoch": 3.3666666666666667,
      "grad_norm": 0.8000469207763672,
      "learning_rate": 0.00013676975945017182,
      "loss": 1.1809,
      "step": 10100
    },
    {
      "epoch": 3.3833333333333333,
      "grad_norm": 0.9390401244163513,
      "learning_rate": 0.00013642611683848798,
      "loss": 1.2028,
      "step": 10150
    },
    {
      "epoch": 3.4,
      "grad_norm": 0.6230164170265198,
      "learning_rate": 0.00013608247422680414,
      "loss": 1.2416,
      "step": 10200
    },
    {
      "epoch": 3.4166666666666665,
      "grad_norm": 0.5256366729736328,
      "learning_rate": 0.0001357388316151203,
      "loss": 1.1451,
      "step": 10250
    },
    {
      "epoch": 3.4333333333333336,
      "grad_norm": 0.7990705370903015,
      "learning_rate": 0.00013539518900343645,
      "loss": 1.2084,
      "step": 10300
    },
    {
      "epoch": 3.45,
      "grad_norm": 0.6929700374603271,
      "learning_rate": 0.00013505154639175258,
      "loss": 1.2568,
      "step": 10350
    },
    {
      "epoch": 3.466666666666667,
      "grad_norm": 0.6371334195137024,
      "learning_rate": 0.00013470790378006874,
      "loss": 1.1666,
      "step": 10400
    },
    {
      "epoch": 3.4833333333333334,
      "grad_norm": 0.9056955575942993,
      "learning_rate": 0.00013436426116838487,
      "loss": 1.1308,
      "step": 10450
    },
    {
      "epoch": 3.5,
      "grad_norm": 0.7029597759246826,
      "learning_rate": 0.00013402061855670103,
      "loss": 1.0726,
      "step": 10500
    },
    {
      "epoch": 3.5166666666666666,
      "grad_norm": 0.8194389939308167,
      "learning_rate": 0.00013367697594501718,
      "loss": 1.0523,
      "step": 10550
    },
    {
      "epoch": 3.533333333333333,
      "grad_norm": 0.48772525787353516,
      "learning_rate": 0.00013333333333333334,
      "loss": 1.0959,
      "step": 10600
    },
    {
      "epoch": 3.55,
      "grad_norm": 0.6802676320075989,
      "learning_rate": 0.0001329896907216495,
      "loss": 1.1957,
      "step": 10650
    },
    {
      "epoch": 3.5666666666666664,
      "grad_norm": 0.7600836753845215,
      "learning_rate": 0.00013264604810996563,
      "loss": 1.1696,
      "step": 10700
    },
    {
      "epoch": 3.5833333333333335,
      "grad_norm": 0.6701776385307312,
      "learning_rate": 0.00013230240549828178,
      "loss": 1.1312,
      "step": 10750
    },
    {
      "epoch": 3.6,
      "grad_norm": 0.6881401538848877,
      "learning_rate": 0.00013195876288659794,
      "loss": 1.1759,
      "step": 10800
    },
    {
      "epoch": 3.6166666666666667,
      "grad_norm": 0.7181477546691895,
      "learning_rate": 0.0001316151202749141,
      "loss": 1.2219,
      "step": 10850
    },
    {
      "epoch": 3.6333333333333333,
      "grad_norm": 0.7607913017272949,
      "learning_rate": 0.00013127147766323026,
      "loss": 1.1617,
      "step": 10900
    },
    {
      "epoch": 3.65,
      "grad_norm": 0.8764116168022156,
      "learning_rate": 0.00013092783505154639,
      "loss": 1.1539,
      "step": 10950
    },
    {
      "epoch": 3.6666666666666665,
      "grad_norm": 0.7274893522262573,
      "learning_rate": 0.00013058419243986254,
      "loss": 1.1308,
      "step": 11000
    },
    {
      "epoch": 3.6833333333333336,
      "grad_norm": 0.7923114895820618,
      "learning_rate": 0.0001302405498281787,
      "loss": 1.1283,
      "step": 11050
    },
    {
      "epoch": 3.7,
      "grad_norm": 0.562752366065979,
      "learning_rate": 0.00012989690721649486,
      "loss": 1.1174,
      "step": 11100
    },
    {
      "epoch": 3.716666666666667,
      "grad_norm": 0.9028100371360779,
      "learning_rate": 0.00012955326460481101,
      "loss": 1.2108,
      "step": 11150
    },
    {
      "epoch": 3.7333333333333334,
      "grad_norm": 0.6666211485862732,
      "learning_rate": 0.00012920962199312717,
      "loss": 1.2537,
      "step": 11200
    },
    {
      "epoch": 3.75,
      "grad_norm": 0.7046383023262024,
      "learning_rate": 0.0001288659793814433,
      "loss": 1.2264,
      "step": 11250
    },
    {
      "epoch": 3.7666666666666666,
      "grad_norm": 0.7835549116134644,
      "learning_rate": 0.00012852233676975946,
      "loss": 1.2586,
      "step": 11300
    },
    {
      "epoch": 3.783333333333333,
      "grad_norm": 1.1768344640731812,
      "learning_rate": 0.00012817869415807562,
      "loss": 1.2241,
      "step": 11350
    },
    {
      "epoch": 3.8,
      "grad_norm": 0.6695596575737,
      "learning_rate": 0.00012783505154639175,
      "loss": 1.1381,
      "step": 11400
    },
    {
      "epoch": 3.8166666666666664,
      "grad_norm": 1.3676828145980835,
      "learning_rate": 0.0001274914089347079,
      "loss": 1.1637,
      "step": 11450
    },
    {
      "epoch": 3.8333333333333335,
      "grad_norm": 0.5408380627632141,
      "learning_rate": 0.00012714776632302406,
      "loss": 1.1649,
      "step": 11500
    },
    {
      "epoch": 3.85,
      "grad_norm": 0.9225865006446838,
      "learning_rate": 0.00012680412371134022,
      "loss": 1.148,
      "step": 11550
    },
    {
      "epoch": 3.8666666666666667,
      "grad_norm": 0.7432395219802856,
      "learning_rate": 0.00012646048109965635,
      "loss": 1.2407,
      "step": 11600
    },
    {
      "epoch": 3.8833333333333333,
      "grad_norm": 1.1419345140457153,
      "learning_rate": 0.0001261168384879725,
      "loss": 1.2512,
      "step": 11650
    },
    {
      "epoch": 3.9,
      "grad_norm": 0.8101910352706909,
      "learning_rate": 0.00012577319587628866,
      "loss": 1.2137,
      "step": 11700
    },
    {
      "epoch": 3.9166666666666665,
      "grad_norm": 1.16474187374115,
      "learning_rate": 0.00012542955326460482,
      "loss": 1.2593,
      "step": 11750
    },
    {
      "epoch": 3.9333333333333336,
      "grad_norm": 0.673358678817749,
      "learning_rate": 0.00012508591065292098,
      "loss": 1.1369,
      "step": 11800
    },
    {
      "epoch": 3.95,
      "grad_norm": 0.9823682904243469,
      "learning_rate": 0.0001247422680412371,
      "loss": 1.146,
      "step": 11850
    },
    {
      "epoch": 3.966666666666667,
      "grad_norm": 0.6780192852020264,
      "learning_rate": 0.00012439862542955326,
      "loss": 1.1532,
      "step": 11900
    },
    {
      "epoch": 3.9833333333333334,
      "grad_norm": 0.6471381187438965,
      "learning_rate": 0.00012405498281786942,
      "loss": 1.2444,
      "step": 11950
    },
    {
      "epoch": 4.0,
      "grad_norm": 0.9242578148841858,
      "learning_rate": 0.00012371134020618558,
      "loss": 1.2148,
      "step": 12000
    },
    {
      "epoch": 4.016666666666667,
      "grad_norm": 0.8181788921356201,
      "learning_rate": 0.00012336769759450174,
      "loss": 1.1533,
      "step": 12050
    },
    {
      "epoch": 4.033333333333333,
      "grad_norm": 0.581044614315033,
      "learning_rate": 0.0001230240549828179,
      "loss": 1.1742,
      "step": 12100
    },
    {
      "epoch": 4.05,
      "grad_norm": 2.0443358421325684,
      "learning_rate": 0.00012268041237113402,
      "loss": 1.0558,
      "step": 12150
    },
    {
      "epoch": 4.066666666666666,
      "grad_norm": 1.0696319341659546,
      "learning_rate": 0.00012233676975945018,
      "loss": 1.1693,
      "step": 12200
    },
    {
      "epoch": 4.083333333333333,
      "grad_norm": 0.6504364013671875,
      "learning_rate": 0.00012199312714776634,
      "loss": 1.098,
      "step": 12250
    },
    {
      "epoch": 4.1,
      "grad_norm": 0.7216753363609314,
      "learning_rate": 0.00012164948453608247,
      "loss": 1.132,
      "step": 12300
    },
    {
      "epoch": 4.116666666666666,
      "grad_norm": 0.8240640759468079,
      "learning_rate": 0.00012130584192439862,
      "loss": 1.1394,
      "step": 12350
    },
    {
      "epoch": 4.133333333333334,
      "grad_norm": 1.7112442255020142,
      "learning_rate": 0.00012096219931271477,
      "loss": 1.1218,
      "step": 12400
    },
    {
      "epoch": 4.15,
      "grad_norm": 0.7697138786315918,
      "learning_rate": 0.00012061855670103093,
      "loss": 1.1353,
      "step": 12450
    },
    {
      "epoch": 4.166666666666667,
      "grad_norm": 0.4613429307937622,
      "learning_rate": 0.00012027491408934708,
      "loss": 1.0563,
      "step": 12500
    },
    {
      "epoch": 4.183333333333334,
      "grad_norm": 0.6444631814956665,
      "learning_rate": 0.00011993127147766323,
      "loss": 1.0544,
      "step": 12550
    },
    {
      "epoch": 4.2,
      "grad_norm": 0.6966649293899536,
      "learning_rate": 0.00011958762886597938,
      "loss": 1.0808,
      "step": 12600
    },
    {
      "epoch": 4.216666666666667,
      "grad_norm": 0.9750837683677673,
      "learning_rate": 0.00011924398625429554,
      "loss": 1.226,
      "step": 12650
    },
    {
      "epoch": 4.233333333333333,
      "grad_norm": 0.5107461214065552,
      "learning_rate": 0.00011890034364261168,
      "loss": 1.1314,
      "step": 12700
    },
    {
      "epoch": 4.25,
      "grad_norm": 1.4627799987792969,
      "learning_rate": 0.00011855670103092784,
      "loss": 1.2893,
      "step": 12750
    },
    {
      "epoch": 4.266666666666667,
      "grad_norm": 0.9754939675331116,
      "learning_rate": 0.000118213058419244,
      "loss": 1.1073,
      "step": 12800
    },
    {
      "epoch": 4.283333333333333,
      "grad_norm": 1.2098749876022339,
      "learning_rate": 0.00011786941580756014,
      "loss": 1.0091,
      "step": 12850
    },
    {
      "epoch": 4.3,
      "grad_norm": 0.5392860770225525,
      "learning_rate": 0.0001175257731958763,
      "loss": 1.074,
      "step": 12900
    },
    {
      "epoch": 4.316666666666666,
      "grad_norm": 1.0701839923858643,
      "learning_rate": 0.00011718213058419246,
      "loss": 1.166,
      "step": 12950
    },
    {
      "epoch": 4.333333333333333,
      "grad_norm": 0.728482723236084,
      "learning_rate": 0.0001168384879725086,
      "loss": 1.2132,
      "step": 13000
    },
    {
      "epoch": 4.35,
      "grad_norm": 3.3743903636932373,
      "learning_rate": 0.00011649484536082476,
      "loss": 1.129,
      "step": 13050
    },
    {
      "epoch": 4.366666666666666,
      "grad_norm": 0.8227960467338562,
      "learning_rate": 0.00011615120274914091,
      "loss": 1.1875,
      "step": 13100
    },
    {
      "epoch": 4.383333333333334,
      "grad_norm": 0.586129903793335,
      "learning_rate": 0.00011580756013745706,
      "loss": 1.0985,
      "step": 13150
    },
    {
      "epoch": 4.4,
      "grad_norm": 0.7233460545539856,
      "learning_rate": 0.00011546391752577319,
      "loss": 1.1568,
      "step": 13200
    },
    {
      "epoch": 4.416666666666667,
      "grad_norm": 0.8178878426551819,
      "learning_rate": 0.00011512027491408935,
      "loss": 1.1799,
      "step": 13250
    },
    {
      "epoch": 4.433333333333334,
      "grad_norm": 0.7883362174034119,
      "learning_rate": 0.00011477663230240549,
      "loss": 1.0558,
      "step": 13300
    },
    {
      "epoch": 4.45,
      "grad_norm": 1.0349963903427124,
      "learning_rate": 0.00011443298969072165,
      "loss": 1.2596,
      "step": 13350
    },
    {
      "epoch": 4.466666666666667,
      "grad_norm": 0.8545379042625427,
      "learning_rate": 0.0001140893470790378,
      "loss": 1.1717,
      "step": 13400
    },
    {
      "epoch": 4.483333333333333,
      "grad_norm": 1.19474196434021,
      "learning_rate": 0.00011374570446735395,
      "loss": 1.129,
      "step": 13450
    },
    {
      "epoch": 4.5,
      "grad_norm": 1.740073800086975,
      "learning_rate": 0.0001134020618556701,
      "loss": 1.1008,
      "step": 13500
    },
    {
      "epoch": 4.516666666666667,
      "grad_norm": 0.9469364881515503,
      "learning_rate": 0.00011305841924398626,
      "loss": 1.0956,
      "step": 13550
    },
    {
      "epoch": 4.533333333333333,
      "grad_norm": 0.8604798913002014,
      "learning_rate": 0.0001127147766323024,
      "loss": 1.1896,
      "step": 13600
    },
    {
      "epoch": 4.55,
      "grad_norm": 0.7396515011787415,
      "learning_rate": 0.00011237113402061856,
      "loss": 1.0512,
      "step": 13650
    },
    {
      "epoch": 4.566666666666666,
      "grad_norm": 0.937104344367981,
      "learning_rate": 0.00011202749140893472,
      "loss": 1.1152,
      "step": 13700
    },
    {
      "epoch": 4.583333333333333,
      "grad_norm": 0.9468133449554443,
      "learning_rate": 0.00011168384879725086,
      "loss": 1.0381,
      "step": 13750
    },
    {
      "epoch": 4.6,
      "grad_norm": 0.7779613137245178,
      "learning_rate": 0.00011134020618556702,
      "loss": 1.0395,
      "step": 13800
    },
    {
      "epoch": 4.616666666666667,
      "grad_norm": 0.9134076833724976,
      "learning_rate": 0.00011099656357388318,
      "loss": 1.0951,
      "step": 13850
    },
    {
      "epoch": 4.633333333333333,
      "grad_norm": 0.3993970453739166,
      "learning_rate": 0.00011065292096219932,
      "loss": 1.1421,
      "step": 13900
    },
    {
      "epoch": 4.65,
      "grad_norm": 0.8960808515548706,
      "learning_rate": 0.00011030927835051548,
      "loss": 1.1141,
      "step": 13950
    },
    {
      "epoch": 4.666666666666667,
      "grad_norm": 0.8083409667015076,
      "learning_rate": 0.00010996563573883164,
      "loss": 1.0632,
      "step": 14000
    },
    {
      "epoch": 4.683333333333334,
      "grad_norm": 0.7203385829925537,
      "learning_rate": 0.00010962199312714778,
      "loss": 1.0335,
      "step": 14050
    },
    {
      "epoch": 4.7,
      "grad_norm": 1.0927156209945679,
      "learning_rate": 0.00010927835051546391,
      "loss": 1.122,
      "step": 14100
    },
    {
      "epoch": 4.716666666666667,
      "grad_norm": 0.9105370044708252,
      "learning_rate": 0.00010893470790378007,
      "loss": 1.2361,
      "step": 14150
    },
    {
      "epoch": 4.733333333333333,
      "grad_norm": 0.9920133352279663,
      "learning_rate": 0.00010859106529209621,
      "loss": 1.0493,
      "step": 14200
    },
    {
      "epoch": 4.75,
      "grad_norm": 0.7797075510025024,
      "learning_rate": 0.00010824742268041237,
      "loss": 1.1558,
      "step": 14250
    },
    {
      "epoch": 4.766666666666667,
      "grad_norm": 0.9971728920936584,
      "learning_rate": 0.00010790378006872852,
      "loss": 1.1013,
      "step": 14300
    },
    {
      "epoch": 4.783333333333333,
      "grad_norm": 1.0941495895385742,
      "learning_rate": 0.00010756013745704467,
      "loss": 1.1306,
      "step": 14350
    },
    {
      "epoch": 4.8,
      "grad_norm": 1.0263404846191406,
      "learning_rate": 0.00010721649484536083,
      "loss": 1.1918,
      "step": 14400
    },
    {
      "epoch": 4.816666666666666,
      "grad_norm": 1.1941815614700317,
      "learning_rate": 0.00010687285223367698,
      "loss": 1.1409,
      "step": 14450
    },
    {
      "epoch": 4.833333333333333,
      "grad_norm": 0.5699111223220825,
      "learning_rate": 0.00010652920962199313,
      "loss": 1.2127,
      "step": 14500
    },
    {
      "epoch": 4.85,
      "grad_norm": 0.7781313061714172,
      "learning_rate": 0.00010618556701030928,
      "loss": 1.1376,
      "step": 14550
    },
    {
      "epoch": 4.866666666666667,
      "grad_norm": 1.1478484869003296,
      "learning_rate": 0.00010584192439862544,
      "loss": 1.1697,
      "step": 14600
    },
    {
      "epoch": 4.883333333333333,
      "grad_norm": 0.6607789397239685,
      "learning_rate": 0.00010549828178694158,
      "loss": 1.0961,
      "step": 14650
    },
    {
      "epoch": 4.9,
      "grad_norm": 0.6426371335983276,
      "learning_rate": 0.00010515463917525774,
      "loss": 1.0282,
      "step": 14700
    },
    {
      "epoch": 4.916666666666667,
      "grad_norm": 0.8193576335906982,
      "learning_rate": 0.0001048109965635739,
      "loss": 1.1913,
      "step": 14750
    },
    {
      "epoch": 4.933333333333334,
      "grad_norm": 1.0655537843704224,
      "learning_rate": 0.00010446735395189004,
      "loss": 1.2441,
      "step": 14800
    },
    {
      "epoch": 4.95,
      "grad_norm": 0.7931993007659912,
      "learning_rate": 0.0001041237113402062,
      "loss": 1.0931,
      "step": 14850
    },
    {
      "epoch": 4.966666666666667,
      "grad_norm": 0.5860578417778015,
      "learning_rate": 0.00010378006872852236,
      "loss": 1.1652,
      "step": 14900
    },
    {
      "epoch": 4.983333333333333,
      "grad_norm": 0.7099295258522034,
      "learning_rate": 0.0001034364261168385,
      "loss": 1.1076,
      "step": 14950
    },
    {
      "epoch": 5.0,
      "grad_norm": 0.6846280097961426,
      "learning_rate": 0.00010309278350515463,
      "loss": 1.1702,
      "step": 15000
    },
    {
      "epoch": 5.016666666666667,
      "grad_norm": 1.0829776525497437,
      "learning_rate": 0.00010274914089347079,
      "loss": 0.997,
      "step": 15050
    },
    {
      "epoch": 5.033333333333333,
      "grad_norm": 0.895159900188446,
      "learning_rate": 0.00010240549828178693,
      "loss": 1.0716,
      "step": 15100
    },
    {
      "epoch": 5.05,
      "grad_norm": 0.9152355790138245,
      "learning_rate": 0.00010206185567010309,
      "loss": 0.9425,
      "step": 15150
    },
    {
      "epoch": 5.066666666666666,
      "grad_norm": 0.4316493570804596,
      "learning_rate": 0.00010171821305841925,
      "loss": 1.0512,
      "step": 15200
    },
    {
      "epoch": 5.083333333333333,
      "grad_norm": 0.6660581827163696,
      "learning_rate": 0.00010137457044673539,
      "loss": 1.0527,
      "step": 15250
    },
    {
      "epoch": 5.1,
      "grad_norm": 0.8138285875320435,
      "learning_rate": 0.00010103092783505155,
      "loss": 1.0118,
      "step": 15300
    },
    {
      "epoch": 5.116666666666666,
      "grad_norm": 1.0281500816345215,
      "learning_rate": 0.0001006872852233677,
      "loss": 1.1614,
      "step": 15350
    },
    {
      "epoch": 5.133333333333334,
      "grad_norm": 0.7777808308601379,
      "learning_rate": 0.00010034364261168385,
      "loss": 1.0414,
      "step": 15400
    },
    {
      "epoch": 5.15,
      "grad_norm": 1.2796882390975952,
      "learning_rate": 0.0001,
      "loss": 1.0179,
      "step": 15450
    },
    {
      "epoch": 5.166666666666667,
      "grad_norm": 0.5546625256538391,
      "learning_rate": 9.965635738831616e-05,
      "loss": 1.0617,
      "step": 15500
    },
    {
      "epoch": 5.183333333333334,
      "grad_norm": 1.220820426940918,
      "learning_rate": 9.93127147766323e-05,
      "loss": 1.1684,
      "step": 15550
    },
    {
      "epoch": 5.2,
      "grad_norm": 0.6583393812179565,
      "learning_rate": 9.896907216494846e-05,
      "loss": 1.092,
      "step": 15600
    },
    {
      "epoch": 5.216666666666667,
      "grad_norm": 1.9202802181243896,
      "learning_rate": 9.862542955326462e-05,
      "loss": 1.0345,
      "step": 15650
    },
    {
      "epoch": 5.233333333333333,
      "grad_norm": 0.899642288684845,
      "learning_rate": 9.828178694158075e-05,
      "loss": 1.0829,
      "step": 15700
    },
    {
      "epoch": 5.25,
      "grad_norm": 0.8743175268173218,
      "learning_rate": 9.793814432989691e-05,
      "loss": 1.0707,
      "step": 15750
    },
    {
      "epoch": 5.266666666666667,
      "grad_norm": 3.1880910396575928,
      "learning_rate": 9.759450171821306e-05,
      "loss": 1.1831,
      "step": 15800
    },
    {
      "epoch": 5.283333333333333,
      "grad_norm": 0.7530702948570251,
      "learning_rate": 9.725085910652921e-05,
      "loss": 1.112,
      "step": 15850
    },
    {
      "epoch": 5.3,
      "grad_norm": 0.6409288048744202,
      "learning_rate": 9.690721649484537e-05,
      "loss": 1.063,
      "step": 15900
    },
    {
      "epoch": 5.316666666666666,
      "grad_norm": 0.5690522193908691,
      "learning_rate": 9.656357388316152e-05,
      "loss": 1.0976,
      "step": 15950
    },
    {
      "epoch": 5.333333333333333,
      "grad_norm": 0.5734624266624451,
      "learning_rate": 9.621993127147767e-05,
      "loss": 0.973,
      "step": 16000
    },
    {
      "epoch": 5.35,
      "grad_norm": 0.9364268779754639,
      "learning_rate": 9.587628865979382e-05,
      "loss": 1.0354,
      "step": 16050
    },
    {
      "epoch": 5.366666666666666,
      "grad_norm": 1.408838152885437,
      "learning_rate": 9.553264604810998e-05,
      "loss": 1.0917,
      "step": 16100
    },
    {
      "epoch": 5.383333333333334,
      "grad_norm": 0.6539489030838013,
      "learning_rate": 9.518900343642611e-05,
      "loss": 1.1213,
      "step": 16150
    },
    {
      "epoch": 5.4,
      "grad_norm": 0.5840668678283691,
      "learning_rate": 9.484536082474227e-05,
      "loss": 1.0801,
      "step": 16200
    },
    {
      "epoch": 5.416666666666667,
      "grad_norm": 0.649293065071106,
      "learning_rate": 9.450171821305843e-05,
      "loss": 1.0541,
      "step": 16250
    },
    {
      "epoch": 5.433333333333334,
      "grad_norm": 0.592671811580658,
      "learning_rate": 9.415807560137457e-05,
      "loss": 1.1269,
      "step": 16300
    },
    {
      "epoch": 5.45,
      "grad_norm": 0.7252998948097229,
      "learning_rate": 9.381443298969073e-05,
      "loss": 1.0889,
      "step": 16350
    },
    {
      "epoch": 5.466666666666667,
      "grad_norm": 0.8437129855155945,
      "learning_rate": 9.347079037800688e-05,
      "loss": 1.0104,
      "step": 16400
    },
    {
      "epoch": 5.483333333333333,
      "grad_norm": 0.5970423221588135,
      "learning_rate": 9.312714776632303e-05,
      "loss": 1.1983,
      "step": 16450
    },
    {
      "epoch": 5.5,
      "grad_norm": 0.6536756157875061,
      "learning_rate": 9.278350515463918e-05,
      "loss": 1.0909,
      "step": 16500
    },
    {
      "epoch": 5.516666666666667,
      "grad_norm": 0.8458763360977173,
      "learning_rate": 9.243986254295534e-05,
      "loss": 1.086,
      "step": 16550
    },
    {
      "epoch": 5.533333333333333,
      "grad_norm": 0.7004075646400452,
      "learning_rate": 9.209621993127147e-05,
      "loss": 1.1141,
      "step": 16600
    },
    {
      "epoch": 5.55,
      "grad_norm": 0.6332960724830627,
      "learning_rate": 9.175257731958763e-05,
      "loss": 1.1128,
      "step": 16650
    },
    {
      "epoch": 5.566666666666666,
      "grad_norm": 1.0327448844909668,
      "learning_rate": 9.140893470790379e-05,
      "loss": 1.1143,
      "step": 16700
    },
    {
      "epoch": 5.583333333333333,
      "grad_norm": 0.5879971981048584,
      "learning_rate": 9.106529209621993e-05,
      "loss": 1.1114,
      "step": 16750
    },
    {
      "epoch": 5.6,
      "grad_norm": 1.1075092554092407,
      "learning_rate": 9.072164948453609e-05,
      "loss": 1.1219,
      "step": 16800
    },
    {
      "epoch": 5.616666666666667,
      "grad_norm": 0.8018264770507812,
      "learning_rate": 9.037800687285224e-05,
      "loss": 1.0635,
      "step": 16850
    },
    {
      "epoch": 5.633333333333333,
      "grad_norm": 0.7140997052192688,
      "learning_rate": 9.003436426116839e-05,
      "loss": 1.1242,
      "step": 16900
    },
    {
      "epoch": 5.65,
      "grad_norm": 0.6958218812942505,
      "learning_rate": 8.969072164948454e-05,
      "loss": 1.1163,
      "step": 16950
    },
    {
      "epoch": 5.666666666666667,
      "grad_norm": 1.3737437725067139,
      "learning_rate": 8.93470790378007e-05,
      "loss": 1.1273,
      "step": 17000
    },
    {
      "epoch": 5.683333333333334,
      "grad_norm": 1.3578484058380127,
      "learning_rate": 8.900343642611683e-05,
      "loss": 1.1455,
      "step": 17050
    },
    {
      "epoch": 5.7,
      "grad_norm": 0.5925753116607666,
      "learning_rate": 8.865979381443299e-05,
      "loss": 1.0379,
      "step": 17100
    },
    {
      "epoch": 5.716666666666667,
      "grad_norm": 0.9957646727561951,
      "learning_rate": 8.831615120274915e-05,
      "loss": 1.0599,
      "step": 17150
    },
    {
      "epoch": 5.733333333333333,
      "grad_norm": 1.549596905708313,
      "learning_rate": 8.797250859106529e-05,
      "loss": 1.1476,
      "step": 17200
    },
    {
      "epoch": 5.75,
      "grad_norm": 0.61615389585495,
      "learning_rate": 8.762886597938145e-05,
      "loss": 1.1192,
      "step": 17250
    },
    {
      "epoch": 5.766666666666667,
      "grad_norm": 0.6676623225212097,
      "learning_rate": 8.72852233676976e-05,
      "loss": 1.0666,
      "step": 17300
    },
    {
      "epoch": 5.783333333333333,
      "grad_norm": 0.6045536994934082,
      "learning_rate": 8.694158075601375e-05,
      "loss": 1.0427,
      "step": 17350
    },
    {
      "epoch": 5.8,
      "grad_norm": 1.6595512628555298,
      "learning_rate": 8.65979381443299e-05,
      "loss": 1.1445,
      "step": 17400
    },
    {
      "epoch": 5.816666666666666,
      "grad_norm": 0.6919371485710144,
      "learning_rate": 8.625429553264606e-05,
      "loss": 1.0912,
      "step": 17450
    },
    {
      "epoch": 5.833333333333333,
      "grad_norm": 1.0352071523666382,
      "learning_rate": 8.591065292096219e-05,
      "loss": 1.0674,
      "step": 17500
    },
    {
      "epoch": 5.85,
      "grad_norm": 1.1265908479690552,
      "learning_rate": 8.556701030927835e-05,
      "loss": 1.1036,
      "step": 17550
    },
    {
      "epoch": 5.866666666666667,
      "grad_norm": 0.7244809865951538,
      "learning_rate": 8.52233676975945e-05,
      "loss": 1.1026,
      "step": 17600
    },
    {
      "epoch": 5.883333333333333,
      "grad_norm": 1.3276067972183228,
      "learning_rate": 8.487972508591065e-05,
      "loss": 1.0917,
      "step": 17650
    },
    {
      "epoch": 5.9,
      "grad_norm": 1.0138040781021118,
      "learning_rate": 8.453608247422681e-05,
      "loss": 1.0651,
      "step": 17700
    },
    {
      "epoch": 5.916666666666667,
      "grad_norm": 0.7563294768333435,
      "learning_rate": 8.419243986254296e-05,
      "loss": 1.2154,
      "step": 17750
    },
    {
      "epoch": 5.933333333333334,
      "grad_norm": 0.789185106754303,
      "learning_rate": 8.384879725085911e-05,
      "loss": 1.1189,
      "step": 17800
    },
    {
      "epoch": 5.95,
      "grad_norm": 1.141954779624939,
      "learning_rate": 8.350515463917527e-05,
      "loss": 1.1136,
      "step": 17850
    },
    {
      "epoch": 5.966666666666667,
      "grad_norm": 0.7704297304153442,
      "learning_rate": 8.316151202749142e-05,
      "loss": 1.0445,
      "step": 17900
    },
    {
      "epoch": 5.983333333333333,
      "grad_norm": 0.8603028059005737,
      "learning_rate": 8.281786941580757e-05,
      "loss": 1.0928,
      "step": 17950
    },
    {
      "epoch": 6.0,
      "grad_norm": 0.7923240065574646,
      "learning_rate": 8.247422680412371e-05,
      "loss": 1.0587,
      "step": 18000
    },
    {
      "epoch": 6.016666666666667,
      "grad_norm": 0.5153055191040039,
      "learning_rate": 8.213058419243987e-05,
      "loss": 1.1051,
      "step": 18050
    },
    {
      "epoch": 6.033333333333333,
      "grad_norm": 4.1873979568481445,
      "learning_rate": 8.178694158075601e-05,
      "loss": 1.0017,
      "step": 18100
    },
    {
      "epoch": 6.05,
      "grad_norm": 1.771226406097412,
      "learning_rate": 8.144329896907217e-05,
      "loss": 1.0189,
      "step": 18150
    },
    {
      "epoch": 6.066666666666666,
      "grad_norm": 2.245094060897827,
      "learning_rate": 8.109965635738833e-05,
      "loss": 0.944,
      "step": 18200
    },
    {
      "epoch": 6.083333333333333,
      "grad_norm": 0.6575630903244019,
      "learning_rate": 8.075601374570447e-05,
      "loss": 0.9988,
      "step": 18250
    },
    {
      "epoch": 6.1,
      "grad_norm": 0.6289917230606079,
      "learning_rate": 8.041237113402063e-05,
      "loss": 1.0002,
      "step": 18300
    },
    {
      "epoch": 6.116666666666666,
      "grad_norm": 0.6476837396621704,
      "learning_rate": 8.006872852233678e-05,
      "loss": 1.1076,
      "step": 18350
    },
    {
      "epoch": 6.133333333333334,
      "grad_norm": 1.01421058177948,
      "learning_rate": 7.972508591065293e-05,
      "loss": 0.9417,
      "step": 18400
    },
    {
      "epoch": 6.15,
      "grad_norm": 0.7023722529411316,
      "learning_rate": 7.938144329896907e-05,
      "loss": 1.0045,
      "step": 18450
    },
    {
      "epoch": 6.166666666666667,
      "grad_norm": 0.6159483194351196,
      "learning_rate": 7.903780068728523e-05,
      "loss": 1.0066,
      "step": 18500
    },
    {
      "epoch": 6.183333333333334,
      "grad_norm": 0.5836666822433472,
      "learning_rate": 7.869415807560137e-05,
      "loss": 0.9678,
      "step": 18550
    },
    {
      "epoch": 6.2,
      "grad_norm": 0.5476958155632019,
      "learning_rate": 7.835051546391753e-05,
      "loss": 1.1136,
      "step": 18600
    },
    {
      "epoch": 6.216666666666667,
      "grad_norm": 0.9903271198272705,
      "learning_rate": 7.800687285223369e-05,
      "loss": 0.9969,
      "step": 18650
    },
    {
      "epoch": 6.233333333333333,
      "grad_norm": 0.8971186280250549,
      "learning_rate": 7.766323024054983e-05,
      "loss": 1.0417,
      "step": 18700
    },
    {
      "epoch": 6.25,
      "grad_norm": 0.5678415894508362,
      "learning_rate": 7.731958762886599e-05,
      "loss": 1.0775,
      "step": 18750
    },
    {
      "epoch": 6.266666666666667,
      "grad_norm": 0.8871099352836609,
      "learning_rate": 7.697594501718214e-05,
      "loss": 1.0646,
      "step": 18800
    },
    {
      "epoch": 6.283333333333333,
      "grad_norm": 0.6461491584777832,
      "learning_rate": 7.663230240549829e-05,
      "loss": 1.1229,
      "step": 18850
    },
    {
      "epoch": 6.3,
      "grad_norm": 0.5816228985786438,
      "learning_rate": 7.628865979381443e-05,
      "loss": 1.1516,
      "step": 18900
    },
    {
      "epoch": 6.316666666666666,
      "grad_norm": 0.657569169998169,
      "learning_rate": 7.594501718213059e-05,
      "loss": 1.0153,
      "step": 18950
    },
    {
      "epoch": 6.333333333333333,
      "grad_norm": 0.7326228618621826,
      "learning_rate": 7.560137457044673e-05,
      "loss": 1.0699,
      "step": 19000
    },
    {
      "epoch": 6.35,
      "grad_norm": 0.7023402452468872,
      "learning_rate": 7.525773195876289e-05,
      "loss": 1.1135,
      "step": 19050
    },
    {
      "epoch": 6.366666666666666,
      "grad_norm": 1.3805937767028809,
      "learning_rate": 7.491408934707905e-05,
      "loss": 1.0596,
      "step": 19100
    },
    {
      "epoch": 6.383333333333334,
      "grad_norm": 1.6076356172561646,
      "learning_rate": 7.457044673539519e-05,
      "loss": 0.9925,
      "step": 19150
    },
    {
      "epoch": 6.4,
      "grad_norm": 0.8273864388465881,
      "learning_rate": 7.422680412371135e-05,
      "loss": 1.1094,
      "step": 19200
    },
    {
      "epoch": 6.416666666666667,
      "grad_norm": 0.7253894805908203,
      "learning_rate": 7.38831615120275e-05,
      "loss": 1.0211,
      "step": 19250
    },
    {
      "epoch": 6.433333333333334,
      "grad_norm": 0.8636149168014526,
      "learning_rate": 7.353951890034365e-05,
      "loss": 1.0489,
      "step": 19300
    },
    {
      "epoch": 6.45,
      "grad_norm": 0.38087141513824463,
      "learning_rate": 7.319587628865979e-05,
      "loss": 1.0599,
      "step": 19350
    },
    {
      "epoch": 6.466666666666667,
      "grad_norm": 0.590444803237915,
      "learning_rate": 7.285223367697595e-05,
      "loss": 0.9531,
      "step": 19400
    },
    {
      "epoch": 6.483333333333333,
      "grad_norm": 0.6907832026481628,
      "learning_rate": 7.250859106529209e-05,
      "loss": 1.0788,
      "step": 19450
    },
    {
      "epoch": 6.5,
      "grad_norm": 0.664371907711029,
      "learning_rate": 7.216494845360825e-05,
      "loss": 0.9976,
      "step": 19500
    },
    {
      "epoch": 6.516666666666667,
      "grad_norm": 1.109654188156128,
      "learning_rate": 7.182130584192441e-05,
      "loss": 1.0625,
      "step": 19550
    },
    {
      "epoch": 6.533333333333333,
      "grad_norm": 0.5521193146705627,
      "learning_rate": 7.147766323024055e-05,
      "loss": 1.0932,
      "step": 19600
    },
    {
      "epoch": 6.55,
      "grad_norm": 0.8069755434989929,
      "learning_rate": 7.113402061855671e-05,
      "loss": 1.1212,
      "step": 19650
    },
    {
      "epoch": 6.566666666666666,
      "grad_norm": 0.8013365864753723,
      "learning_rate": 7.079037800687286e-05,
      "loss": 1.0516,
      "step": 19700
    },
    {
      "epoch": 6.583333333333333,
      "grad_norm": 0.8379924297332764,
      "learning_rate": 7.044673539518901e-05,
      "loss": 1.0842,
      "step": 19750
    },
    {
      "epoch": 6.6,
      "grad_norm": 1.1748422384262085,
      "learning_rate": 7.010309278350515e-05,
      "loss": 1.0351,
      "step": 19800
    },
    {
      "epoch": 6.616666666666667,
      "grad_norm": 0.9272463917732239,
      "learning_rate": 6.975945017182131e-05,
      "loss": 1.0605,
      "step": 19850
    },
    {
      "epoch": 6.633333333333333,
      "grad_norm": 0.7314144968986511,
      "learning_rate": 6.941580756013745e-05,
      "loss": 1.0804,
      "step": 19900
    },
    {
      "epoch": 6.65,
      "grad_norm": 0.7270752787590027,
      "learning_rate": 6.907216494845361e-05,
      "loss": 1.0608,
      "step": 19950
    },
    {
      "epoch": 6.666666666666667,
      "grad_norm": 1.2986708879470825,
      "learning_rate": 6.872852233676977e-05,
      "loss": 1.1216,
      "step": 20000
    },
    {
      "epoch": 6.683333333333334,
      "grad_norm": 0.6564404964447021,
      "learning_rate": 6.838487972508591e-05,
      "loss": 1.0057,
      "step": 20050
    },
    {
      "epoch": 6.7,
      "grad_norm": 0.6642833352088928,
      "learning_rate": 6.804123711340207e-05,
      "loss": 1.1442,
      "step": 20100
    },
    {
      "epoch": 6.716666666666667,
      "grad_norm": 0.8407028913497925,
      "learning_rate": 6.769759450171823e-05,
      "loss": 1.0858,
      "step": 20150
    },
    {
      "epoch": 6.733333333333333,
      "grad_norm": 1.951358437538147,
      "learning_rate": 6.735395189003437e-05,
      "loss": 1.0698,
      "step": 20200
    },
    {
      "epoch": 6.75,
      "grad_norm": 1.6499428749084473,
      "learning_rate": 6.701030927835051e-05,
      "loss": 0.9186,
      "step": 20250
    },
    {
      "epoch": 6.766666666666667,
      "grad_norm": 0.5222465395927429,
      "learning_rate": 6.666666666666667e-05,
      "loss": 1.0838,
      "step": 20300
    },
    {
      "epoch": 6.783333333333333,
      "grad_norm": 0.5331166386604309,
      "learning_rate": 6.632302405498281e-05,
      "loss": 0.9877,
      "step": 20350
    },
    {
      "epoch": 6.8,
      "grad_norm": 0.6848657131195068,
      "learning_rate": 6.597938144329897e-05,
      "loss": 1.0771,
      "step": 20400
    },
    {
      "epoch": 6.816666666666666,
      "grad_norm": 0.6943029165267944,
      "learning_rate": 6.563573883161513e-05,
      "loss": 1.0961,
      "step": 20450
    },
    {
      "epoch": 6.833333333333333,
      "grad_norm": 0.6320214867591858,
      "learning_rate": 6.529209621993127e-05,
      "loss": 1.1218,
      "step": 20500
    },
    {
      "epoch": 6.85,
      "grad_norm": 1.0814119577407837,
      "learning_rate": 6.494845360824743e-05,
      "loss": 1.0655,
      "step": 20550
    },
    {
      "epoch": 6.866666666666667,
      "grad_norm": 0.7253329753875732,
      "learning_rate": 6.460481099656359e-05,
      "loss": 1.015,
      "step": 20600
    },
    {
      "epoch": 6.883333333333333,
      "grad_norm": 1.3643252849578857,
      "learning_rate": 6.426116838487973e-05,
      "loss": 1.0667,
      "step": 20650
    },
    {
      "epoch": 6.9,
      "grad_norm": 0.7002884745597839,
      "learning_rate": 6.391752577319587e-05,
      "loss": 1.0169,
      "step": 20700
    },
    {
      "epoch": 6.916666666666667,
      "grad_norm": 0.9496894478797913,
      "learning_rate": 6.357388316151203e-05,
      "loss": 1.0185,
      "step": 20750
    },
    {
      "epoch": 6.933333333333334,
      "grad_norm": 0.9539006352424622,
      "learning_rate": 6.323024054982817e-05,
      "loss": 1.0076,
      "step": 20800
    },
    {
      "epoch": 6.95,
      "grad_norm": 0.5883692502975464,
      "learning_rate": 6.288659793814433e-05,
      "loss": 1.0313,
      "step": 20850
    },
    {
      "epoch": 6.966666666666667,
      "grad_norm": 0.8150976300239563,
      "learning_rate": 6.254295532646049e-05,
      "loss": 1.0948,
      "step": 20900
    },
    {
      "epoch": 6.983333333333333,
      "grad_norm": 4.302337169647217,
      "learning_rate": 6.219931271477663e-05,
      "loss": 1.064,
      "step": 20950
    },
    {
      "epoch": 7.0,
      "grad_norm": 0.9026161432266235,
      "learning_rate": 6.185567010309279e-05,
      "loss": 0.99,
      "step": 21000
    },
    {
      "epoch": 7.016666666666667,
      "grad_norm": 0.5749202370643616,
      "learning_rate": 6.151202749140895e-05,
      "loss": 0.9595,
      "step": 21050
    },
    {
      "epoch": 7.033333333333333,
      "grad_norm": 0.7084643840789795,
      "learning_rate": 6.116838487972509e-05,
      "loss": 1.0108,
      "step": 21100
    },
    {
      "epoch": 7.05,
      "grad_norm": 0.6709973216056824,
      "learning_rate": 6.0824742268041234e-05,
      "loss": 0.9843,
      "step": 21150
    },
    {
      "epoch": 7.066666666666666,
      "grad_norm": 3.234602212905884,
      "learning_rate": 6.0481099656357384e-05,
      "loss": 0.9884,
      "step": 21200
    },
    {
      "epoch": 7.083333333333333,
      "grad_norm": 0.8624099493026733,
      "learning_rate": 6.013745704467354e-05,
      "loss": 0.9791,
      "step": 21250
    },
    {
      "epoch": 7.1,
      "grad_norm": 0.6509665250778198,
      "learning_rate": 5.979381443298969e-05,
      "loss": 1.0453,
      "step": 21300
    },
    {
      "epoch": 7.116666666666666,
      "grad_norm": 0.652929961681366,
      "learning_rate": 5.945017182130584e-05,
      "loss": 0.954,
      "step": 21350
    },
    {
      "epoch": 7.133333333333334,
      "grad_norm": 0.4043656885623932,
      "learning_rate": 5.9106529209622e-05,
      "loss": 1.0681,
      "step": 21400
    },
    {
      "epoch": 7.15,
      "grad_norm": 0.7714486122131348,
      "learning_rate": 5.876288659793815e-05,
      "loss": 0.9792,
      "step": 21450
    },
    {
      "epoch": 7.166666666666667,
      "grad_norm": 0.9308910965919495,
      "learning_rate": 5.84192439862543e-05,
      "loss": 1.0231,
      "step": 21500
    },
    {
      "epoch": 7.183333333333334,
      "grad_norm": 0.6845197081565857,
      "learning_rate": 5.807560137457046e-05,
      "loss": 1.0834,
      "step": 21550
    },
    {
      "epoch": 7.2,
      "grad_norm": 1.318260669708252,
      "learning_rate": 5.7731958762886594e-05,
      "loss": 1.0153,
      "step": 21600
    },
    {
      "epoch": 7.216666666666667,
      "grad_norm": 0.8391659259796143,
      "learning_rate": 5.7388316151202745e-05,
      "loss": 1.1382,
      "step": 21650
    },
    {
      "epoch": 7.233333333333333,
      "grad_norm": 0.9411714673042297,
      "learning_rate": 5.70446735395189e-05,
      "loss": 0.973,
      "step": 21700
    },
    {
      "epoch": 7.25,
      "grad_norm": 1.3797119855880737,
      "learning_rate": 5.670103092783505e-05,
      "loss": 1.0472,
      "step": 21750
    },
    {
      "epoch": 7.266666666666667,
      "grad_norm": 0.758650004863739,
      "learning_rate": 5.63573883161512e-05,
      "loss": 1.0015,
      "step": 21800
    },
    {
      "epoch": 7.283333333333333,
      "grad_norm": 0.8684297204017639,
      "learning_rate": 5.601374570446736e-05,
      "loss": 1.0006,
      "step": 21850
    },
    {
      "epoch": 7.3,
      "grad_norm": 0.4680854082107544,
      "learning_rate": 5.567010309278351e-05,
      "loss": 1.0365,
      "step": 21900
    },
    {
      "epoch": 7.316666666666666,
      "grad_norm": 0.7657792568206787,
      "learning_rate": 5.532646048109966e-05,
      "loss": 0.9599,
      "step": 21950
    },
    {
      "epoch": 7.333333333333333,
      "grad_norm": 1.4043596982955933,
      "learning_rate": 5.498281786941582e-05,
      "loss": 1.0111,
      "step": 22000
    },
    {
      "epoch": 7.35,
      "grad_norm": 0.547640323638916,
      "learning_rate": 5.4639175257731955e-05,
      "loss": 0.9329,
      "step": 22050
    },
    {
      "epoch": 7.366666666666666,
      "grad_norm": 0.6470785140991211,
      "learning_rate": 5.4295532646048105e-05,
      "loss": 1.0192,
      "step": 22100
    },
    {
      "epoch": 7.383333333333334,
      "grad_norm": 0.6982589960098267,
      "learning_rate": 5.395189003436426e-05,
      "loss": 1.027,
      "step": 22150
    },
    {
      "epoch": 7.4,
      "grad_norm": 0.8265833258628845,
      "learning_rate": 5.360824742268041e-05,
      "loss": 1.0338,
      "step": 22200
    },
    {
      "epoch": 7.416666666666667,
      "grad_norm": 0.7468980550765991,
      "learning_rate": 5.326460481099656e-05,
      "loss": 0.9929,
      "step": 22250
    },
    {
      "epoch": 7.433333333333334,
      "grad_norm": 0.9228223562240601,
      "learning_rate": 5.292096219931272e-05,
      "loss": 0.972,
      "step": 22300
    },
    {
      "epoch": 7.45,
      "grad_norm": 1.084977626800537,
      "learning_rate": 5.257731958762887e-05,
      "loss": 1.0734,
      "step": 22350
    },
    {
      "epoch": 7.466666666666667,
      "grad_norm": 0.5493879914283752,
      "learning_rate": 5.223367697594502e-05,
      "loss": 1.0168,
      "step": 22400
    },
    {
      "epoch": 7.483333333333333,
      "grad_norm": 1.4838836193084717,
      "learning_rate": 5.189003436426118e-05,
      "loss": 1.0411,
      "step": 22450
    },
    {
      "epoch": 7.5,
      "grad_norm": 0.8153328895568848,
      "learning_rate": 5.1546391752577315e-05,
      "loss": 1.0628,
      "step": 22500
    },
    {
      "epoch": 7.516666666666667,
      "grad_norm": 0.7005701065063477,
      "learning_rate": 5.1202749140893466e-05,
      "loss": 1.0106,
      "step": 22550
    },
    {
      "epoch": 7.533333333333333,
      "grad_norm": 0.7364360094070435,
      "learning_rate": 5.085910652920962e-05,
      "loss": 1.0048,
      "step": 22600
    },
    {
      "epoch": 7.55,
      "grad_norm": 1.1714566946029663,
      "learning_rate": 5.051546391752577e-05,
      "loss": 1.0571,
      "step": 22650
    },
    {
      "epoch": 7.566666666666666,
      "grad_norm": 0.733133852481842,
      "learning_rate": 5.0171821305841924e-05,
      "loss": 1.0018,
      "step": 22700
    },
    {
      "epoch": 7.583333333333333,
      "grad_norm": 1.0561046600341797,
      "learning_rate": 4.982817869415808e-05,
      "loss": 1.1302,
      "step": 22750
    },
    {
      "epoch": 7.6,
      "grad_norm": 1.5667526721954346,
      "learning_rate": 4.948453608247423e-05,
      "loss": 1.0065,
      "step": 22800
    },
    {
      "epoch": 7.616666666666667,
      "grad_norm": 0.6200568675994873,
      "learning_rate": 4.9140893470790375e-05,
      "loss": 1.0723,
      "step": 22850
    },
    {
      "epoch": 7.633333333333333,
      "grad_norm": 0.8505762219429016,
      "learning_rate": 4.879725085910653e-05,
      "loss": 1.0675,
      "step": 22900
    },
    {
      "epoch": 7.65,
      "grad_norm": 0.6318209767341614,
      "learning_rate": 4.845360824742268e-05,
      "loss": 1.0374,
      "step": 22950
    },
    {
      "epoch": 7.666666666666667,
      "grad_norm": 0.56381756067276,
      "learning_rate": 4.810996563573883e-05,
      "loss": 1.0444,
      "step": 23000
    },
    {
      "epoch": 7.683333333333334,
      "grad_norm": 0.6808928847312927,
      "learning_rate": 4.776632302405499e-05,
      "loss": 1.0186,
      "step": 23050
    },
    {
      "epoch": 7.7,
      "grad_norm": 0.6523030996322632,
      "learning_rate": 4.7422680412371134e-05,
      "loss": 0.9656,
      "step": 23100
    },
    {
      "epoch": 7.716666666666667,
      "grad_norm": 1.4315985441207886,
      "learning_rate": 4.7079037800687284e-05,
      "loss": 0.9868,
      "step": 23150
    },
    {
      "epoch": 7.733333333333333,
      "grad_norm": 0.6577559113502502,
      "learning_rate": 4.673539518900344e-05,
      "loss": 1.0362,
      "step": 23200
    },
    {
      "epoch": 7.75,
      "grad_norm": 0.9222854375839233,
      "learning_rate": 4.639175257731959e-05,
      "loss": 0.974,
      "step": 23250
    },
    {
      "epoch": 7.766666666666667,
      "grad_norm": 0.9747443199157715,
      "learning_rate": 4.6048109965635736e-05,
      "loss": 0.9652,
      "step": 23300
    },
    {
      "epoch": 7.783333333333333,
      "grad_norm": 1.6141554117202759,
      "learning_rate": 4.570446735395189e-05,
      "loss": 1.062,
      "step": 23350
    },
    {
      "epoch": 7.8,
      "grad_norm": 1.0754148960113525,
      "learning_rate": 4.536082474226804e-05,
      "loss": 0.9777,
      "step": 23400
    },
    {
      "epoch": 7.816666666666666,
      "grad_norm": 0.6452747583389282,
      "learning_rate": 4.5017182130584194e-05,
      "loss": 1.0128,
      "step": 23450
    },
    {
      "epoch": 7.833333333333333,
      "grad_norm": 1.4306477308273315,
      "learning_rate": 4.467353951890035e-05,
      "loss": 1.0008,
      "step": 23500
    },
    {
      "epoch": 7.85,
      "grad_norm": 0.6899235248565674,
      "learning_rate": 4.4329896907216494e-05,
      "loss": 0.9544,
      "step": 23550
    },
    {
      "epoch": 7.866666666666667,
      "grad_norm": 1.362906575202942,
      "learning_rate": 4.3986254295532645e-05,
      "loss": 1.0268,
      "step": 23600
    },
    {
      "epoch": 7.883333333333333,
      "grad_norm": 0.6774058938026428,
      "learning_rate": 4.36426116838488e-05,
      "loss": 0.9827,
      "step": 23650
    },
    {
      "epoch": 7.9,
      "grad_norm": 0.9651318788528442,
      "learning_rate": 4.329896907216495e-05,
      "loss": 1.0429,
      "step": 23700
    },
    {
      "epoch": 7.916666666666667,
      "grad_norm": 4.0271992683410645,
      "learning_rate": 4.2955326460481096e-05,
      "loss": 0.8895,
      "step": 23750
    },
    {
      "epoch": 7.933333333333334,
      "grad_norm": 0.9638205766677856,
      "learning_rate": 4.261168384879725e-05,
      "loss": 1.0759,
      "step": 23800
    },
    {
      "epoch": 7.95,
      "grad_norm": 0.7905473113059998,
      "learning_rate": 4.2268041237113404e-05,
      "loss": 0.9886,
      "step": 23850
    },
    {
      "epoch": 7.966666666666667,
      "grad_norm": 0.9565443396568298,
      "learning_rate": 4.1924398625429554e-05,
      "loss": 1.05,
      "step": 23900
    },
    {
      "epoch": 7.983333333333333,
      "grad_norm": 0.6024625301361084,
      "learning_rate": 4.158075601374571e-05,
      "loss": 1.0058,
      "step": 23950
    },
    {
      "epoch": 8.0,
      "grad_norm": 0.7948227524757385,
      "learning_rate": 4.1237113402061855e-05,
      "loss": 0.972,
      "step": 24000
    },
    {
      "epoch": 8.016666666666667,
      "grad_norm": 0.5643396377563477,
      "learning_rate": 4.0893470790378005e-05,
      "loss": 0.9853,
      "step": 24050
    },
    {
      "epoch": 8.033333333333333,
      "grad_norm": 0.39677903056144714,
      "learning_rate": 4.054982817869416e-05,
      "loss": 0.9544,
      "step": 24100
    },
    {
      "epoch": 8.05,
      "grad_norm": 1.5203958749771118,
      "learning_rate": 4.020618556701031e-05,
      "loss": 0.9633,
      "step": 24150
    },
    {
      "epoch": 8.066666666666666,
      "grad_norm": 0.6920294165611267,
      "learning_rate": 3.9862542955326463e-05,
      "loss": 0.9661,
      "step": 24200
    },
    {
      "epoch": 8.083333333333334,
      "grad_norm": 0.5611373782157898,
      "learning_rate": 3.9518900343642614e-05,
      "loss": 1.0492,
      "step": 24250
    },
    {
      "epoch": 8.1,
      "grad_norm": 0.5949259996414185,
      "learning_rate": 3.9175257731958764e-05,
      "loss": 0.9944,
      "step": 24300
    },
    {
      "epoch": 8.116666666666667,
      "grad_norm": 0.48710188269615173,
      "learning_rate": 3.8831615120274915e-05,
      "loss": 1.0485,
      "step": 24350
    },
    {
      "epoch": 8.133333333333333,
      "grad_norm": 1.1254422664642334,
      "learning_rate": 3.848797250859107e-05,
      "loss": 1.0174,
      "step": 24400
    },
    {
      "epoch": 8.15,
      "grad_norm": 0.7493789792060852,
      "learning_rate": 3.8144329896907216e-05,
      "loss": 0.9947,
      "step": 24450
    },
    {
      "epoch": 8.166666666666666,
      "grad_norm": 0.5660680532455444,
      "learning_rate": 3.7800687285223366e-05,
      "loss": 0.898,
      "step": 24500
    },
    {
      "epoch": 8.183333333333334,
      "grad_norm": 0.5145279169082642,
      "learning_rate": 3.745704467353952e-05,
      "loss": 0.9005,
      "step": 24550
    },
    {
      "epoch": 8.2,
      "grad_norm": 0.4746307134628296,
      "learning_rate": 3.7113402061855674e-05,
      "loss": 1.0057,
      "step": 24600
    },
    {
      "epoch": 8.216666666666667,
      "grad_norm": 1.518799901008606,
      "learning_rate": 3.6769759450171824e-05,
      "loss": 1.01,
      "step": 24650
    },
    {
      "epoch": 8.233333333333333,
      "grad_norm": 1.580295205116272,
      "learning_rate": 3.6426116838487974e-05,
      "loss": 0.9867,
      "step": 24700
    },
    {
      "epoch": 8.25,
      "grad_norm": 1.426261067390442,
      "learning_rate": 3.6082474226804125e-05,
      "loss": 1.0289,
      "step": 24750
    },
    {
      "epoch": 8.266666666666667,
      "grad_norm": 0.5122865438461304,
      "learning_rate": 3.5738831615120275e-05,
      "loss": 0.9664,
      "step": 24800
    },
    {
      "epoch": 8.283333333333333,
      "grad_norm": 0.5623199939727783,
      "learning_rate": 3.539518900343643e-05,
      "loss": 0.9833,
      "step": 24850
    },
    {
      "epoch": 8.3,
      "grad_norm": 0.6022099852561951,
      "learning_rate": 3.5051546391752576e-05,
      "loss": 1.0304,
      "step": 24900
    },
    {
      "epoch": 8.316666666666666,
      "grad_norm": 0.7057931423187256,
      "learning_rate": 3.4707903780068726e-05,
      "loss": 0.9882,
      "step": 24950
    },
    {
      "epoch": 8.333333333333334,
      "grad_norm": 0.821458101272583,
      "learning_rate": 3.4364261168384884e-05,
      "loss": 1.0559,
      "step": 25000
    },
    {
      "epoch": 8.35,
      "grad_norm": 0.48684990406036377,
      "learning_rate": 3.4020618556701034e-05,
      "loss": 0.9124,
      "step": 25050
    },
    {
      "epoch": 8.366666666666667,
      "grad_norm": 0.7286893129348755,
      "learning_rate": 3.3676975945017185e-05,
      "loss": 0.9949,
      "step": 25100
    },
    {
      "epoch": 8.383333333333333,
      "grad_norm": 1.4235578775405884,
      "learning_rate": 3.3333333333333335e-05,
      "loss": 0.9574,
      "step": 25150
    },
    {
      "epoch": 8.4,
      "grad_norm": 0.7874222993850708,
      "learning_rate": 3.2989690721649485e-05,
      "loss": 0.9418,
      "step": 25200
    },
    {
      "epoch": 8.416666666666666,
      "grad_norm": 1.0503437519073486,
      "learning_rate": 3.2646048109965636e-05,
      "loss": 0.9669,
      "step": 25250
    },
    {
      "epoch": 8.433333333333334,
      "grad_norm": 0.8688302040100098,
      "learning_rate": 3.230240549828179e-05,
      "loss": 0.9981,
      "step": 25300
    },
    {
      "epoch": 8.45,
      "grad_norm": 0.9658960700035095,
      "learning_rate": 3.1958762886597937e-05,
      "loss": 1.0133,
      "step": 25350
    },
    {
      "epoch": 8.466666666666667,
      "grad_norm": 1.0425279140472412,
      "learning_rate": 3.161512027491409e-05,
      "loss": 0.9939,
      "step": 25400
    },
    {
      "epoch": 8.483333333333333,
      "grad_norm": 0.47720375657081604,
      "learning_rate": 3.1271477663230244e-05,
      "loss": 1.0249,
      "step": 25450
    },
    {
      "epoch": 8.5,
      "grad_norm": 0.9889461398124695,
      "learning_rate": 3.0927835051546395e-05,
      "loss": 0.9437,
      "step": 25500
    },
    {
      "epoch": 8.516666666666667,
      "grad_norm": 0.8626887202262878,
      "learning_rate": 3.0584192439862545e-05,
      "loss": 0.9158,
      "step": 25550
    },
    {
      "epoch": 8.533333333333333,
      "grad_norm": 0.6116783022880554,
      "learning_rate": 3.0240549828178692e-05,
      "loss": 0.9954,
      "step": 25600
    },
    {
      "epoch": 8.55,
      "grad_norm": 0.8083984851837158,
      "learning_rate": 2.9896907216494846e-05,
      "loss": 0.9235,
      "step": 25650
    },
    {
      "epoch": 8.566666666666666,
      "grad_norm": 0.8703007698059082,
      "learning_rate": 2.9553264604811e-05,
      "loss": 0.9819,
      "step": 25700
    },
    {
      "epoch": 8.583333333333334,
      "grad_norm": 0.6467596292495728,
      "learning_rate": 2.920962199312715e-05,
      "loss": 1.0382,
      "step": 25750
    },
    {
      "epoch": 8.6,
      "grad_norm": 1.6990723609924316,
      "learning_rate": 2.8865979381443297e-05,
      "loss": 1.0138,
      "step": 25800
    },
    {
      "epoch": 8.616666666666667,
      "grad_norm": 1.0523899793624878,
      "learning_rate": 2.852233676975945e-05,
      "loss": 0.9662,
      "step": 25850
    },
    {
      "epoch": 8.633333333333333,
      "grad_norm": 0.7499775290489197,
      "learning_rate": 2.81786941580756e-05,
      "loss": 0.9618,
      "step": 25900
    },
    {
      "epoch": 8.65,
      "grad_norm": 0.9114580750465393,
      "learning_rate": 2.7835051546391755e-05,
      "loss": 1.0695,
      "step": 25950
    },
    {
      "epoch": 8.666666666666666,
      "grad_norm": 0.6623262763023376,
      "learning_rate": 2.749140893470791e-05,
      "loss": 1.0648,
      "step": 26000
    },
    {
      "epoch": 8.683333333333334,
      "grad_norm": 0.5628896355628967,
      "learning_rate": 2.7147766323024053e-05,
      "loss": 1.0236,
      "step": 26050
    },
    {
      "epoch": 8.7,
      "grad_norm": 1.0811147689819336,
      "learning_rate": 2.6804123711340206e-05,
      "loss": 0.9705,
      "step": 26100
    },
    {
      "epoch": 8.716666666666667,
      "grad_norm": 0.5769674777984619,
      "learning_rate": 2.646048109965636e-05,
      "loss": 0.9157,
      "step": 26150
    },
    {
      "epoch": 8.733333333333333,
      "grad_norm": 0.8711725473403931,
      "learning_rate": 2.611683848797251e-05,
      "loss": 0.9226,
      "step": 26200
    },
    {
      "epoch": 8.75,
      "grad_norm": 0.6811186075210571,
      "learning_rate": 2.5773195876288658e-05,
      "loss": 1.0223,
      "step": 26250
    },
    {
      "epoch": 8.766666666666667,
      "grad_norm": 0.6000105142593384,
      "learning_rate": 2.542955326460481e-05,
      "loss": 0.8701,
      "step": 26300
    },
    {
      "epoch": 8.783333333333333,
      "grad_norm": 0.8887050151824951,
      "learning_rate": 2.5085910652920962e-05,
      "loss": 1.1024,
      "step": 26350
    },
    {
      "epoch": 8.8,
      "grad_norm": 0.5825205445289612,
      "learning_rate": 2.4742268041237116e-05,
      "loss": 0.9907,
      "step": 26400
    },
    {
      "epoch": 8.816666666666666,
      "grad_norm": 0.4143356382846832,
      "learning_rate": 2.4398625429553266e-05,
      "loss": 0.9195,
      "step": 26450
    },
    {
      "epoch": 8.833333333333334,
      "grad_norm": 1.387588620185852,
      "learning_rate": 2.4054982817869417e-05,
      "loss": 0.9521,
      "step": 26500
    },
    {
      "epoch": 8.85,
      "grad_norm": 0.6344162225723267,
      "learning_rate": 2.3711340206185567e-05,
      "loss": 0.9912,
      "step": 26550
    },
    {
      "epoch": 8.866666666666667,
      "grad_norm": 0.8724848031997681,
      "learning_rate": 2.336769759450172e-05,
      "loss": 0.9892,
      "step": 26600
    },
    {
      "epoch": 8.883333333333333,
      "grad_norm": 0.9904910922050476,
      "learning_rate": 2.3024054982817868e-05,
      "loss": 0.9962,
      "step": 26650
    },
    {
      "epoch": 8.9,
      "grad_norm": 2.3210954666137695,
      "learning_rate": 2.268041237113402e-05,
      "loss": 0.9601,
      "step": 26700
    },
    {
      "epoch": 8.916666666666666,
      "grad_norm": 1.021145224571228,
      "learning_rate": 2.2336769759450175e-05,
      "loss": 0.9117,
      "step": 26750
    },
    {
      "epoch": 8.933333333333334,
      "grad_norm": 0.7511757016181946,
      "learning_rate": 2.1993127147766322e-05,
      "loss": 1.0068,
      "step": 26800
    },
    {
      "epoch": 8.95,
      "grad_norm": 0.8002687096595764,
      "learning_rate": 2.1649484536082476e-05,
      "loss": 0.929,
      "step": 26850
    },
    {
      "epoch": 8.966666666666667,
      "grad_norm": 0.8944467306137085,
      "learning_rate": 2.1305841924398627e-05,
      "loss": 1.0008,
      "step": 26900
    },
    {
      "epoch": 8.983333333333333,
      "grad_norm": 0.8826352953910828,
      "learning_rate": 2.0962199312714777e-05,
      "loss": 1.0563,
      "step": 26950
    },
    {
      "epoch": 9.0,
      "grad_norm": 2.706380605697632,
      "learning_rate": 2.0618556701030927e-05,
      "loss": 1.0346,
      "step": 27000
    },
    {
      "epoch": 9.016666666666667,
      "grad_norm": 0.5973635911941528,
      "learning_rate": 2.027491408934708e-05,
      "loss": 1.0413,
      "step": 27050
    },
    {
      "epoch": 9.033333333333333,
      "grad_norm": 0.4582461714744568,
      "learning_rate": 1.9931271477663232e-05,
      "loss": 0.9623,
      "step": 27100
    },
    {
      "epoch": 9.05,
      "grad_norm": 0.5408592820167542,
      "learning_rate": 1.9587628865979382e-05,
      "loss": 0.9661,
      "step": 27150
    },
    {
      "epoch": 9.066666666666666,
      "grad_norm": 0.8568602204322815,
      "learning_rate": 1.9243986254295536e-05,
      "loss": 0.999,
      "step": 27200
    },
    {
      "epoch": 9.083333333333334,
      "grad_norm": 0.7831098437309265,
      "learning_rate": 1.8900343642611683e-05,
      "loss": 0.9243,
      "step": 27250
    },
    {
      "epoch": 9.1,
      "grad_norm": 0.594505250453949,
      "learning_rate": 1.8556701030927837e-05,
      "loss": 0.9998,
      "step": 27300
    },
    {
      "epoch": 9.116666666666667,
      "grad_norm": 0.8232148885726929,
      "learning_rate": 1.8213058419243987e-05,
      "loss": 1.0675,
      "step": 27350
    },
    {
      "epoch": 9.133333333333333,
      "grad_norm": 0.5318227410316467,
      "learning_rate": 1.7869415807560138e-05,
      "loss": 1.0039,
      "step": 27400
    },
    {
      "epoch": 9.15,
      "grad_norm": 0.5255089402198792,
      "learning_rate": 1.7525773195876288e-05,
      "loss": 1.0117,
      "step": 27450
    },
    {
      "epoch": 9.166666666666666,
      "grad_norm": 0.7934473156929016,
      "learning_rate": 1.7182130584192442e-05,
      "loss": 0.9183,
      "step": 27500
    },
    {
      "epoch": 9.183333333333334,
      "grad_norm": 0.6562435030937195,
      "learning_rate": 1.6838487972508592e-05,
      "loss": 0.9849,
      "step": 27550
    },
    {
      "epoch": 9.2,
      "grad_norm": 0.5889375805854797,
      "learning_rate": 1.6494845360824743e-05,
      "loss": 0.878,
      "step": 27600
    },
    {
      "epoch": 9.216666666666667,
      "grad_norm": 0.48469871282577515,
      "learning_rate": 1.6151202749140896e-05,
      "loss": 0.9834,
      "step": 27650
    },
    {
      "epoch": 9.233333333333333,
      "grad_norm": 0.6879616975784302,
      "learning_rate": 1.5807560137457044e-05,
      "loss": 0.9536,
      "step": 27700
    },
    {
      "epoch": 9.25,
      "grad_norm": 0.5672629475593567,
      "learning_rate": 1.5463917525773197e-05,
      "loss": 0.9195,
      "step": 27750
    },
    {
      "epoch": 9.266666666666667,
      "grad_norm": 1.0549800395965576,
      "learning_rate": 1.5120274914089346e-05,
      "loss": 0.9915,
      "step": 27800
    },
    {
      "epoch": 9.283333333333333,
      "grad_norm": 0.9544962048530579,
      "learning_rate": 1.47766323024055e-05,
      "loss": 0.9203,
      "step": 27850
    },
    {
      "epoch": 9.3,
      "grad_norm": 0.8558393120765686,
      "learning_rate": 1.4432989690721649e-05,
      "loss": 0.9449,
      "step": 27900
    },
    {
      "epoch": 9.316666666666666,
      "grad_norm": 0.5685266256332397,
      "learning_rate": 1.40893470790378e-05,
      "loss": 0.8951,
      "step": 27950
    },
    {
      "epoch": 9.333333333333334,
      "grad_norm": 0.6663776636123657,
      "learning_rate": 1.3745704467353954e-05,
      "loss": 0.9179,
      "step": 28000
    },
    {
      "epoch": 9.35,
      "grad_norm": 0.728217363357544,
      "learning_rate": 1.3402061855670103e-05,
      "loss": 1.0121,
      "step": 28050
    },
    {
      "epoch": 9.366666666666667,
      "grad_norm": 0.6769915223121643,
      "learning_rate": 1.3058419243986255e-05,
      "loss": 0.9431,
      "step": 28100
    },
    {
      "epoch": 9.383333333333333,
      "grad_norm": 0.6164215207099915,
      "learning_rate": 1.2714776632302406e-05,
      "loss": 1.0328,
      "step": 28150
    },
    {
      "epoch": 9.4,
      "grad_norm": 1.2426998615264893,
      "learning_rate": 1.2371134020618558e-05,
      "loss": 0.9154,
      "step": 28200
    },
    {
      "epoch": 9.416666666666666,
      "grad_norm": 0.944620668888092,
      "learning_rate": 1.2027491408934708e-05,
      "loss": 1.0336,
      "step": 28250
    },
    {
      "epoch": 9.433333333333334,
      "grad_norm": 0.7364893555641174,
      "learning_rate": 1.168384879725086e-05,
      "loss": 0.969,
      "step": 28300
    },
    {
      "epoch": 9.45,
      "grad_norm": 0.7754094004631042,
      "learning_rate": 1.134020618556701e-05,
      "loss": 0.858,
      "step": 28350
    },
    {
      "epoch": 9.466666666666667,
      "grad_norm": 0.5162155032157898,
      "learning_rate": 1.0996563573883161e-05,
      "loss": 0.8831,
      "step": 28400
    },
    {
      "epoch": 9.483333333333333,
      "grad_norm": 0.775831937789917,
      "learning_rate": 1.0652920962199313e-05,
      "loss": 0.9278,
      "step": 28450
    },
    {
      "epoch": 9.5,
      "grad_norm": 1.3590492010116577,
      "learning_rate": 1.0309278350515464e-05,
      "loss": 1.0363,
      "step": 28500
    },
    {
      "epoch": 9.516666666666667,
      "grad_norm": 2.069061517715454,
      "learning_rate": 9.965635738831616e-06,
      "loss": 1.0542,
      "step": 28550
    },
    {
      "epoch": 9.533333333333333,
      "grad_norm": 1.452987551689148,
      "learning_rate": 9.621993127147768e-06,
      "loss": 0.9723,
      "step": 28600
    },
    {
      "epoch": 9.55,
      "grad_norm": 1.3216198682785034,
      "learning_rate": 9.278350515463918e-06,
      "loss": 0.8506,
      "step": 28650
    },
    {
      "epoch": 9.566666666666666,
      "grad_norm": 0.7446334958076477,
      "learning_rate": 8.934707903780069e-06,
      "loss": 0.9407,
      "step": 28700
    },
    {
      "epoch": 9.583333333333334,
      "grad_norm": 0.736117959022522,
      "learning_rate": 8.591065292096221e-06,
      "loss": 0.8849,
      "step": 28750
    },
    {
      "epoch": 9.6,
      "grad_norm": 0.7553805708885193,
      "learning_rate": 8.247422680412371e-06,
      "loss": 0.9555,
      "step": 28800
    },
    {
      "epoch": 9.616666666666667,
      "grad_norm": 0.7462998628616333,
      "learning_rate": 7.903780068728522e-06,
      "loss": 0.9631,
      "step": 28850
    },
    {
      "epoch": 9.633333333333333,
      "grad_norm": 0.793249249458313,
      "learning_rate": 7.560137457044673e-06,
      "loss": 0.8779,
      "step": 28900
    },
    {
      "epoch": 9.65,
      "grad_norm": 0.5997868180274963,
      "learning_rate": 7.216494845360824e-06,
      "loss": 0.9362,
      "step": 28950
    },
    {
      "epoch": 9.666666666666666,
      "grad_norm": 1.3313640356063843,
      "learning_rate": 6.872852233676977e-06,
      "loss": 0.9633,
      "step": 29000
    },
    {
      "epoch": 9.683333333333334,
      "grad_norm": 0.5774540901184082,
      "learning_rate": 6.529209621993128e-06,
      "loss": 0.9202,
      "step": 29050
    },
    {
      "epoch": 9.7,
      "grad_norm": 0.6301815509796143,
      "learning_rate": 6.185567010309279e-06,
      "loss": 1.0216,
      "step": 29100
    },
    {
      "epoch": 9.716666666666667,
      "grad_norm": 0.6663414835929871,
      "learning_rate": 5.84192439862543e-06,
      "loss": 0.9769,
      "step": 29150
    },
    {
      "epoch": 9.733333333333333,
      "grad_norm": 1.1660521030426025,
      "learning_rate": 5.498281786941581e-06,
      "loss": 0.9735,
      "step": 29200
    },
    {
      "epoch": 9.75,
      "grad_norm": 0.5888983607292175,
      "learning_rate": 5.154639175257732e-06,
      "loss": 0.9855,
      "step": 29250
    },
    {
      "epoch": 9.766666666666667,
      "grad_norm": 1.4610614776611328,
      "learning_rate": 4.810996563573884e-06,
      "loss": 0.9089,
      "step": 29300
    },
    {
      "epoch": 9.783333333333333,
      "grad_norm": 1.067167043685913,
      "learning_rate": 4.467353951890034e-06,
      "loss": 1.0669,
      "step": 29350
    },
    {
      "epoch": 9.8,
      "grad_norm": 0.7347313165664673,
      "learning_rate": 4.123711340206186e-06,
      "loss": 0.9283,
      "step": 29400
    },
    {
      "epoch": 9.816666666666666,
      "grad_norm": 0.7439813613891602,
      "learning_rate": 3.7800687285223365e-06,
      "loss": 0.9557,
      "step": 29450
    },
    {
      "epoch": 9.833333333333334,
      "grad_norm": 0.7244111895561218,
      "learning_rate": 3.4364261168384886e-06,
      "loss": 1.0958,
      "step": 29500
    },
    {
      "epoch": 9.85,
      "grad_norm": 0.7741409540176392,
      "learning_rate": 3.0927835051546395e-06,
      "loss": 1.0236,
      "step": 29550
    },
    {
      "epoch": 9.866666666666667,
      "grad_norm": 0.7579082250595093,
      "learning_rate": 2.7491408934707903e-06,
      "loss": 1.0119,
      "step": 29600
    },
    {
      "epoch": 9.883333333333333,
      "grad_norm": 1.1897900104522705,
      "learning_rate": 2.405498281786942e-06,
      "loss": 0.9537,
      "step": 29650
    },
    {
      "epoch": 9.9,
      "grad_norm": 1.5671799182891846,
      "learning_rate": 2.061855670103093e-06,
      "loss": 0.8084,
      "step": 29700
    },
    {
      "epoch": 9.916666666666666,
      "grad_norm": 0.9159346222877502,
      "learning_rate": 1.7182130584192443e-06,
      "loss": 0.9658,
      "step": 29750
    },
    {
      "epoch": 9.933333333333334,
      "grad_norm": 0.8917185068130493,
      "learning_rate": 1.3745704467353952e-06,
      "loss": 0.9657,
      "step": 29800
    },
    {
      "epoch": 9.95,
      "grad_norm": 0.7039968371391296,
      "learning_rate": 1.0309278350515464e-06,
      "loss": 0.8796,
      "step": 29850
    },
    {
      "epoch": 9.966666666666667,
      "grad_norm": 0.7210199236869812,
      "learning_rate": 6.872852233676976e-07,
      "loss": 0.9831,
      "step": 29900
    }
  ],
  "logging_steps": 50,
  "max_steps": 30000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 30,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.2991683418214892e+18,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
