{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.998423265592151,
  "eval_steps": 500,
  "global_step": 34230,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.004379817799579537,
      "grad_norm": 3.435281991958618,
      "learning_rate": 9.72762645914397e-06,
      "loss": 2.8457,
      "step": 50
    },
    {
      "epoch": 0.008759635599159075,
      "grad_norm": 4.571619033813477,
      "learning_rate": 1.945525291828794e-05,
      "loss": 2.3745,
      "step": 100
    },
    {
      "epoch": 0.013139453398738612,
      "grad_norm": 3.028986692428589,
      "learning_rate": 2.918287937743191e-05,
      "loss": 1.9636,
      "step": 150
    },
    {
      "epoch": 0.01751927119831815,
      "grad_norm": 2.485875129699707,
      "learning_rate": 3.891050583657588e-05,
      "loss": 1.7477,
      "step": 200
    },
    {
      "epoch": 0.02189908899789769,
      "grad_norm": 2.353454828262329,
      "learning_rate": 4.863813229571985e-05,
      "loss": 1.7849,
      "step": 250
    },
    {
      "epoch": 0.026278906797477224,
      "grad_norm": 2.5581748485565186,
      "learning_rate": 5.836575875486382e-05,
      "loss": 1.801,
      "step": 300
    },
    {
      "epoch": 0.030658724597056763,
      "grad_norm": 2.555314302444458,
      "learning_rate": 6.809338521400779e-05,
      "loss": 1.7632,
      "step": 350
    },
    {
      "epoch": 0.0350385423966363,
      "grad_norm": 2.5583627223968506,
      "learning_rate": 7.782101167315176e-05,
      "loss": 1.7242,
      "step": 400
    },
    {
      "epoch": 0.039418360196215835,
      "grad_norm": 2.4423158168792725,
      "learning_rate": 8.754863813229573e-05,
      "loss": 1.7042,
      "step": 450
    },
    {
      "epoch": 0.04379817799579538,
      "grad_norm": 2.5554559230804443,
      "learning_rate": 9.72762645914397e-05,
      "loss": 1.7302,
      "step": 500
    },
    {
      "epoch": 0.04817799579537491,
      "grad_norm": 2.0983660221099854,
      "learning_rate": 0.00010700389105058367,
      "loss": 1.6995,
      "step": 550
    },
    {
      "epoch": 0.05255781359495445,
      "grad_norm": 1.977146029472351,
      "learning_rate": 0.00011673151750972763,
      "loss": 1.7539,
      "step": 600
    },
    {
      "epoch": 0.056937631394533984,
      "grad_norm": 2.669210433959961,
      "learning_rate": 0.0001264591439688716,
      "loss": 1.6493,
      "step": 650
    },
    {
      "epoch": 0.06131744919411353,
      "grad_norm": 2.1884896755218506,
      "learning_rate": 0.00013618677042801557,
      "loss": 1.7456,
      "step": 700
    },
    {
      "epoch": 0.06569726699369306,
      "grad_norm": 1.767849087715149,
      "learning_rate": 0.00014591439688715953,
      "loss": 1.7829,
      "step": 750
    },
    {
      "epoch": 0.0700770847932726,
      "grad_norm": 2.116128444671631,
      "learning_rate": 0.0001556420233463035,
      "loss": 1.7758,
      "step": 800
    },
    {
      "epoch": 0.07445690259285213,
      "grad_norm": 2.111527681350708,
      "learning_rate": 0.00016536964980544747,
      "loss": 1.6515,
      "step": 850
    },
    {
      "epoch": 0.07883672039243167,
      "grad_norm": 1.8119914531707764,
      "learning_rate": 0.00017509727626459145,
      "loss": 1.6988,
      "step": 900
    },
    {
      "epoch": 0.08321653819201122,
      "grad_norm": 2.295483112335205,
      "learning_rate": 0.00018482490272373543,
      "loss": 1.6806,
      "step": 950
    },
    {
      "epoch": 0.08759635599159075,
      "grad_norm": 1.607660174369812,
      "learning_rate": 0.0001945525291828794,
      "loss": 1.7381,
      "step": 1000
    },
    {
      "epoch": 0.09197617379117029,
      "grad_norm": 2.243065595626831,
      "learning_rate": 0.00019986754966887417,
      "loss": 1.6559,
      "step": 1050
    },
    {
      "epoch": 0.09635599159074983,
      "grad_norm": 2.1004507541656494,
      "learning_rate": 0.00019956652618904276,
      "loss": 1.6905,
      "step": 1100
    },
    {
      "epoch": 0.10073580939032936,
      "grad_norm": 1.995080828666687,
      "learning_rate": 0.00019926550270921135,
      "loss": 1.6681,
      "step": 1150
    },
    {
      "epoch": 0.1051156271899089,
      "grad_norm": 1.8975276947021484,
      "learning_rate": 0.0001989644792293799,
      "loss": 1.7112,
      "step": 1200
    },
    {
      "epoch": 0.10949544498948843,
      "grad_norm": 1.5287480354309082,
      "learning_rate": 0.00019866345574954847,
      "loss": 1.5868,
      "step": 1250
    },
    {
      "epoch": 0.11387526278906797,
      "grad_norm": 1.4756803512573242,
      "learning_rate": 0.00019836243226971706,
      "loss": 1.645,
      "step": 1300
    },
    {
      "epoch": 0.11825508058864752,
      "grad_norm": 1.9053561687469482,
      "learning_rate": 0.00019806140878988562,
      "loss": 1.6843,
      "step": 1350
    },
    {
      "epoch": 0.12263489838822705,
      "grad_norm": 1.4803378582000732,
      "learning_rate": 0.00019776038531005418,
      "loss": 1.668,
      "step": 1400
    },
    {
      "epoch": 0.12701471618780658,
      "grad_norm": 1.989900827407837,
      "learning_rate": 0.00019745936183022277,
      "loss": 1.618,
      "step": 1450
    },
    {
      "epoch": 0.13139453398738613,
      "grad_norm": 1.6233470439910889,
      "learning_rate": 0.00019715833835039134,
      "loss": 1.6419,
      "step": 1500
    },
    {
      "epoch": 0.13577435178696567,
      "grad_norm": 1.317926049232483,
      "learning_rate": 0.00019685731487055993,
      "loss": 1.7132,
      "step": 1550
    },
    {
      "epoch": 0.1401541695865452,
      "grad_norm": 3.008188247680664,
      "learning_rate": 0.0001965562913907285,
      "loss": 1.6388,
      "step": 1600
    },
    {
      "epoch": 0.14453398738612475,
      "grad_norm": 1.7396661043167114,
      "learning_rate": 0.00019625526791089705,
      "loss": 1.6371,
      "step": 1650
    },
    {
      "epoch": 0.14891380518570427,
      "grad_norm": 1.8430118560791016,
      "learning_rate": 0.0001959542444310656,
      "loss": 1.6305,
      "step": 1700
    },
    {
      "epoch": 0.15329362298528382,
      "grad_norm": 1.8691744804382324,
      "learning_rate": 0.0001956532209512342,
      "loss": 1.6424,
      "step": 1750
    },
    {
      "epoch": 0.15767344078486334,
      "grad_norm": 1.5105018615722656,
      "learning_rate": 0.0001953521974714028,
      "loss": 1.7381,
      "step": 1800
    },
    {
      "epoch": 0.1620532585844429,
      "grad_norm": 1.5208607912063599,
      "learning_rate": 0.00019505117399157135,
      "loss": 1.7494,
      "step": 1850
    },
    {
      "epoch": 0.16643307638402244,
      "grad_norm": 1.5021837949752808,
      "learning_rate": 0.00019475015051173992,
      "loss": 1.6764,
      "step": 1900
    },
    {
      "epoch": 0.17081289418360196,
      "grad_norm": 2.175304651260376,
      "learning_rate": 0.0001944491270319085,
      "loss": 1.6736,
      "step": 1950
    },
    {
      "epoch": 0.1751927119831815,
      "grad_norm": 2.672173261642456,
      "learning_rate": 0.00019414810355207707,
      "loss": 1.6977,
      "step": 2000
    },
    {
      "epoch": 0.17957252978276103,
      "grad_norm": 1.3643311262130737,
      "learning_rate": 0.00019384708007224563,
      "loss": 1.612,
      "step": 2050
    },
    {
      "epoch": 0.18395234758234058,
      "grad_norm": 1.6844713687896729,
      "learning_rate": 0.00019354605659241422,
      "loss": 1.6461,
      "step": 2100
    },
    {
      "epoch": 0.1883321653819201,
      "grad_norm": 1.8596502542495728,
      "learning_rate": 0.0001932450331125828,
      "loss": 1.7374,
      "step": 2150
    },
    {
      "epoch": 0.19271198318149965,
      "grad_norm": 1.6061502695083618,
      "learning_rate": 0.00019294400963275137,
      "loss": 1.6537,
      "step": 2200
    },
    {
      "epoch": 0.19709180098107917,
      "grad_norm": 1.5405219793319702,
      "learning_rate": 0.00019264298615291993,
      "loss": 1.6046,
      "step": 2250
    },
    {
      "epoch": 0.20147161878065872,
      "grad_norm": 1.716681957244873,
      "learning_rate": 0.0001923419626730885,
      "loss": 1.649,
      "step": 2300
    },
    {
      "epoch": 0.20585143658023827,
      "grad_norm": 1.4713847637176514,
      "learning_rate": 0.00019204093919325709,
      "loss": 1.6144,
      "step": 2350
    },
    {
      "epoch": 0.2102312543798178,
      "grad_norm": 1.7023416757583618,
      "learning_rate": 0.00019173991571342567,
      "loss": 1.6945,
      "step": 2400
    },
    {
      "epoch": 0.21461107217939734,
      "grad_norm": 2.56640887260437,
      "learning_rate": 0.00019143889223359424,
      "loss": 1.6397,
      "step": 2450
    },
    {
      "epoch": 0.21899088997897687,
      "grad_norm": 1.8027092218399048,
      "learning_rate": 0.0001911378687537628,
      "loss": 1.5944,
      "step": 2500
    },
    {
      "epoch": 0.22337070777855642,
      "grad_norm": 1.95404851436615,
      "learning_rate": 0.00019083684527393136,
      "loss": 1.6591,
      "step": 2550
    },
    {
      "epoch": 0.22775052557813594,
      "grad_norm": 1.5051137208938599,
      "learning_rate": 0.00019053582179409995,
      "loss": 1.659,
      "step": 2600
    },
    {
      "epoch": 0.2321303433777155,
      "grad_norm": 1.4507468938827515,
      "learning_rate": 0.0001902347983142685,
      "loss": 1.6086,
      "step": 2650
    },
    {
      "epoch": 0.23651016117729504,
      "grad_norm": 1.5008114576339722,
      "learning_rate": 0.0001899337748344371,
      "loss": 1.6305,
      "step": 2700
    },
    {
      "epoch": 0.24088997897687456,
      "grad_norm": 1.3465723991394043,
      "learning_rate": 0.00018963275135460567,
      "loss": 1.6157,
      "step": 2750
    },
    {
      "epoch": 0.2452697967764541,
      "grad_norm": 1.4190410375595093,
      "learning_rate": 0.00018933172787477425,
      "loss": 1.6534,
      "step": 2800
    },
    {
      "epoch": 0.24964961457603363,
      "grad_norm": 1.7906603813171387,
      "learning_rate": 0.00018903070439494282,
      "loss": 1.6828,
      "step": 2850
    },
    {
      "epoch": 0.25402943237561315,
      "grad_norm": 2.0109636783599854,
      "learning_rate": 0.00018872968091511138,
      "loss": 1.6344,
      "step": 2900
    },
    {
      "epoch": 0.25840925017519273,
      "grad_norm": 1.5605100393295288,
      "learning_rate": 0.00018842865743527994,
      "loss": 1.6292,
      "step": 2950
    },
    {
      "epoch": 0.26278906797477225,
      "grad_norm": 1.4789583683013916,
      "learning_rate": 0.00018812763395544853,
      "loss": 1.6553,
      "step": 3000
    },
    {
      "epoch": 0.26716888577435177,
      "grad_norm": 2.32258677482605,
      "learning_rate": 0.00018782661047561712,
      "loss": 1.6243,
      "step": 3050
    },
    {
      "epoch": 0.27154870357393135,
      "grad_norm": 1.7127445936203003,
      "learning_rate": 0.00018752558699578568,
      "loss": 1.6199,
      "step": 3100
    },
    {
      "epoch": 0.27592852137351087,
      "grad_norm": 1.402748703956604,
      "learning_rate": 0.00018722456351595424,
      "loss": 1.6654,
      "step": 3150
    },
    {
      "epoch": 0.2803083391730904,
      "grad_norm": 1.4994187355041504,
      "learning_rate": 0.00018692354003612283,
      "loss": 1.7211,
      "step": 3200
    },
    {
      "epoch": 0.2846881569726699,
      "grad_norm": 1.998810052871704,
      "learning_rate": 0.0001866225165562914,
      "loss": 1.5136,
      "step": 3250
    },
    {
      "epoch": 0.2890679747722495,
      "grad_norm": 1.8366889953613281,
      "learning_rate": 0.00018632149307645996,
      "loss": 1.7397,
      "step": 3300
    },
    {
      "epoch": 0.293447792571829,
      "grad_norm": 1.266883373260498,
      "learning_rate": 0.00018602046959662855,
      "loss": 1.7197,
      "step": 3350
    },
    {
      "epoch": 0.29782761037140854,
      "grad_norm": 1.70806086063385,
      "learning_rate": 0.00018571944611679714,
      "loss": 1.6539,
      "step": 3400
    },
    {
      "epoch": 0.3022074281709881,
      "grad_norm": 1.3915460109710693,
      "learning_rate": 0.0001854184226369657,
      "loss": 1.5116,
      "step": 3450
    },
    {
      "epoch": 0.30658724597056763,
      "grad_norm": 1.279021143913269,
      "learning_rate": 0.00018511739915713426,
      "loss": 1.6407,
      "step": 3500
    },
    {
      "epoch": 0.31096706377014716,
      "grad_norm": 1.2144681215286255,
      "learning_rate": 0.00018481637567730282,
      "loss": 1.6209,
      "step": 3550
    },
    {
      "epoch": 0.3153468815697267,
      "grad_norm": 1.7271085977554321,
      "learning_rate": 0.00018451535219747141,
      "loss": 1.6278,
      "step": 3600
    },
    {
      "epoch": 0.31972669936930626,
      "grad_norm": 1.2753769159317017,
      "learning_rate": 0.00018421432871764,
      "loss": 1.6845,
      "step": 3650
    },
    {
      "epoch": 0.3241065171688858,
      "grad_norm": 1.3315339088439941,
      "learning_rate": 0.00018391330523780857,
      "loss": 1.5969,
      "step": 3700
    },
    {
      "epoch": 0.3284863349684653,
      "grad_norm": 1.5219298601150513,
      "learning_rate": 0.00018361228175797713,
      "loss": 1.6648,
      "step": 3750
    },
    {
      "epoch": 0.3328661527680449,
      "grad_norm": 1.973115086555481,
      "learning_rate": 0.0001833112582781457,
      "loss": 1.6183,
      "step": 3800
    },
    {
      "epoch": 0.3372459705676244,
      "grad_norm": 1.4568928480148315,
      "learning_rate": 0.00018301023479831428,
      "loss": 1.6195,
      "step": 3850
    },
    {
      "epoch": 0.3416257883672039,
      "grad_norm": 1.2239148616790771,
      "learning_rate": 0.00018270921131848284,
      "loss": 1.6497,
      "step": 3900
    },
    {
      "epoch": 0.34600560616678344,
      "grad_norm": 1.7206789255142212,
      "learning_rate": 0.00018240818783865143,
      "loss": 1.5482,
      "step": 3950
    },
    {
      "epoch": 0.350385423966363,
      "grad_norm": 1.53490149974823,
      "learning_rate": 0.00018210716435882,
      "loss": 1.6162,
      "step": 4000
    },
    {
      "epoch": 0.35476524176594254,
      "grad_norm": 1.4122334718704224,
      "learning_rate": 0.00018180614087898858,
      "loss": 1.6466,
      "step": 4050
    },
    {
      "epoch": 0.35914505956552206,
      "grad_norm": 1.2972396612167358,
      "learning_rate": 0.00018150511739915715,
      "loss": 1.5891,
      "step": 4100
    },
    {
      "epoch": 0.3635248773651016,
      "grad_norm": 1.335240364074707,
      "learning_rate": 0.0001812040939193257,
      "loss": 1.6517,
      "step": 4150
    },
    {
      "epoch": 0.36790469516468116,
      "grad_norm": 1.2486116886138916,
      "learning_rate": 0.00018090307043949427,
      "loss": 1.674,
      "step": 4200
    },
    {
      "epoch": 0.3722845129642607,
      "grad_norm": 1.277298092842102,
      "learning_rate": 0.00018060204695966286,
      "loss": 1.669,
      "step": 4250
    },
    {
      "epoch": 0.3766643307638402,
      "grad_norm": 1.3660879135131836,
      "learning_rate": 0.00018030102347983145,
      "loss": 1.5815,
      "step": 4300
    },
    {
      "epoch": 0.3810441485634198,
      "grad_norm": 1.4163755178451538,
      "learning_rate": 0.00018,
      "loss": 1.6171,
      "step": 4350
    },
    {
      "epoch": 0.3854239663629993,
      "grad_norm": 1.4509265422821045,
      "learning_rate": 0.00017969897652016857,
      "loss": 1.6483,
      "step": 4400
    },
    {
      "epoch": 0.3898037841625788,
      "grad_norm": 1.299857258796692,
      "learning_rate": 0.00017939795304033716,
      "loss": 1.6415,
      "step": 4450
    },
    {
      "epoch": 0.39418360196215835,
      "grad_norm": 1.2478687763214111,
      "learning_rate": 0.00017909692956050573,
      "loss": 1.6148,
      "step": 4500
    },
    {
      "epoch": 0.3985634197617379,
      "grad_norm": 1.4402998685836792,
      "learning_rate": 0.0001787959060806743,
      "loss": 1.6329,
      "step": 4550
    },
    {
      "epoch": 0.40294323756131745,
      "grad_norm": 1.4689570665359497,
      "learning_rate": 0.00017849488260084288,
      "loss": 1.578,
      "step": 4600
    },
    {
      "epoch": 0.40732305536089697,
      "grad_norm": 1.3790158033370972,
      "learning_rate": 0.00017819385912101144,
      "loss": 1.6278,
      "step": 4650
    },
    {
      "epoch": 0.41170287316047655,
      "grad_norm": 1.3211910724639893,
      "learning_rate": 0.00017789283564118003,
      "loss": 1.5497,
      "step": 4700
    },
    {
      "epoch": 0.41608269096005607,
      "grad_norm": 1.4588291645050049,
      "learning_rate": 0.0001775918121613486,
      "loss": 1.6382,
      "step": 4750
    },
    {
      "epoch": 0.4204625087596356,
      "grad_norm": 1.314435601234436,
      "learning_rate": 0.00017729078868151715,
      "loss": 1.6406,
      "step": 4800
    },
    {
      "epoch": 0.4248423265592151,
      "grad_norm": 1.3714408874511719,
      "learning_rate": 0.00017698976520168574,
      "loss": 1.5934,
      "step": 4850
    },
    {
      "epoch": 0.4292221443587947,
      "grad_norm": 1.381964921951294,
      "learning_rate": 0.00017668874172185433,
      "loss": 1.6695,
      "step": 4900
    },
    {
      "epoch": 0.4336019621583742,
      "grad_norm": 1.824715495109558,
      "learning_rate": 0.0001763877182420229,
      "loss": 1.7306,
      "step": 4950
    },
    {
      "epoch": 0.43798177995795373,
      "grad_norm": 1.1901803016662598,
      "learning_rate": 0.00017608669476219146,
      "loss": 1.6612,
      "step": 5000
    },
    {
      "epoch": 0.4423615977575333,
      "grad_norm": 1.2577261924743652,
      "learning_rate": 0.00017578567128236002,
      "loss": 1.6457,
      "step": 5050
    },
    {
      "epoch": 0.44674141555711283,
      "grad_norm": 1.423357367515564,
      "learning_rate": 0.0001754846478025286,
      "loss": 1.6209,
      "step": 5100
    },
    {
      "epoch": 0.45112123335669235,
      "grad_norm": 1.344194769859314,
      "learning_rate": 0.00017518362432269717,
      "loss": 1.6436,
      "step": 5150
    },
    {
      "epoch": 0.4555010511562719,
      "grad_norm": 1.283442497253418,
      "learning_rate": 0.00017488260084286576,
      "loss": 1.5892,
      "step": 5200
    },
    {
      "epoch": 0.45988086895585145,
      "grad_norm": 1.3406428098678589,
      "learning_rate": 0.00017458157736303432,
      "loss": 1.5469,
      "step": 5250
    },
    {
      "epoch": 0.464260686755431,
      "grad_norm": 1.4942926168441772,
      "learning_rate": 0.0001742805538832029,
      "loss": 1.6584,
      "step": 5300
    },
    {
      "epoch": 0.4686405045550105,
      "grad_norm": 1.6709541082382202,
      "learning_rate": 0.00017397953040337147,
      "loss": 1.6,
      "step": 5350
    },
    {
      "epoch": 0.4730203223545901,
      "grad_norm": 1.3974028825759888,
      "learning_rate": 0.00017367850692354004,
      "loss": 1.7085,
      "step": 5400
    },
    {
      "epoch": 0.4774001401541696,
      "grad_norm": 1.4816739559173584,
      "learning_rate": 0.0001733774834437086,
      "loss": 1.6932,
      "step": 5450
    },
    {
      "epoch": 0.4817799579537491,
      "grad_norm": 1.1935863494873047,
      "learning_rate": 0.0001730764599638772,
      "loss": 1.6225,
      "step": 5500
    },
    {
      "epoch": 0.48615977575332864,
      "grad_norm": 1.4248281717300415,
      "learning_rate": 0.00017277543648404578,
      "loss": 1.6743,
      "step": 5550
    },
    {
      "epoch": 0.4905395935529082,
      "grad_norm": 1.1352788209915161,
      "learning_rate": 0.00017247441300421434,
      "loss": 1.5996,
      "step": 5600
    },
    {
      "epoch": 0.49491941135248774,
      "grad_norm": 1.2682044506072998,
      "learning_rate": 0.0001721733895243829,
      "loss": 1.6401,
      "step": 5650
    },
    {
      "epoch": 0.49929922915206726,
      "grad_norm": 1.2533936500549316,
      "learning_rate": 0.0001718723660445515,
      "loss": 1.6017,
      "step": 5700
    },
    {
      "epoch": 0.5036790469516468,
      "grad_norm": 1.7724554538726807,
      "learning_rate": 0.00017157134256472005,
      "loss": 1.594,
      "step": 5750
    },
    {
      "epoch": 0.5080588647512263,
      "grad_norm": 1.5835938453674316,
      "learning_rate": 0.00017127031908488862,
      "loss": 1.6489,
      "step": 5800
    },
    {
      "epoch": 0.5124386825508059,
      "grad_norm": 1.5619081258773804,
      "learning_rate": 0.0001709692956050572,
      "loss": 1.6993,
      "step": 5850
    },
    {
      "epoch": 0.5168185003503855,
      "grad_norm": 1.4962546825408936,
      "learning_rate": 0.00017066827212522577,
      "loss": 1.6214,
      "step": 5900
    },
    {
      "epoch": 0.521198318149965,
      "grad_norm": 1.4853190183639526,
      "learning_rate": 0.00017036724864539436,
      "loss": 1.5661,
      "step": 5950
    },
    {
      "epoch": 0.5255781359495445,
      "grad_norm": 1.9406431913375854,
      "learning_rate": 0.00017006622516556292,
      "loss": 1.6864,
      "step": 6000
    },
    {
      "epoch": 0.529957953749124,
      "grad_norm": 1.3626078367233276,
      "learning_rate": 0.00016976520168573148,
      "loss": 1.6497,
      "step": 6050
    },
    {
      "epoch": 0.5343377715487035,
      "grad_norm": 1.5591671466827393,
      "learning_rate": 0.00016946417820590007,
      "loss": 1.6327,
      "step": 6100
    },
    {
      "epoch": 0.5387175893482831,
      "grad_norm": 1.3114748001098633,
      "learning_rate": 0.00016916315472606866,
      "loss": 1.6665,
      "step": 6150
    },
    {
      "epoch": 0.5430974071478627,
      "grad_norm": 1.5988694429397583,
      "learning_rate": 0.00016886213124623722,
      "loss": 1.6438,
      "step": 6200
    },
    {
      "epoch": 0.5474772249474422,
      "grad_norm": 1.3312216997146606,
      "learning_rate": 0.00016856110776640579,
      "loss": 1.6579,
      "step": 6250
    },
    {
      "epoch": 0.5518570427470217,
      "grad_norm": 1.5342870950698853,
      "learning_rate": 0.00016826008428657435,
      "loss": 1.6773,
      "step": 6300
    },
    {
      "epoch": 0.5562368605466013,
      "grad_norm": 2.038602113723755,
      "learning_rate": 0.00016795906080674294,
      "loss": 1.6528,
      "step": 6350
    },
    {
      "epoch": 0.5606166783461808,
      "grad_norm": 1.705918312072754,
      "learning_rate": 0.0001676580373269115,
      "loss": 1.6994,
      "step": 6400
    },
    {
      "epoch": 0.5649964961457603,
      "grad_norm": 1.4452626705169678,
      "learning_rate": 0.0001673570138470801,
      "loss": 1.5837,
      "step": 6450
    },
    {
      "epoch": 0.5693763139453398,
      "grad_norm": 1.181293249130249,
      "learning_rate": 0.00016705599036724865,
      "loss": 1.5703,
      "step": 6500
    },
    {
      "epoch": 0.5737561317449195,
      "grad_norm": 1.525970458984375,
      "learning_rate": 0.00016675496688741724,
      "loss": 1.5793,
      "step": 6550
    },
    {
      "epoch": 0.578135949544499,
      "grad_norm": 1.4444389343261719,
      "learning_rate": 0.0001664539434075858,
      "loss": 1.6357,
      "step": 6600
    },
    {
      "epoch": 0.5825157673440785,
      "grad_norm": 1.616320252418518,
      "learning_rate": 0.00016615291992775437,
      "loss": 1.5991,
      "step": 6650
    },
    {
      "epoch": 0.586895585143658,
      "grad_norm": 1.1337963342666626,
      "learning_rate": 0.00016585189644792293,
      "loss": 1.6215,
      "step": 6700
    },
    {
      "epoch": 0.5912754029432375,
      "grad_norm": 1.413107991218567,
      "learning_rate": 0.00016555087296809152,
      "loss": 1.7219,
      "step": 6750
    },
    {
      "epoch": 0.5956552207428171,
      "grad_norm": 1.3712000846862793,
      "learning_rate": 0.0001652498494882601,
      "loss": 1.6742,
      "step": 6800
    },
    {
      "epoch": 0.6000350385423966,
      "grad_norm": 1.1625936031341553,
      "learning_rate": 0.00016494882600842867,
      "loss": 1.5747,
      "step": 6850
    },
    {
      "epoch": 0.6044148563419762,
      "grad_norm": 1.3500334024429321,
      "learning_rate": 0.00016464780252859723,
      "loss": 1.6349,
      "step": 6900
    },
    {
      "epoch": 0.6087946741415557,
      "grad_norm": 1.501381754875183,
      "learning_rate": 0.00016434677904876582,
      "loss": 1.6614,
      "step": 6950
    },
    {
      "epoch": 0.6131744919411353,
      "grad_norm": 2.0485188961029053,
      "learning_rate": 0.00016404575556893438,
      "loss": 1.6228,
      "step": 7000
    },
    {
      "epoch": 0.6175543097407148,
      "grad_norm": 1.4829307794570923,
      "learning_rate": 0.00016374473208910295,
      "loss": 1.5624,
      "step": 7050
    },
    {
      "epoch": 0.6219341275402943,
      "grad_norm": 1.6211812496185303,
      "learning_rate": 0.00016344370860927153,
      "loss": 1.5518,
      "step": 7100
    },
    {
      "epoch": 0.6263139453398738,
      "grad_norm": 1.313510775566101,
      "learning_rate": 0.0001631426851294401,
      "loss": 1.5938,
      "step": 7150
    },
    {
      "epoch": 0.6306937631394534,
      "grad_norm": 1.2726154327392578,
      "learning_rate": 0.00016284166164960869,
      "loss": 1.5833,
      "step": 7200
    },
    {
      "epoch": 0.635073580939033,
      "grad_norm": 1.5245575904846191,
      "learning_rate": 0.00016254063816977725,
      "loss": 1.6368,
      "step": 7250
    },
    {
      "epoch": 0.6394533987386125,
      "grad_norm": 1.1503214836120605,
      "learning_rate": 0.0001622396146899458,
      "loss": 1.6344,
      "step": 7300
    },
    {
      "epoch": 0.643833216538192,
      "grad_norm": 1.2302666902542114,
      "learning_rate": 0.0001619385912101144,
      "loss": 1.6119,
      "step": 7350
    },
    {
      "epoch": 0.6482130343377716,
      "grad_norm": 1.266003966331482,
      "learning_rate": 0.000161637567730283,
      "loss": 1.7217,
      "step": 7400
    },
    {
      "epoch": 0.6525928521373511,
      "grad_norm": 1.2256615161895752,
      "learning_rate": 0.00016133654425045155,
      "loss": 1.6337,
      "step": 7450
    },
    {
      "epoch": 0.6569726699369306,
      "grad_norm": 1.5925779342651367,
      "learning_rate": 0.00016103552077062011,
      "loss": 1.5881,
      "step": 7500
    },
    {
      "epoch": 0.6613524877365101,
      "grad_norm": 1.1734551191329956,
      "learning_rate": 0.00016073449729078868,
      "loss": 1.6323,
      "step": 7550
    },
    {
      "epoch": 0.6657323055360898,
      "grad_norm": 1.4634692668914795,
      "learning_rate": 0.00016043347381095727,
      "loss": 1.6454,
      "step": 7600
    },
    {
      "epoch": 0.6701121233356693,
      "grad_norm": 1.3694988489151,
      "learning_rate": 0.00016013245033112583,
      "loss": 1.679,
      "step": 7650
    },
    {
      "epoch": 0.6744919411352488,
      "grad_norm": 1.2401602268218994,
      "learning_rate": 0.00015983142685129442,
      "loss": 1.554,
      "step": 7700
    },
    {
      "epoch": 0.6788717589348283,
      "grad_norm": 1.5289441347122192,
      "learning_rate": 0.00015953040337146298,
      "loss": 1.5445,
      "step": 7750
    },
    {
      "epoch": 0.6832515767344078,
      "grad_norm": 1.522354006767273,
      "learning_rate": 0.00015922937989163157,
      "loss": 1.5895,
      "step": 7800
    },
    {
      "epoch": 0.6876313945339874,
      "grad_norm": 1.6265839338302612,
      "learning_rate": 0.00015892835641180013,
      "loss": 1.6341,
      "step": 7850
    },
    {
      "epoch": 0.6920112123335669,
      "grad_norm": 1.7081000804901123,
      "learning_rate": 0.0001586273329319687,
      "loss": 1.5368,
      "step": 7900
    },
    {
      "epoch": 0.6963910301331465,
      "grad_norm": 1.567068099975586,
      "learning_rate": 0.00015832630945213726,
      "loss": 1.5249,
      "step": 7950
    },
    {
      "epoch": 0.700770847932726,
      "grad_norm": 1.156413197517395,
      "learning_rate": 0.00015802528597230585,
      "loss": 1.5994,
      "step": 8000
    },
    {
      "epoch": 0.7051506657323056,
      "grad_norm": 1.2911593914031982,
      "learning_rate": 0.00015772426249247444,
      "loss": 1.6523,
      "step": 8050
    },
    {
      "epoch": 0.7095304835318851,
      "grad_norm": 1.5198878049850464,
      "learning_rate": 0.000157423239012643,
      "loss": 1.5764,
      "step": 8100
    },
    {
      "epoch": 0.7139103013314646,
      "grad_norm": 1.2743011713027954,
      "learning_rate": 0.00015712221553281156,
      "loss": 1.6675,
      "step": 8150
    },
    {
      "epoch": 0.7182901191310441,
      "grad_norm": 1.58554208278656,
      "learning_rate": 0.00015682119205298015,
      "loss": 1.6126,
      "step": 8200
    },
    {
      "epoch": 0.7226699369306236,
      "grad_norm": 1.20383620262146,
      "learning_rate": 0.0001565201685731487,
      "loss": 1.5977,
      "step": 8250
    },
    {
      "epoch": 0.7270497547302032,
      "grad_norm": 1.2698572874069214,
      "learning_rate": 0.00015621914509331727,
      "loss": 1.6541,
      "step": 8300
    },
    {
      "epoch": 0.7314295725297828,
      "grad_norm": 1.460609793663025,
      "learning_rate": 0.00015591812161348586,
      "loss": 1.6368,
      "step": 8350
    },
    {
      "epoch": 0.7358093903293623,
      "grad_norm": 1.5610108375549316,
      "learning_rate": 0.00015561709813365443,
      "loss": 1.7302,
      "step": 8400
    },
    {
      "epoch": 0.7401892081289418,
      "grad_norm": 1.4780635833740234,
      "learning_rate": 0.00015531607465382301,
      "loss": 1.6791,
      "step": 8450
    },
    {
      "epoch": 0.7445690259285214,
      "grad_norm": 1.400941014289856,
      "learning_rate": 0.00015501505117399158,
      "loss": 1.606,
      "step": 8500
    },
    {
      "epoch": 0.7489488437281009,
      "grad_norm": 1.698575735092163,
      "learning_rate": 0.00015471402769416014,
      "loss": 1.6215,
      "step": 8550
    },
    {
      "epoch": 0.7533286615276804,
      "grad_norm": 1.3748314380645752,
      "learning_rate": 0.00015441300421432873,
      "loss": 1.6317,
      "step": 8600
    },
    {
      "epoch": 0.7577084793272599,
      "grad_norm": 1.1422451734542847,
      "learning_rate": 0.00015411198073449732,
      "loss": 1.6004,
      "step": 8650
    },
    {
      "epoch": 0.7620882971268396,
      "grad_norm": 1.3141088485717773,
      "learning_rate": 0.00015381095725466588,
      "loss": 1.569,
      "step": 8700
    },
    {
      "epoch": 0.7664681149264191,
      "grad_norm": 1.2704089879989624,
      "learning_rate": 0.00015350993377483444,
      "loss": 1.684,
      "step": 8750
    },
    {
      "epoch": 0.7708479327259986,
      "grad_norm": 1.4361313581466675,
      "learning_rate": 0.000153208910295003,
      "loss": 1.5951,
      "step": 8800
    },
    {
      "epoch": 0.7752277505255781,
      "grad_norm": 1.2294952869415283,
      "learning_rate": 0.0001529078868151716,
      "loss": 1.6265,
      "step": 8850
    },
    {
      "epoch": 0.7796075683251577,
      "grad_norm": 1.3247931003570557,
      "learning_rate": 0.00015260686333534016,
      "loss": 1.5799,
      "step": 8900
    },
    {
      "epoch": 0.7839873861247372,
      "grad_norm": 1.2710869312286377,
      "learning_rate": 0.00015230583985550875,
      "loss": 1.5914,
      "step": 8950
    },
    {
      "epoch": 0.7883672039243167,
      "grad_norm": 1.3359915018081665,
      "learning_rate": 0.0001520048163756773,
      "loss": 1.5749,
      "step": 9000
    },
    {
      "epoch": 0.7927470217238963,
      "grad_norm": 1.5398682355880737,
      "learning_rate": 0.0001517037928958459,
      "loss": 1.5734,
      "step": 9050
    },
    {
      "epoch": 0.7971268395234758,
      "grad_norm": 1.4073281288146973,
      "learning_rate": 0.00015140276941601446,
      "loss": 1.6402,
      "step": 9100
    },
    {
      "epoch": 0.8015066573230554,
      "grad_norm": 1.364536166191101,
      "learning_rate": 0.00015110174593618302,
      "loss": 1.5904,
      "step": 9150
    },
    {
      "epoch": 0.8058864751226349,
      "grad_norm": 1.4030472040176392,
      "learning_rate": 0.00015080072245635159,
      "loss": 1.5752,
      "step": 9200
    },
    {
      "epoch": 0.8102662929222144,
      "grad_norm": 1.2598294019699097,
      "learning_rate": 0.00015049969897652017,
      "loss": 1.5934,
      "step": 9250
    },
    {
      "epoch": 0.8146461107217939,
      "grad_norm": 1.891524076461792,
      "learning_rate": 0.00015019867549668876,
      "loss": 1.6137,
      "step": 9300
    },
    {
      "epoch": 0.8190259285213735,
      "grad_norm": 1.9541641473770142,
      "learning_rate": 0.00014989765201685733,
      "loss": 1.6094,
      "step": 9350
    },
    {
      "epoch": 0.8234057463209531,
      "grad_norm": 1.50436532497406,
      "learning_rate": 0.0001495966285370259,
      "loss": 1.6008,
      "step": 9400
    },
    {
      "epoch": 0.8277855641205326,
      "grad_norm": 1.8933005332946777,
      "learning_rate": 0.00014929560505719448,
      "loss": 1.6712,
      "step": 9450
    },
    {
      "epoch": 0.8321653819201121,
      "grad_norm": 1.326215147972107,
      "learning_rate": 0.00014899458157736304,
      "loss": 1.6243,
      "step": 9500
    },
    {
      "epoch": 0.8365451997196917,
      "grad_norm": 1.4753458499908447,
      "learning_rate": 0.0001486935580975316,
      "loss": 1.715,
      "step": 9550
    },
    {
      "epoch": 0.8409250175192712,
      "grad_norm": 1.2897975444793701,
      "learning_rate": 0.0001483925346177002,
      "loss": 1.6318,
      "step": 9600
    },
    {
      "epoch": 0.8453048353188507,
      "grad_norm": 1.4581003189086914,
      "learning_rate": 0.00014809151113786875,
      "loss": 1.6677,
      "step": 9650
    },
    {
      "epoch": 0.8496846531184302,
      "grad_norm": 1.1162225008010864,
      "learning_rate": 0.00014779048765803734,
      "loss": 1.6601,
      "step": 9700
    },
    {
      "epoch": 0.8540644709180099,
      "grad_norm": 1.1457672119140625,
      "learning_rate": 0.0001474894641782059,
      "loss": 1.6779,
      "step": 9750
    },
    {
      "epoch": 0.8584442887175894,
      "grad_norm": 1.3912042379379272,
      "learning_rate": 0.00014718844069837447,
      "loss": 1.6795,
      "step": 9800
    },
    {
      "epoch": 0.8628241065171689,
      "grad_norm": 1.331969141960144,
      "learning_rate": 0.00014688741721854303,
      "loss": 1.6577,
      "step": 9850
    },
    {
      "epoch": 0.8672039243167484,
      "grad_norm": 1.277234673500061,
      "learning_rate": 0.00014658639373871165,
      "loss": 1.6186,
      "step": 9900
    },
    {
      "epoch": 0.8715837421163279,
      "grad_norm": 1.2418769598007202,
      "learning_rate": 0.0001462853702588802,
      "loss": 1.5827,
      "step": 9950
    },
    {
      "epoch": 0.8759635599159075,
      "grad_norm": 1.1636873483657837,
      "learning_rate": 0.00014598434677904877,
      "loss": 1.6085,
      "step": 10000
    },
    {
      "epoch": 0.880343377715487,
      "grad_norm": 1.1489967107772827,
      "learning_rate": 0.00014568332329921733,
      "loss": 1.617,
      "step": 10050
    },
    {
      "epoch": 0.8847231955150666,
      "grad_norm": 2.331305742263794,
      "learning_rate": 0.00014538229981938592,
      "loss": 1.5575,
      "step": 10100
    },
    {
      "epoch": 0.8891030133146461,
      "grad_norm": 1.294609785079956,
      "learning_rate": 0.00014508127633955449,
      "loss": 1.611,
      "step": 10150
    },
    {
      "epoch": 0.8934828311142257,
      "grad_norm": 1.3413370847702026,
      "learning_rate": 0.00014478025285972308,
      "loss": 1.6279,
      "step": 10200
    },
    {
      "epoch": 0.8978626489138052,
      "grad_norm": 1.7797210216522217,
      "learning_rate": 0.00014447922937989164,
      "loss": 1.6018,
      "step": 10250
    },
    {
      "epoch": 0.9022424667133847,
      "grad_norm": 1.5906974077224731,
      "learning_rate": 0.00014417820590006023,
      "loss": 1.558,
      "step": 10300
    },
    {
      "epoch": 0.9066222845129642,
      "grad_norm": 1.1848493814468384,
      "learning_rate": 0.0001438771824202288,
      "loss": 1.637,
      "step": 10350
    },
    {
      "epoch": 0.9110021023125437,
      "grad_norm": 1.0869629383087158,
      "learning_rate": 0.00014357615894039735,
      "loss": 1.5737,
      "step": 10400
    },
    {
      "epoch": 0.9153819201121234,
      "grad_norm": 1.6596102714538574,
      "learning_rate": 0.00014327513546056591,
      "loss": 1.6506,
      "step": 10450
    },
    {
      "epoch": 0.9197617379117029,
      "grad_norm": 1.106020212173462,
      "learning_rate": 0.0001429741119807345,
      "loss": 1.5794,
      "step": 10500
    },
    {
      "epoch": 0.9241415557112824,
      "grad_norm": 1.5169965028762817,
      "learning_rate": 0.0001426730885009031,
      "loss": 1.6363,
      "step": 10550
    },
    {
      "epoch": 0.928521373510862,
      "grad_norm": 1.4697755575180054,
      "learning_rate": 0.00014237206502107165,
      "loss": 1.5932,
      "step": 10600
    },
    {
      "epoch": 0.9329011913104415,
      "grad_norm": 1.5610522031784058,
      "learning_rate": 0.00014207104154124022,
      "loss": 1.6318,
      "step": 10650
    },
    {
      "epoch": 0.937281009110021,
      "grad_norm": 1.2602479457855225,
      "learning_rate": 0.0001417700180614088,
      "loss": 1.7088,
      "step": 10700
    },
    {
      "epoch": 0.9416608269096005,
      "grad_norm": 1.2232226133346558,
      "learning_rate": 0.00014146899458157737,
      "loss": 1.5759,
      "step": 10750
    },
    {
      "epoch": 0.9460406447091801,
      "grad_norm": 1.1760053634643555,
      "learning_rate": 0.00014116797110174593,
      "loss": 1.5581,
      "step": 10800
    },
    {
      "epoch": 0.9504204625087597,
      "grad_norm": 1.3160041570663452,
      "learning_rate": 0.00014086694762191452,
      "loss": 1.6653,
      "step": 10850
    },
    {
      "epoch": 0.9548002803083392,
      "grad_norm": 1.374417781829834,
      "learning_rate": 0.00014056592414208308,
      "loss": 1.6448,
      "step": 10900
    },
    {
      "epoch": 0.9591800981079187,
      "grad_norm": 1.6732534170150757,
      "learning_rate": 0.00014026490066225167,
      "loss": 1.6494,
      "step": 10950
    },
    {
      "epoch": 0.9635599159074982,
      "grad_norm": 1.4362208843231201,
      "learning_rate": 0.00013996387718242023,
      "loss": 1.6024,
      "step": 11000
    },
    {
      "epoch": 0.9679397337070778,
      "grad_norm": 1.2814075946807861,
      "learning_rate": 0.0001396628537025888,
      "loss": 1.6584,
      "step": 11050
    },
    {
      "epoch": 0.9723195515066573,
      "grad_norm": 1.1079457998275757,
      "learning_rate": 0.00013936183022275736,
      "loss": 1.5584,
      "step": 11100
    },
    {
      "epoch": 0.9766993693062369,
      "grad_norm": 1.8242696523666382,
      "learning_rate": 0.00013906080674292598,
      "loss": 1.6747,
      "step": 11150
    },
    {
      "epoch": 0.9810791871058164,
      "grad_norm": 1.4586400985717773,
      "learning_rate": 0.00013875978326309454,
      "loss": 1.5864,
      "step": 11200
    },
    {
      "epoch": 0.985459004905396,
      "grad_norm": 1.076587200164795,
      "learning_rate": 0.0001384587597832631,
      "loss": 1.6782,
      "step": 11250
    },
    {
      "epoch": 0.9898388227049755,
      "grad_norm": 1.3051252365112305,
      "learning_rate": 0.00013815773630343166,
      "loss": 1.5991,
      "step": 11300
    },
    {
      "epoch": 0.994218640504555,
      "grad_norm": 1.373781442642212,
      "learning_rate": 0.00013785671282360025,
      "loss": 1.6289,
      "step": 11350
    },
    {
      "epoch": 0.9985984583041345,
      "grad_norm": 1.2085638046264648,
      "learning_rate": 0.00013755568934376881,
      "loss": 1.6433,
      "step": 11400
    },
    {
      "epoch": 1.002978276103714,
      "grad_norm": 0.8326178789138794,
      "learning_rate": 0.0001372546658639374,
      "loss": 1.4978,
      "step": 11450
    },
    {
      "epoch": 1.0073580939032936,
      "grad_norm": 0.8134517073631287,
      "learning_rate": 0.00013695364238410597,
      "loss": 1.5933,
      "step": 11500
    },
    {
      "epoch": 1.011737911702873,
      "grad_norm": 1.0472066402435303,
      "learning_rate": 0.00013665261890427456,
      "loss": 1.6254,
      "step": 11550
    },
    {
      "epoch": 1.0161177295024526,
      "grad_norm": 0.9054408073425293,
      "learning_rate": 0.00013635159542444312,
      "loss": 1.6338,
      "step": 11600
    },
    {
      "epoch": 1.0204975473020321,
      "grad_norm": 1.039628028869629,
      "learning_rate": 0.00013605057194461168,
      "loss": 1.5085,
      "step": 11650
    },
    {
      "epoch": 1.0248773651016119,
      "grad_norm": 0.8618564605712891,
      "learning_rate": 0.00013574954846478024,
      "loss": 1.6076,
      "step": 11700
    },
    {
      "epoch": 1.0292571829011914,
      "grad_norm": 1.3077696561813354,
      "learning_rate": 0.00013544852498494883,
      "loss": 1.6142,
      "step": 11750
    },
    {
      "epoch": 1.033637000700771,
      "grad_norm": 1.0362564325332642,
      "learning_rate": 0.00013514750150511742,
      "loss": 1.661,
      "step": 11800
    },
    {
      "epoch": 1.0380168185003504,
      "grad_norm": 1.023711085319519,
      "learning_rate": 0.00013484647802528598,
      "loss": 1.5901,
      "step": 11850
    },
    {
      "epoch": 1.04239663629993,
      "grad_norm": 1.3169869184494019,
      "learning_rate": 0.00013454545454545455,
      "loss": 1.6123,
      "step": 11900
    },
    {
      "epoch": 1.0467764540995095,
      "grad_norm": 0.8485040068626404,
      "learning_rate": 0.00013424443106562314,
      "loss": 1.5861,
      "step": 11950
    },
    {
      "epoch": 1.051156271899089,
      "grad_norm": 1.0917741060256958,
      "learning_rate": 0.0001339434075857917,
      "loss": 1.5182,
      "step": 12000
    },
    {
      "epoch": 1.0555360896986685,
      "grad_norm": 1.0597161054611206,
      "learning_rate": 0.00013364238410596026,
      "loss": 1.5317,
      "step": 12050
    },
    {
      "epoch": 1.059915907498248,
      "grad_norm": 0.994206428527832,
      "learning_rate": 0.00013334136062612885,
      "loss": 1.5571,
      "step": 12100
    },
    {
      "epoch": 1.0642957252978276,
      "grad_norm": 0.9442524313926697,
      "learning_rate": 0.0001330403371462974,
      "loss": 1.6615,
      "step": 12150
    },
    {
      "epoch": 1.068675543097407,
      "grad_norm": 1.2577415704727173,
      "learning_rate": 0.000132739313666466,
      "loss": 1.6011,
      "step": 12200
    },
    {
      "epoch": 1.0730553608969866,
      "grad_norm": 0.9681110978126526,
      "learning_rate": 0.00013243829018663456,
      "loss": 1.6482,
      "step": 12250
    },
    {
      "epoch": 1.0774351786965661,
      "grad_norm": 1.1515228748321533,
      "learning_rate": 0.00013213726670680313,
      "loss": 1.5571,
      "step": 12300
    },
    {
      "epoch": 1.0818149964961457,
      "grad_norm": 1.2183303833007812,
      "learning_rate": 0.0001318362432269717,
      "loss": 1.5101,
      "step": 12350
    },
    {
      "epoch": 1.0861948142957254,
      "grad_norm": 0.8386996984481812,
      "learning_rate": 0.0001315352197471403,
      "loss": 1.6539,
      "step": 12400
    },
    {
      "epoch": 1.090574632095305,
      "grad_norm": 0.9747015237808228,
      "learning_rate": 0.00013123419626730887,
      "loss": 1.6027,
      "step": 12450
    },
    {
      "epoch": 1.0949544498948844,
      "grad_norm": 0.99690181016922,
      "learning_rate": 0.00013093317278747743,
      "loss": 1.5973,
      "step": 12500
    },
    {
      "epoch": 1.099334267694464,
      "grad_norm": 1.0532236099243164,
      "learning_rate": 0.000130632149307646,
      "loss": 1.658,
      "step": 12550
    },
    {
      "epoch": 1.1037140854940435,
      "grad_norm": 1.2455511093139648,
      "learning_rate": 0.00013033112582781458,
      "loss": 1.5822,
      "step": 12600
    },
    {
      "epoch": 1.108093903293623,
      "grad_norm": 1.1195238828659058,
      "learning_rate": 0.00013003010234798314,
      "loss": 1.5164,
      "step": 12650
    },
    {
      "epoch": 1.1124737210932025,
      "grad_norm": 1.0095642805099487,
      "learning_rate": 0.00012972907886815173,
      "loss": 1.576,
      "step": 12700
    },
    {
      "epoch": 1.116853538892782,
      "grad_norm": 1.0798654556274414,
      "learning_rate": 0.0001294280553883203,
      "loss": 1.5706,
      "step": 12750
    },
    {
      "epoch": 1.1212333566923616,
      "grad_norm": 1.190269112586975,
      "learning_rate": 0.00012912703190848888,
      "loss": 1.5496,
      "step": 12800
    },
    {
      "epoch": 1.125613174491941,
      "grad_norm": 1.1276531219482422,
      "learning_rate": 0.00012882600842865745,
      "loss": 1.6533,
      "step": 12850
    },
    {
      "epoch": 1.1299929922915206,
      "grad_norm": 1.1492033004760742,
      "learning_rate": 0.000128524984948826,
      "loss": 1.5022,
      "step": 12900
    },
    {
      "epoch": 1.1343728100911001,
      "grad_norm": 1.4079179763793945,
      "learning_rate": 0.00012822396146899457,
      "loss": 1.5104,
      "step": 12950
    },
    {
      "epoch": 1.1387526278906797,
      "grad_norm": 1.3301897048950195,
      "learning_rate": 0.00012792293798916316,
      "loss": 1.6076,
      "step": 13000
    },
    {
      "epoch": 1.1431324456902594,
      "grad_norm": 1.568882942199707,
      "learning_rate": 0.00012762191450933175,
      "loss": 1.5117,
      "step": 13050
    },
    {
      "epoch": 1.1475122634898387,
      "grad_norm": 1.2308311462402344,
      "learning_rate": 0.0001273208910295003,
      "loss": 1.5378,
      "step": 13100
    },
    {
      "epoch": 1.1518920812894184,
      "grad_norm": 0.7846047282218933,
      "learning_rate": 0.00012701986754966887,
      "loss": 1.5585,
      "step": 13150
    },
    {
      "epoch": 1.156271899088998,
      "grad_norm": 0.9687591791152954,
      "learning_rate": 0.00012671884406983744,
      "loss": 1.6236,
      "step": 13200
    },
    {
      "epoch": 1.1606517168885775,
      "grad_norm": 1.178251028060913,
      "learning_rate": 0.00012641782059000603,
      "loss": 1.6685,
      "step": 13250
    },
    {
      "epoch": 1.165031534688157,
      "grad_norm": 0.8725735545158386,
      "learning_rate": 0.0001261167971101746,
      "loss": 1.6115,
      "step": 13300
    },
    {
      "epoch": 1.1694113524877365,
      "grad_norm": 1.042421817779541,
      "learning_rate": 0.00012581577363034318,
      "loss": 1.5252,
      "step": 13350
    },
    {
      "epoch": 1.173791170287316,
      "grad_norm": 0.9259981513023376,
      "learning_rate": 0.00012551475015051174,
      "loss": 1.657,
      "step": 13400
    },
    {
      "epoch": 1.1781709880868956,
      "grad_norm": 1.3258715867996216,
      "learning_rate": 0.00012521372667068033,
      "loss": 1.5639,
      "step": 13450
    },
    {
      "epoch": 1.182550805886475,
      "grad_norm": 0.9908029437065125,
      "learning_rate": 0.0001249127031908489,
      "loss": 1.6373,
      "step": 13500
    },
    {
      "epoch": 1.1869306236860546,
      "grad_norm": 1.1869921684265137,
      "learning_rate": 0.00012461167971101745,
      "loss": 1.5325,
      "step": 13550
    },
    {
      "epoch": 1.1913104414856341,
      "grad_norm": 1.1076440811157227,
      "learning_rate": 0.00012431065623118602,
      "loss": 1.6249,
      "step": 13600
    },
    {
      "epoch": 1.1956902592852137,
      "grad_norm": 1.0982377529144287,
      "learning_rate": 0.00012400963275135463,
      "loss": 1.5464,
      "step": 13650
    },
    {
      "epoch": 1.2000700770847932,
      "grad_norm": 0.9773062467575073,
      "learning_rate": 0.0001237086092715232,
      "loss": 1.4738,
      "step": 13700
    },
    {
      "epoch": 1.2044498948843727,
      "grad_norm": 0.8766072392463684,
      "learning_rate": 0.00012340758579169176,
      "loss": 1.5585,
      "step": 13750
    },
    {
      "epoch": 1.2088297126839525,
      "grad_norm": 0.9375047087669373,
      "learning_rate": 0.00012310656231186032,
      "loss": 1.5955,
      "step": 13800
    },
    {
      "epoch": 1.213209530483532,
      "grad_norm": 1.1985960006713867,
      "learning_rate": 0.0001228055388320289,
      "loss": 1.5607,
      "step": 13850
    },
    {
      "epoch": 1.2175893482831115,
      "grad_norm": 0.9889920949935913,
      "learning_rate": 0.00012250451535219747,
      "loss": 1.5934,
      "step": 13900
    },
    {
      "epoch": 1.221969166082691,
      "grad_norm": 1.2326725721359253,
      "learning_rate": 0.00012220349187236606,
      "loss": 1.5663,
      "step": 13950
    },
    {
      "epoch": 1.2263489838822705,
      "grad_norm": 0.9624714255332947,
      "learning_rate": 0.00012190246839253462,
      "loss": 1.5022,
      "step": 14000
    },
    {
      "epoch": 1.23072880168185,
      "grad_norm": 0.9246013760566711,
      "learning_rate": 0.0001216014449127032,
      "loss": 1.5554,
      "step": 14050
    },
    {
      "epoch": 1.2351086194814296,
      "grad_norm": 1.0452187061309814,
      "learning_rate": 0.00012130042143287178,
      "loss": 1.6439,
      "step": 14100
    },
    {
      "epoch": 1.239488437281009,
      "grad_norm": 1.1700525283813477,
      "learning_rate": 0.00012099939795304034,
      "loss": 1.5402,
      "step": 14150
    },
    {
      "epoch": 1.2438682550805886,
      "grad_norm": 1.3361058235168457,
      "learning_rate": 0.00012069837447320891,
      "loss": 1.5787,
      "step": 14200
    },
    {
      "epoch": 1.2482480728801681,
      "grad_norm": 1.0385946035385132,
      "learning_rate": 0.0001203973509933775,
      "loss": 1.6016,
      "step": 14250
    },
    {
      "epoch": 1.2526278906797477,
      "grad_norm": 1.1986078023910522,
      "learning_rate": 0.00012009632751354607,
      "loss": 1.5729,
      "step": 14300
    },
    {
      "epoch": 1.2570077084793272,
      "grad_norm": 1.0590211153030396,
      "learning_rate": 0.00011979530403371464,
      "loss": 1.5167,
      "step": 14350
    },
    {
      "epoch": 1.2613875262789067,
      "grad_norm": 1.450353980064392,
      "learning_rate": 0.0001194942805538832,
      "loss": 1.6346,
      "step": 14400
    },
    {
      "epoch": 1.2657673440784865,
      "grad_norm": 1.0384513139724731,
      "learning_rate": 0.00011919325707405178,
      "loss": 1.6675,
      "step": 14450
    },
    {
      "epoch": 1.2701471618780658,
      "grad_norm": 0.984882116317749,
      "learning_rate": 0.00011889223359422034,
      "loss": 1.6263,
      "step": 14500
    },
    {
      "epoch": 1.2745269796776455,
      "grad_norm": 1.205359935760498,
      "learning_rate": 0.00011859121011438892,
      "loss": 1.5824,
      "step": 14550
    },
    {
      "epoch": 1.278906797477225,
      "grad_norm": 0.973696231842041,
      "learning_rate": 0.00011829018663455751,
      "loss": 1.6331,
      "step": 14600
    },
    {
      "epoch": 1.2832866152768045,
      "grad_norm": 1.099837064743042,
      "learning_rate": 0.00011798916315472608,
      "loss": 1.6289,
      "step": 14650
    },
    {
      "epoch": 1.287666433076384,
      "grad_norm": 1.0316965579986572,
      "learning_rate": 0.00011768813967489465,
      "loss": 1.5559,
      "step": 14700
    },
    {
      "epoch": 1.2920462508759636,
      "grad_norm": 0.9169427156448364,
      "learning_rate": 0.00011738711619506322,
      "loss": 1.5922,
      "step": 14750
    },
    {
      "epoch": 1.296426068675543,
      "grad_norm": 1.2392091751098633,
      "learning_rate": 0.00011708609271523178,
      "loss": 1.5737,
      "step": 14800
    },
    {
      "epoch": 1.3008058864751226,
      "grad_norm": 0.9102716445922852,
      "learning_rate": 0.00011678506923540036,
      "loss": 1.6506,
      "step": 14850
    },
    {
      "epoch": 1.3051857042747022,
      "grad_norm": 1.0982558727264404,
      "learning_rate": 0.00011648404575556895,
      "loss": 1.566,
      "step": 14900
    },
    {
      "epoch": 1.3095655220742817,
      "grad_norm": 1.0040682554244995,
      "learning_rate": 0.00011618302227573752,
      "loss": 1.5326,
      "step": 14950
    },
    {
      "epoch": 1.3139453398738612,
      "grad_norm": 1.006309986114502,
      "learning_rate": 0.00011588199879590609,
      "loss": 1.5127,
      "step": 15000
    },
    {
      "epoch": 1.3183251576734407,
      "grad_norm": 1.1218757629394531,
      "learning_rate": 0.00011558097531607466,
      "loss": 1.6159,
      "step": 15050
    },
    {
      "epoch": 1.3227049754730202,
      "grad_norm": 1.065969467163086,
      "learning_rate": 0.00011527995183624322,
      "loss": 1.6117,
      "step": 15100
    },
    {
      "epoch": 1.3270847932725998,
      "grad_norm": 1.0418848991394043,
      "learning_rate": 0.0001149789283564118,
      "loss": 1.4955,
      "step": 15150
    },
    {
      "epoch": 1.3314646110721795,
      "grad_norm": 1.2132453918457031,
      "learning_rate": 0.00011467790487658039,
      "loss": 1.5471,
      "step": 15200
    },
    {
      "epoch": 1.3358444288717588,
      "grad_norm": 1.1911426782608032,
      "learning_rate": 0.00011437688139674895,
      "loss": 1.5756,
      "step": 15250
    },
    {
      "epoch": 1.3402242466713385,
      "grad_norm": 1.1851320266723633,
      "learning_rate": 0.00011407585791691753,
      "loss": 1.6278,
      "step": 15300
    },
    {
      "epoch": 1.344604064470918,
      "grad_norm": 0.9503275752067566,
      "learning_rate": 0.0001137748344370861,
      "loss": 1.6039,
      "step": 15350
    },
    {
      "epoch": 1.3489838822704976,
      "grad_norm": 1.0593632459640503,
      "learning_rate": 0.00011347381095725467,
      "loss": 1.5612,
      "step": 15400
    },
    {
      "epoch": 1.3533637000700771,
      "grad_norm": 1.4890857934951782,
      "learning_rate": 0.00011317278747742324,
      "loss": 1.5738,
      "step": 15450
    },
    {
      "epoch": 1.3577435178696566,
      "grad_norm": 0.9265629053115845,
      "learning_rate": 0.00011287176399759183,
      "loss": 1.4411,
      "step": 15500
    },
    {
      "epoch": 1.3621233356692362,
      "grad_norm": 1.0451816320419312,
      "learning_rate": 0.0001125707405177604,
      "loss": 1.6451,
      "step": 15550
    },
    {
      "epoch": 1.3665031534688157,
      "grad_norm": 1.2584394216537476,
      "learning_rate": 0.00011226971703792897,
      "loss": 1.6061,
      "step": 15600
    },
    {
      "epoch": 1.3708829712683952,
      "grad_norm": 0.9713506698608398,
      "learning_rate": 0.00011196869355809753,
      "loss": 1.5753,
      "step": 15650
    },
    {
      "epoch": 1.3752627890679747,
      "grad_norm": 1.5247431993484497,
      "learning_rate": 0.00011166767007826611,
      "loss": 1.5278,
      "step": 15700
    },
    {
      "epoch": 1.3796426068675542,
      "grad_norm": 1.1042920351028442,
      "learning_rate": 0.00011136664659843467,
      "loss": 1.5632,
      "step": 15750
    },
    {
      "epoch": 1.3840224246671338,
      "grad_norm": 0.9798741936683655,
      "learning_rate": 0.00011106562311860325,
      "loss": 1.5824,
      "step": 15800
    },
    {
      "epoch": 1.3884022424667135,
      "grad_norm": 1.080142855644226,
      "learning_rate": 0.00011076459963877184,
      "loss": 1.6441,
      "step": 15850
    },
    {
      "epoch": 1.3927820602662928,
      "grad_norm": 1.136744737625122,
      "learning_rate": 0.00011046357615894041,
      "loss": 1.5355,
      "step": 15900
    },
    {
      "epoch": 1.3971618780658726,
      "grad_norm": 1.5781508684158325,
      "learning_rate": 0.00011016255267910897,
      "loss": 1.6341,
      "step": 15950
    },
    {
      "epoch": 1.401541695865452,
      "grad_norm": 1.18716299533844,
      "learning_rate": 0.00010986152919927755,
      "loss": 1.5706,
      "step": 16000
    },
    {
      "epoch": 1.4059215136650316,
      "grad_norm": 1.0250173807144165,
      "learning_rate": 0.00010956050571944611,
      "loss": 1.5191,
      "step": 16050
    },
    {
      "epoch": 1.4103013314646111,
      "grad_norm": 1.0737937688827515,
      "learning_rate": 0.00010925948223961469,
      "loss": 1.5537,
      "step": 16100
    },
    {
      "epoch": 1.4146811492641906,
      "grad_norm": 1.0019581317901611,
      "learning_rate": 0.00010895845875978328,
      "loss": 1.5755,
      "step": 16150
    },
    {
      "epoch": 1.4190609670637702,
      "grad_norm": 1.0674667358398438,
      "learning_rate": 0.00010865743527995185,
      "loss": 1.5691,
      "step": 16200
    },
    {
      "epoch": 1.4234407848633497,
      "grad_norm": 0.9624090194702148,
      "learning_rate": 0.00010835641180012042,
      "loss": 1.5623,
      "step": 16250
    },
    {
      "epoch": 1.4278206026629292,
      "grad_norm": 0.7746119499206543,
      "learning_rate": 0.00010805538832028899,
      "loss": 1.5691,
      "step": 16300
    },
    {
      "epoch": 1.4322004204625087,
      "grad_norm": 1.0880969762802124,
      "learning_rate": 0.00010775436484045755,
      "loss": 1.5509,
      "step": 16350
    },
    {
      "epoch": 1.4365802382620882,
      "grad_norm": 1.4074418544769287,
      "learning_rate": 0.00010745334136062613,
      "loss": 1.5051,
      "step": 16400
    },
    {
      "epoch": 1.4409600560616678,
      "grad_norm": 1.239571213722229,
      "learning_rate": 0.00010715231788079472,
      "loss": 1.6392,
      "step": 16450
    },
    {
      "epoch": 1.4453398738612473,
      "grad_norm": 1.0745162963867188,
      "learning_rate": 0.00010685129440096328,
      "loss": 1.6079,
      "step": 16500
    },
    {
      "epoch": 1.4497196916608268,
      "grad_norm": 1.0030878782272339,
      "learning_rate": 0.00010655027092113186,
      "loss": 1.5273,
      "step": 16550
    },
    {
      "epoch": 1.4540995094604066,
      "grad_norm": 0.8546118140220642,
      "learning_rate": 0.00010624924744130042,
      "loss": 1.5911,
      "step": 16600
    },
    {
      "epoch": 1.4584793272599859,
      "grad_norm": 1.435864806175232,
      "learning_rate": 0.000105948223961469,
      "loss": 1.5805,
      "step": 16650
    },
    {
      "epoch": 1.4628591450595656,
      "grad_norm": 1.0191024541854858,
      "learning_rate": 0.00010564720048163757,
      "loss": 1.5191,
      "step": 16700
    },
    {
      "epoch": 1.4672389628591451,
      "grad_norm": 1.1126501560211182,
      "learning_rate": 0.00010534617700180616,
      "loss": 1.5872,
      "step": 16750
    },
    {
      "epoch": 1.4716187806587246,
      "grad_norm": 0.919356644153595,
      "learning_rate": 0.00010504515352197472,
      "loss": 1.5399,
      "step": 16800
    },
    {
      "epoch": 1.4759985984583042,
      "grad_norm": 0.9729161262512207,
      "learning_rate": 0.0001047441300421433,
      "loss": 1.5889,
      "step": 16850
    },
    {
      "epoch": 1.4803784162578837,
      "grad_norm": 1.200327754020691,
      "learning_rate": 0.00010444310656231186,
      "loss": 1.5687,
      "step": 16900
    },
    {
      "epoch": 1.4847582340574632,
      "grad_norm": 0.8676286339759827,
      "learning_rate": 0.00010414208308248044,
      "loss": 1.5305,
      "step": 16950
    },
    {
      "epoch": 1.4891380518570427,
      "grad_norm": 1.0015695095062256,
      "learning_rate": 0.000103841059602649,
      "loss": 1.6434,
      "step": 17000
    },
    {
      "epoch": 1.4935178696566223,
      "grad_norm": 1.003063440322876,
      "learning_rate": 0.00010354003612281757,
      "loss": 1.5711,
      "step": 17050
    },
    {
      "epoch": 1.4978976874562018,
      "grad_norm": 0.9907123446464539,
      "learning_rate": 0.00010323901264298616,
      "loss": 1.567,
      "step": 17100
    },
    {
      "epoch": 1.5022775052557815,
      "grad_norm": 1.096901774406433,
      "learning_rate": 0.00010293798916315474,
      "loss": 1.5989,
      "step": 17150
    },
    {
      "epoch": 1.5066573230553608,
      "grad_norm": 1.0498453378677368,
      "learning_rate": 0.0001026369656833233,
      "loss": 1.5754,
      "step": 17200
    },
    {
      "epoch": 1.5110371408549406,
      "grad_norm": 1.279315710067749,
      "learning_rate": 0.00010233594220349188,
      "loss": 1.5675,
      "step": 17250
    },
    {
      "epoch": 1.5154169586545199,
      "grad_norm": 1.0601626634597778,
      "learning_rate": 0.00010203491872366044,
      "loss": 1.5421,
      "step": 17300
    },
    {
      "epoch": 1.5197967764540996,
      "grad_norm": 1.1238672733306885,
      "learning_rate": 0.00010173389524382902,
      "loss": 1.5681,
      "step": 17350
    },
    {
      "epoch": 1.524176594253679,
      "grad_norm": 1.1029260158538818,
      "learning_rate": 0.0001014328717639976,
      "loss": 1.6599,
      "step": 17400
    },
    {
      "epoch": 1.5285564120532587,
      "grad_norm": 0.8851905465126038,
      "learning_rate": 0.00010113184828416618,
      "loss": 1.5252,
      "step": 17450
    },
    {
      "epoch": 1.5329362298528382,
      "grad_norm": 0.8580682873725891,
      "learning_rate": 0.00010083082480433474,
      "loss": 1.6401,
      "step": 17500
    },
    {
      "epoch": 1.5373160476524177,
      "grad_norm": 1.3879177570343018,
      "learning_rate": 0.00010052980132450332,
      "loss": 1.5676,
      "step": 17550
    },
    {
      "epoch": 1.5416958654519972,
      "grad_norm": 1.5072213411331177,
      "learning_rate": 0.00010022877784467188,
      "loss": 1.6437,
      "step": 17600
    },
    {
      "epoch": 1.5460756832515767,
      "grad_norm": 1.129300832748413,
      "learning_rate": 9.992775436484046e-05,
      "loss": 1.5568,
      "step": 17650
    },
    {
      "epoch": 1.5504555010511563,
      "grad_norm": 0.9278823137283325,
      "learning_rate": 9.962673088500903e-05,
      "loss": 1.6189,
      "step": 17700
    },
    {
      "epoch": 1.5548353188507358,
      "grad_norm": 1.0998470783233643,
      "learning_rate": 9.932570740517761e-05,
      "loss": 1.5225,
      "step": 17750
    },
    {
      "epoch": 1.5592151366503153,
      "grad_norm": 0.8616781234741211,
      "learning_rate": 9.902468392534619e-05,
      "loss": 1.6424,
      "step": 17800
    },
    {
      "epoch": 1.5635949544498948,
      "grad_norm": 0.9554282426834106,
      "learning_rate": 9.872366044551475e-05,
      "loss": 1.5207,
      "step": 17850
    },
    {
      "epoch": 1.5679747722494746,
      "grad_norm": 0.9565818905830383,
      "learning_rate": 9.842263696568332e-05,
      "loss": 1.6144,
      "step": 17900
    },
    {
      "epoch": 1.5723545900490539,
      "grad_norm": 1.0876692533493042,
      "learning_rate": 9.81216134858519e-05,
      "loss": 1.5866,
      "step": 17950
    },
    {
      "epoch": 1.5767344078486336,
      "grad_norm": 1.0416616201400757,
      "learning_rate": 9.782059000602048e-05,
      "loss": 1.5394,
      "step": 18000
    },
    {
      "epoch": 1.581114225648213,
      "grad_norm": 0.8728327751159668,
      "learning_rate": 9.751956652618904e-05,
      "loss": 1.5592,
      "step": 18050
    },
    {
      "epoch": 1.5854940434477927,
      "grad_norm": 1.093692660331726,
      "learning_rate": 9.721854304635763e-05,
      "loss": 1.4851,
      "step": 18100
    },
    {
      "epoch": 1.589873861247372,
      "grad_norm": 1.080755352973938,
      "learning_rate": 9.691751956652619e-05,
      "loss": 1.5606,
      "step": 18150
    },
    {
      "epoch": 1.5942536790469517,
      "grad_norm": 1.1578516960144043,
      "learning_rate": 9.661649608669477e-05,
      "loss": 1.6148,
      "step": 18200
    },
    {
      "epoch": 1.5986334968465312,
      "grad_norm": 1.1284353733062744,
      "learning_rate": 9.631547260686334e-05,
      "loss": 1.5814,
      "step": 18250
    },
    {
      "epoch": 1.6030133146461107,
      "grad_norm": 0.9272077083587646,
      "learning_rate": 9.601444912703192e-05,
      "loss": 1.545,
      "step": 18300
    },
    {
      "epoch": 1.6073931324456903,
      "grad_norm": 1.161931037902832,
      "learning_rate": 9.571342564720048e-05,
      "loss": 1.5751,
      "step": 18350
    },
    {
      "epoch": 1.6117729502452698,
      "grad_norm": 0.9867935180664062,
      "learning_rate": 9.541240216736907e-05,
      "loss": 1.5895,
      "step": 18400
    },
    {
      "epoch": 1.6161527680448493,
      "grad_norm": 1.2590816020965576,
      "learning_rate": 9.511137868753763e-05,
      "loss": 1.5361,
      "step": 18450
    },
    {
      "epoch": 1.6205325858444288,
      "grad_norm": 0.9347783327102661,
      "learning_rate": 9.481035520770621e-05,
      "loss": 1.535,
      "step": 18500
    },
    {
      "epoch": 1.6249124036440086,
      "grad_norm": 0.9858534932136536,
      "learning_rate": 9.450933172787478e-05,
      "loss": 1.51,
      "step": 18550
    },
    {
      "epoch": 1.6292922214435879,
      "grad_norm": 0.9959672093391418,
      "learning_rate": 9.420830824804336e-05,
      "loss": 1.5777,
      "step": 18600
    },
    {
      "epoch": 1.6336720392431676,
      "grad_norm": 1.2080849409103394,
      "learning_rate": 9.390728476821192e-05,
      "loss": 1.5744,
      "step": 18650
    },
    {
      "epoch": 1.638051857042747,
      "grad_norm": 1.075301170349121,
      "learning_rate": 9.36062612883805e-05,
      "loss": 1.5729,
      "step": 18700
    },
    {
      "epoch": 1.6424316748423267,
      "grad_norm": 0.8861211538314819,
      "learning_rate": 9.330523780854907e-05,
      "loss": 1.5787,
      "step": 18750
    },
    {
      "epoch": 1.646811492641906,
      "grad_norm": 1.063317894935608,
      "learning_rate": 9.300421432871765e-05,
      "loss": 1.5909,
      "step": 18800
    },
    {
      "epoch": 1.6511913104414857,
      "grad_norm": 1.1386487483978271,
      "learning_rate": 9.270319084888621e-05,
      "loss": 1.5724,
      "step": 18850
    },
    {
      "epoch": 1.6555711282410652,
      "grad_norm": 0.9243414402008057,
      "learning_rate": 9.240216736905479e-05,
      "loss": 1.6283,
      "step": 18900
    },
    {
      "epoch": 1.6599509460406447,
      "grad_norm": 1.0706342458724976,
      "learning_rate": 9.210114388922336e-05,
      "loss": 1.5964,
      "step": 18950
    },
    {
      "epoch": 1.6643307638402243,
      "grad_norm": 0.9693303108215332,
      "learning_rate": 9.180012040939194e-05,
      "loss": 1.5884,
      "step": 19000
    },
    {
      "epoch": 1.6687105816398038,
      "grad_norm": 1.4209845066070557,
      "learning_rate": 9.149909692956051e-05,
      "loss": 1.5214,
      "step": 19050
    },
    {
      "epoch": 1.6730903994393833,
      "grad_norm": 0.9192053079605103,
      "learning_rate": 9.119807344972908e-05,
      "loss": 1.6074,
      "step": 19100
    },
    {
      "epoch": 1.6774702172389628,
      "grad_norm": 1.2181373834609985,
      "learning_rate": 9.089704996989765e-05,
      "loss": 1.6095,
      "step": 19150
    },
    {
      "epoch": 1.6818500350385424,
      "grad_norm": 1.1505565643310547,
      "learning_rate": 9.059602649006623e-05,
      "loss": 1.5956,
      "step": 19200
    },
    {
      "epoch": 1.6862298528381219,
      "grad_norm": 1.1056711673736572,
      "learning_rate": 9.02950030102348e-05,
      "loss": 1.5787,
      "step": 19250
    },
    {
      "epoch": 1.6906096706377016,
      "grad_norm": 0.9970217943191528,
      "learning_rate": 8.999397953040337e-05,
      "loss": 1.5489,
      "step": 19300
    },
    {
      "epoch": 1.694989488437281,
      "grad_norm": 1.2893413305282593,
      "learning_rate": 8.969295605057196e-05,
      "loss": 1.5864,
      "step": 19350
    },
    {
      "epoch": 1.6993693062368607,
      "grad_norm": 1.0796622037887573,
      "learning_rate": 8.939193257074052e-05,
      "loss": 1.6198,
      "step": 19400
    },
    {
      "epoch": 1.70374912403644,
      "grad_norm": 0.9998568296432495,
      "learning_rate": 8.90909090909091e-05,
      "loss": 1.6414,
      "step": 19450
    },
    {
      "epoch": 1.7081289418360197,
      "grad_norm": 0.9610077738761902,
      "learning_rate": 8.878988561107767e-05,
      "loss": 1.58,
      "step": 19500
    },
    {
      "epoch": 1.712508759635599,
      "grad_norm": 0.9611478447914124,
      "learning_rate": 8.848886213124625e-05,
      "loss": 1.5718,
      "step": 19550
    },
    {
      "epoch": 1.7168885774351788,
      "grad_norm": 0.8880366683006287,
      "learning_rate": 8.818783865141481e-05,
      "loss": 1.5873,
      "step": 19600
    },
    {
      "epoch": 1.7212683952347583,
      "grad_norm": 0.9619722366333008,
      "learning_rate": 8.78868151715834e-05,
      "loss": 1.4919,
      "step": 19650
    },
    {
      "epoch": 1.7256482130343378,
      "grad_norm": 1.0640604496002197,
      "learning_rate": 8.758579169175196e-05,
      "loss": 1.6693,
      "step": 19700
    },
    {
      "epoch": 1.7300280308339173,
      "grad_norm": 0.9408689737319946,
      "learning_rate": 8.728476821192054e-05,
      "loss": 1.5739,
      "step": 19750
    },
    {
      "epoch": 1.7344078486334968,
      "grad_norm": 0.9279860854148865,
      "learning_rate": 8.698374473208911e-05,
      "loss": 1.5219,
      "step": 19800
    },
    {
      "epoch": 1.7387876664330764,
      "grad_norm": 1.1730738878250122,
      "learning_rate": 8.668272125225769e-05,
      "loss": 1.5603,
      "step": 19850
    },
    {
      "epoch": 1.7431674842326559,
      "grad_norm": 1.0409067869186401,
      "learning_rate": 8.638169777242625e-05,
      "loss": 1.5467,
      "step": 19900
    },
    {
      "epoch": 1.7475473020322354,
      "grad_norm": 1.1594935655593872,
      "learning_rate": 8.608067429259483e-05,
      "loss": 1.5173,
      "step": 19950
    },
    {
      "epoch": 1.751927119831815,
      "grad_norm": 1.002456545829773,
      "learning_rate": 8.57796508127634e-05,
      "loss": 1.5303,
      "step": 20000
    },
    {
      "epoch": 1.7563069376313947,
      "grad_norm": 1.076075553894043,
      "learning_rate": 8.547862733293198e-05,
      "loss": 1.5061,
      "step": 20050
    },
    {
      "epoch": 1.760686755430974,
      "grad_norm": 1.1228125095367432,
      "learning_rate": 8.517760385310054e-05,
      "loss": 1.5969,
      "step": 20100
    },
    {
      "epoch": 1.7650665732305537,
      "grad_norm": 1.0656344890594482,
      "learning_rate": 8.487658037326912e-05,
      "loss": 1.5748,
      "step": 20150
    },
    {
      "epoch": 1.769446391030133,
      "grad_norm": 0.9606330394744873,
      "learning_rate": 8.457555689343769e-05,
      "loss": 1.6043,
      "step": 20200
    },
    {
      "epoch": 1.7738262088297128,
      "grad_norm": 0.9700941443443298,
      "learning_rate": 8.427453341360625e-05,
      "loss": 1.5717,
      "step": 20250
    },
    {
      "epoch": 1.778206026629292,
      "grad_norm": 1.0859087705612183,
      "learning_rate": 8.397350993377484e-05,
      "loss": 1.5321,
      "step": 20300
    },
    {
      "epoch": 1.7825858444288718,
      "grad_norm": 1.2286921739578247,
      "learning_rate": 8.36724864539434e-05,
      "loss": 1.5239,
      "step": 20350
    },
    {
      "epoch": 1.7869656622284513,
      "grad_norm": 1.581420660018921,
      "learning_rate": 8.337146297411198e-05,
      "loss": 1.5584,
      "step": 20400
    },
    {
      "epoch": 1.7913454800280308,
      "grad_norm": 1.0646761655807495,
      "learning_rate": 8.307043949428056e-05,
      "loss": 1.6052,
      "step": 20450
    },
    {
      "epoch": 1.7957252978276104,
      "grad_norm": 1.0110853910446167,
      "learning_rate": 8.276941601444913e-05,
      "loss": 1.6387,
      "step": 20500
    },
    {
      "epoch": 1.80010511562719,
      "grad_norm": 0.9660242199897766,
      "learning_rate": 8.24683925346177e-05,
      "loss": 1.5576,
      "step": 20550
    },
    {
      "epoch": 1.8044849334267694,
      "grad_norm": 1.0819889307022095,
      "learning_rate": 8.216736905478628e-05,
      "loss": 1.4833,
      "step": 20600
    },
    {
      "epoch": 1.808864751226349,
      "grad_norm": 1.187778115272522,
      "learning_rate": 8.186634557495485e-05,
      "loss": 1.5317,
      "step": 20650
    },
    {
      "epoch": 1.8132445690259287,
      "grad_norm": 1.4042924642562866,
      "learning_rate": 8.156532209512342e-05,
      "loss": 1.5839,
      "step": 20700
    },
    {
      "epoch": 1.817624386825508,
      "grad_norm": 1.023697853088379,
      "learning_rate": 8.1264298615292e-05,
      "loss": 1.6719,
      "step": 20750
    },
    {
      "epoch": 1.8220042046250877,
      "grad_norm": 1.2043901681900024,
      "learning_rate": 8.096327513546057e-05,
      "loss": 1.638,
      "step": 20800
    },
    {
      "epoch": 1.826384022424667,
      "grad_norm": 0.9980692863464355,
      "learning_rate": 8.066225165562914e-05,
      "loss": 1.5233,
      "step": 20850
    },
    {
      "epoch": 1.8307638402242468,
      "grad_norm": 0.9922679662704468,
      "learning_rate": 8.036122817579773e-05,
      "loss": 1.5682,
      "step": 20900
    },
    {
      "epoch": 1.835143658023826,
      "grad_norm": 0.8628540635108948,
      "learning_rate": 8.006020469596629e-05,
      "loss": 1.5937,
      "step": 20950
    },
    {
      "epoch": 1.8395234758234058,
      "grad_norm": 0.9702863693237305,
      "learning_rate": 7.975918121613486e-05,
      "loss": 1.6896,
      "step": 21000
    },
    {
      "epoch": 1.8439032936229853,
      "grad_norm": 1.018959879875183,
      "learning_rate": 7.945815773630344e-05,
      "loss": 1.6455,
      "step": 21050
    },
    {
      "epoch": 1.8482831114225649,
      "grad_norm": 0.8962773084640503,
      "learning_rate": 7.915713425647202e-05,
      "loss": 1.595,
      "step": 21100
    },
    {
      "epoch": 1.8526629292221444,
      "grad_norm": 0.9788581728935242,
      "learning_rate": 7.885611077664058e-05,
      "loss": 1.6436,
      "step": 21150
    },
    {
      "epoch": 1.857042747021724,
      "grad_norm": 1.1091394424438477,
      "learning_rate": 7.855508729680915e-05,
      "loss": 1.5225,
      "step": 21200
    },
    {
      "epoch": 1.8614225648213034,
      "grad_norm": 1.0559685230255127,
      "learning_rate": 7.825406381697773e-05,
      "loss": 1.5659,
      "step": 21250
    },
    {
      "epoch": 1.865802382620883,
      "grad_norm": 1.318290114402771,
      "learning_rate": 7.795304033714629e-05,
      "loss": 1.6535,
      "step": 21300
    },
    {
      "epoch": 1.8701822004204625,
      "grad_norm": 0.9883343577384949,
      "learning_rate": 7.765201685731487e-05,
      "loss": 1.5245,
      "step": 21350
    },
    {
      "epoch": 1.874562018220042,
      "grad_norm": 1.1170744895935059,
      "learning_rate": 7.735099337748344e-05,
      "loss": 1.6002,
      "step": 21400
    },
    {
      "epoch": 1.8789418360196217,
      "grad_norm": 1.176936388015747,
      "learning_rate": 7.704996989765202e-05,
      "loss": 1.6696,
      "step": 21450
    },
    {
      "epoch": 1.883321653819201,
      "grad_norm": 1.302302360534668,
      "learning_rate": 7.674894641782058e-05,
      "loss": 1.6291,
      "step": 21500
    },
    {
      "epoch": 1.8877014716187808,
      "grad_norm": 1.1410624980926514,
      "learning_rate": 7.644792293798917e-05,
      "loss": 1.5716,
      "step": 21550
    },
    {
      "epoch": 1.89208128941836,
      "grad_norm": 0.9477565884590149,
      "learning_rate": 7.614689945815773e-05,
      "loss": 1.4734,
      "step": 21600
    },
    {
      "epoch": 1.8964611072179398,
      "grad_norm": 1.1182748079299927,
      "learning_rate": 7.584587597832631e-05,
      "loss": 1.5727,
      "step": 21650
    },
    {
      "epoch": 1.9008409250175191,
      "grad_norm": 1.1760246753692627,
      "learning_rate": 7.554485249849489e-05,
      "loss": 1.6415,
      "step": 21700
    },
    {
      "epoch": 1.9052207428170989,
      "grad_norm": 0.9678730964660645,
      "learning_rate": 7.524382901866346e-05,
      "loss": 1.5443,
      "step": 21750
    },
    {
      "epoch": 1.9096005606166784,
      "grad_norm": 1.0783129930496216,
      "learning_rate": 7.494280553883202e-05,
      "loss": 1.5857,
      "step": 21800
    },
    {
      "epoch": 1.913980378416258,
      "grad_norm": 1.05893075466156,
      "learning_rate": 7.464178205900061e-05,
      "loss": 1.6104,
      "step": 21850
    },
    {
      "epoch": 1.9183601962158374,
      "grad_norm": 0.921094536781311,
      "learning_rate": 7.434075857916918e-05,
      "loss": 1.5476,
      "step": 21900
    },
    {
      "epoch": 1.922740014015417,
      "grad_norm": 1.1262918710708618,
      "learning_rate": 7.403973509933775e-05,
      "loss": 1.5566,
      "step": 21950
    },
    {
      "epoch": 1.9271198318149965,
      "grad_norm": 1.0240695476531982,
      "learning_rate": 7.373871161950633e-05,
      "loss": 1.5993,
      "step": 22000
    },
    {
      "epoch": 1.931499649614576,
      "grad_norm": 1.0109847784042358,
      "learning_rate": 7.34376881396749e-05,
      "loss": 1.6232,
      "step": 22050
    },
    {
      "epoch": 1.9358794674141557,
      "grad_norm": 0.9997427463531494,
      "learning_rate": 7.313666465984347e-05,
      "loss": 1.6787,
      "step": 22100
    },
    {
      "epoch": 1.940259285213735,
      "grad_norm": 1.0477368831634521,
      "learning_rate": 7.283564118001206e-05,
      "loss": 1.4851,
      "step": 22150
    },
    {
      "epoch": 1.9446391030133148,
      "grad_norm": 0.916252851486206,
      "learning_rate": 7.253461770018062e-05,
      "loss": 1.609,
      "step": 22200
    },
    {
      "epoch": 1.949018920812894,
      "grad_norm": 0.8456435799598694,
      "learning_rate": 7.22335942203492e-05,
      "loss": 1.5267,
      "step": 22250
    },
    {
      "epoch": 1.9533987386124738,
      "grad_norm": 1.3297897577285767,
      "learning_rate": 7.193257074051777e-05,
      "loss": 1.5506,
      "step": 22300
    },
    {
      "epoch": 1.9577785564120531,
      "grad_norm": 0.9965943098068237,
      "learning_rate": 7.163154726068633e-05,
      "loss": 1.5905,
      "step": 22350
    },
    {
      "epoch": 1.9621583742116329,
      "grad_norm": 1.1543062925338745,
      "learning_rate": 7.133052378085491e-05,
      "loss": 1.5773,
      "step": 22400
    },
    {
      "epoch": 1.9665381920112124,
      "grad_norm": 1.0050262212753296,
      "learning_rate": 7.102950030102348e-05,
      "loss": 1.5116,
      "step": 22450
    },
    {
      "epoch": 1.970918009810792,
      "grad_norm": 0.9336248636245728,
      "learning_rate": 7.072847682119206e-05,
      "loss": 1.5123,
      "step": 22500
    },
    {
      "epoch": 1.9752978276103714,
      "grad_norm": 1.2085715532302856,
      "learning_rate": 7.042745334136062e-05,
      "loss": 1.5813,
      "step": 22550
    },
    {
      "epoch": 1.979677645409951,
      "grad_norm": 0.9317188858985901,
      "learning_rate": 7.012642986152921e-05,
      "loss": 1.5374,
      "step": 22600
    },
    {
      "epoch": 1.9840574632095305,
      "grad_norm": 1.0661334991455078,
      "learning_rate": 6.982540638169777e-05,
      "loss": 1.6057,
      "step": 22650
    },
    {
      "epoch": 1.98843728100911,
      "grad_norm": 1.5738525390625,
      "learning_rate": 6.952438290186635e-05,
      "loss": 1.6075,
      "step": 22700
    },
    {
      "epoch": 1.9928170988086895,
      "grad_norm": 1.0720552206039429,
      "learning_rate": 6.922335942203491e-05,
      "loss": 1.5199,
      "step": 22750
    },
    {
      "epoch": 1.997196916608269,
      "grad_norm": 1.0459922552108765,
      "learning_rate": 6.89223359422035e-05,
      "loss": 1.538,
      "step": 22800
    },
    {
      "epoch": 2.001576734407849,
      "grad_norm": 0.5912075638771057,
      "learning_rate": 6.862131246237206e-05,
      "loss": 1.4943,
      "step": 22850
    },
    {
      "epoch": 2.005956552207428,
      "grad_norm": 0.8111058473587036,
      "learning_rate": 6.832028898254064e-05,
      "loss": 1.638,
      "step": 22900
    },
    {
      "epoch": 2.010336370007008,
      "grad_norm": 0.9308294653892517,
      "learning_rate": 6.801926550270921e-05,
      "loss": 1.5482,
      "step": 22950
    },
    {
      "epoch": 2.014716187806587,
      "grad_norm": 0.8521226644515991,
      "learning_rate": 6.771824202287779e-05,
      "loss": 1.5454,
      "step": 23000
    },
    {
      "epoch": 2.019096005606167,
      "grad_norm": 0.9030941724777222,
      "learning_rate": 6.741721854304635e-05,
      "loss": 1.5304,
      "step": 23050
    },
    {
      "epoch": 2.023475823405746,
      "grad_norm": 0.9942550659179688,
      "learning_rate": 6.711619506321494e-05,
      "loss": 1.4705,
      "step": 23100
    },
    {
      "epoch": 2.027855641205326,
      "grad_norm": 0.7326646447181702,
      "learning_rate": 6.68151715833835e-05,
      "loss": 1.5103,
      "step": 23150
    },
    {
      "epoch": 2.032235459004905,
      "grad_norm": 0.9467636942863464,
      "learning_rate": 6.651414810355208e-05,
      "loss": 1.5622,
      "step": 23200
    },
    {
      "epoch": 2.036615276804485,
      "grad_norm": 0.6864845156669617,
      "learning_rate": 6.621312462372066e-05,
      "loss": 1.5691,
      "step": 23250
    },
    {
      "epoch": 2.0409950946040643,
      "grad_norm": 0.7879392504692078,
      "learning_rate": 6.591210114388923e-05,
      "loss": 1.5777,
      "step": 23300
    },
    {
      "epoch": 2.045374912403644,
      "grad_norm": 0.8525217175483704,
      "learning_rate": 6.56110776640578e-05,
      "loss": 1.4487,
      "step": 23350
    },
    {
      "epoch": 2.0497547302032237,
      "grad_norm": 0.8426555395126343,
      "learning_rate": 6.531005418422637e-05,
      "loss": 1.4536,
      "step": 23400
    },
    {
      "epoch": 2.054134548002803,
      "grad_norm": 0.9917440414428711,
      "learning_rate": 6.500903070439495e-05,
      "loss": 1.5826,
      "step": 23450
    },
    {
      "epoch": 2.058514365802383,
      "grad_norm": 0.7805219292640686,
      "learning_rate": 6.470800722456352e-05,
      "loss": 1.5316,
      "step": 23500
    },
    {
      "epoch": 2.062894183601962,
      "grad_norm": 0.7245782017707825,
      "learning_rate": 6.44069837447321e-05,
      "loss": 1.4833,
      "step": 23550
    },
    {
      "epoch": 2.067274001401542,
      "grad_norm": 0.8432750701904297,
      "learning_rate": 6.410596026490066e-05,
      "loss": 1.5079,
      "step": 23600
    },
    {
      "epoch": 2.071653819201121,
      "grad_norm": 0.8241874575614929,
      "learning_rate": 6.380493678506924e-05,
      "loss": 1.5797,
      "step": 23650
    },
    {
      "epoch": 2.076033637000701,
      "grad_norm": 1.0322068929672241,
      "learning_rate": 6.350391330523781e-05,
      "loss": 1.604,
      "step": 23700
    },
    {
      "epoch": 2.08041345480028,
      "grad_norm": 0.7452061176300049,
      "learning_rate": 6.320288982540639e-05,
      "loss": 1.5057,
      "step": 23750
    },
    {
      "epoch": 2.08479327259986,
      "grad_norm": 0.8493556380271912,
      "learning_rate": 6.290186634557495e-05,
      "loss": 1.6146,
      "step": 23800
    },
    {
      "epoch": 2.089173090399439,
      "grad_norm": 0.8478427529335022,
      "learning_rate": 6.260084286574354e-05,
      "loss": 1.6049,
      "step": 23850
    },
    {
      "epoch": 2.093552908199019,
      "grad_norm": 0.7911936640739441,
      "learning_rate": 6.22998193859121e-05,
      "loss": 1.4923,
      "step": 23900
    },
    {
      "epoch": 2.0979327259985983,
      "grad_norm": 1.0518124103546143,
      "learning_rate": 6.199879590608068e-05,
      "loss": 1.5374,
      "step": 23950
    },
    {
      "epoch": 2.102312543798178,
      "grad_norm": 0.9427656531333923,
      "learning_rate": 6.169777242624924e-05,
      "loss": 1.6311,
      "step": 24000
    },
    {
      "epoch": 2.1066923615977577,
      "grad_norm": 0.9840878248214722,
      "learning_rate": 6.139674894641783e-05,
      "loss": 1.5249,
      "step": 24050
    },
    {
      "epoch": 2.111072179397337,
      "grad_norm": 0.7684658169746399,
      "learning_rate": 6.109572546658639e-05,
      "loss": 1.5083,
      "step": 24100
    },
    {
      "epoch": 2.115451997196917,
      "grad_norm": 0.9647395610809326,
      "learning_rate": 6.079470198675497e-05,
      "loss": 1.4574,
      "step": 24150
    },
    {
      "epoch": 2.119831814996496,
      "grad_norm": 0.9031826853752136,
      "learning_rate": 6.049367850692354e-05,
      "loss": 1.604,
      "step": 24200
    },
    {
      "epoch": 2.124211632796076,
      "grad_norm": 1.3321406841278076,
      "learning_rate": 6.019265502709211e-05,
      "loss": 1.4842,
      "step": 24250
    },
    {
      "epoch": 2.128591450595655,
      "grad_norm": 0.8042626976966858,
      "learning_rate": 5.989163154726068e-05,
      "loss": 1.4722,
      "step": 24300
    },
    {
      "epoch": 2.132971268395235,
      "grad_norm": 0.7149448990821838,
      "learning_rate": 5.9590608067429264e-05,
      "loss": 1.4147,
      "step": 24350
    },
    {
      "epoch": 2.137351086194814,
      "grad_norm": 0.8477080464363098,
      "learning_rate": 5.928958458759783e-05,
      "loss": 1.501,
      "step": 24400
    },
    {
      "epoch": 2.141730903994394,
      "grad_norm": 0.9347062110900879,
      "learning_rate": 5.89885611077664e-05,
      "loss": 1.5494,
      "step": 24450
    },
    {
      "epoch": 2.146110721793973,
      "grad_norm": 1.0791139602661133,
      "learning_rate": 5.8687537627934985e-05,
      "loss": 1.5388,
      "step": 24500
    },
    {
      "epoch": 2.150490539593553,
      "grad_norm": 0.8394634127616882,
      "learning_rate": 5.8386514148103554e-05,
      "loss": 1.5588,
      "step": 24550
    },
    {
      "epoch": 2.1548703573931323,
      "grad_norm": 0.853500485420227,
      "learning_rate": 5.808549066827212e-05,
      "loss": 1.6266,
      "step": 24600
    },
    {
      "epoch": 2.159250175192712,
      "grad_norm": 0.7649693489074707,
      "learning_rate": 5.7784467188440706e-05,
      "loss": 1.5393,
      "step": 24650
    },
    {
      "epoch": 2.1636299929922913,
      "grad_norm": 0.764248788356781,
      "learning_rate": 5.7483443708609275e-05,
      "loss": 1.6189,
      "step": 24700
    },
    {
      "epoch": 2.168009810791871,
      "grad_norm": 0.7803455591201782,
      "learning_rate": 5.7182420228777844e-05,
      "loss": 1.4499,
      "step": 24750
    },
    {
      "epoch": 2.172389628591451,
      "grad_norm": 0.8076237440109253,
      "learning_rate": 5.6881396748946427e-05,
      "loss": 1.5344,
      "step": 24800
    },
    {
      "epoch": 2.17676944639103,
      "grad_norm": 0.8365883827209473,
      "learning_rate": 5.6580373269114996e-05,
      "loss": 1.4609,
      "step": 24850
    },
    {
      "epoch": 2.18114926419061,
      "grad_norm": 0.8517405390739441,
      "learning_rate": 5.6279349789283565e-05,
      "loss": 1.5063,
      "step": 24900
    },
    {
      "epoch": 2.185529081990189,
      "grad_norm": 1.1089725494384766,
      "learning_rate": 5.597832630945215e-05,
      "loss": 1.4927,
      "step": 24950
    },
    {
      "epoch": 2.189908899789769,
      "grad_norm": 0.7461808919906616,
      "learning_rate": 5.5677302829620716e-05,
      "loss": 1.438,
      "step": 25000
    },
    {
      "epoch": 2.194288717589348,
      "grad_norm": 0.8176751136779785,
      "learning_rate": 5.5376279349789286e-05,
      "loss": 1.4942,
      "step": 25050
    },
    {
      "epoch": 2.198668535388928,
      "grad_norm": 0.7761714458465576,
      "learning_rate": 5.507525586995786e-05,
      "loss": 1.4993,
      "step": 25100
    },
    {
      "epoch": 2.2030483531885072,
      "grad_norm": 0.7366490364074707,
      "learning_rate": 5.477423239012644e-05,
      "loss": 1.567,
      "step": 25150
    },
    {
      "epoch": 2.207428170988087,
      "grad_norm": 0.9504721164703369,
      "learning_rate": 5.4473208910295006e-05,
      "loss": 1.5609,
      "step": 25200
    },
    {
      "epoch": 2.2118079887876663,
      "grad_norm": 0.7509013414382935,
      "learning_rate": 5.4172185430463575e-05,
      "loss": 1.547,
      "step": 25250
    },
    {
      "epoch": 2.216187806587246,
      "grad_norm": 1.0199962854385376,
      "learning_rate": 5.387116195063215e-05,
      "loss": 1.5213,
      "step": 25300
    },
    {
      "epoch": 2.2205676243868253,
      "grad_norm": 0.8433205485343933,
      "learning_rate": 5.357013847080072e-05,
      "loss": 1.5077,
      "step": 25350
    },
    {
      "epoch": 2.224947442186405,
      "grad_norm": 0.9982761144638062,
      "learning_rate": 5.3269114990969296e-05,
      "loss": 1.5224,
      "step": 25400
    },
    {
      "epoch": 2.2293272599859844,
      "grad_norm": 0.921866774559021,
      "learning_rate": 5.296809151113787e-05,
      "loss": 1.568,
      "step": 25450
    },
    {
      "epoch": 2.233707077785564,
      "grad_norm": 0.8048225045204163,
      "learning_rate": 5.266706803130644e-05,
      "loss": 1.4979,
      "step": 25500
    },
    {
      "epoch": 2.238086895585144,
      "grad_norm": 1.1633797883987427,
      "learning_rate": 5.236604455147501e-05,
      "loss": 1.5931,
      "step": 25550
    },
    {
      "epoch": 2.242466713384723,
      "grad_norm": 0.8646876811981201,
      "learning_rate": 5.206502107164359e-05,
      "loss": 1.5598,
      "step": 25600
    },
    {
      "epoch": 2.246846531184303,
      "grad_norm": 0.9573482275009155,
      "learning_rate": 5.176399759181216e-05,
      "loss": 1.5836,
      "step": 25650
    },
    {
      "epoch": 2.251226348983882,
      "grad_norm": 1.0124139785766602,
      "learning_rate": 5.146297411198073e-05,
      "loss": 1.5538,
      "step": 25700
    },
    {
      "epoch": 2.255606166783462,
      "grad_norm": 0.8676666021347046,
      "learning_rate": 5.1161950632149314e-05,
      "loss": 1.5775,
      "step": 25750
    },
    {
      "epoch": 2.2599859845830412,
      "grad_norm": 0.8746645450592041,
      "learning_rate": 5.086092715231788e-05,
      "loss": 1.5147,
      "step": 25800
    },
    {
      "epoch": 2.264365802382621,
      "grad_norm": 0.7703273296356201,
      "learning_rate": 5.055990367248645e-05,
      "loss": 1.5852,
      "step": 25850
    },
    {
      "epoch": 2.2687456201822003,
      "grad_norm": 1.0961909294128418,
      "learning_rate": 5.0258880192655034e-05,
      "loss": 1.5714,
      "step": 25900
    },
    {
      "epoch": 2.27312543798178,
      "grad_norm": 0.8219934701919556,
      "learning_rate": 4.9957856712823604e-05,
      "loss": 1.5381,
      "step": 25950
    },
    {
      "epoch": 2.2775052557813593,
      "grad_norm": 0.790363609790802,
      "learning_rate": 4.965683323299218e-05,
      "loss": 1.5474,
      "step": 26000
    },
    {
      "epoch": 2.281885073580939,
      "grad_norm": 0.6844293475151062,
      "learning_rate": 4.935580975316075e-05,
      "loss": 1.5173,
      "step": 26050
    },
    {
      "epoch": 2.286264891380519,
      "grad_norm": 0.7608484029769897,
      "learning_rate": 4.9054786273329324e-05,
      "loss": 1.5413,
      "step": 26100
    },
    {
      "epoch": 2.290644709180098,
      "grad_norm": 0.6006502509117126,
      "learning_rate": 4.8753762793497893e-05,
      "loss": 1.4984,
      "step": 26150
    },
    {
      "epoch": 2.2950245269796774,
      "grad_norm": 0.7681960463523865,
      "learning_rate": 4.845273931366647e-05,
      "loss": 1.5281,
      "step": 26200
    },
    {
      "epoch": 2.299404344779257,
      "grad_norm": 0.8452368378639221,
      "learning_rate": 4.8151715833835045e-05,
      "loss": 1.5724,
      "step": 26250
    },
    {
      "epoch": 2.303784162578837,
      "grad_norm": 0.8869666457176208,
      "learning_rate": 4.7850692354003614e-05,
      "loss": 1.5207,
      "step": 26300
    },
    {
      "epoch": 2.308163980378416,
      "grad_norm": 0.7465249300003052,
      "learning_rate": 4.754966887417219e-05,
      "loss": 1.6328,
      "step": 26350
    },
    {
      "epoch": 2.312543798177996,
      "grad_norm": 1.4093939065933228,
      "learning_rate": 4.724864539434076e-05,
      "loss": 1.4915,
      "step": 26400
    },
    {
      "epoch": 2.3169236159775752,
      "grad_norm": 0.8004738092422485,
      "learning_rate": 4.6947621914509335e-05,
      "loss": 1.503,
      "step": 26450
    },
    {
      "epoch": 2.321303433777155,
      "grad_norm": 0.5665629506111145,
      "learning_rate": 4.6646598434677904e-05,
      "loss": 1.4788,
      "step": 26500
    },
    {
      "epoch": 2.3256832515767343,
      "grad_norm": 0.8308345079421997,
      "learning_rate": 4.634557495484648e-05,
      "loss": 1.5334,
      "step": 26550
    },
    {
      "epoch": 2.330063069376314,
      "grad_norm": 0.8933815956115723,
      "learning_rate": 4.604455147501505e-05,
      "loss": 1.5556,
      "step": 26600
    },
    {
      "epoch": 2.3344428871758933,
      "grad_norm": 0.8045844435691833,
      "learning_rate": 4.5743527995183625e-05,
      "loss": 1.4993,
      "step": 26650
    },
    {
      "epoch": 2.338822704975473,
      "grad_norm": 0.8610736131668091,
      "learning_rate": 4.54425045153522e-05,
      "loss": 1.5987,
      "step": 26700
    },
    {
      "epoch": 2.3432025227750524,
      "grad_norm": 0.8459040522575378,
      "learning_rate": 4.514148103552077e-05,
      "loss": 1.5467,
      "step": 26750
    },
    {
      "epoch": 2.347582340574632,
      "grad_norm": 1.004062533378601,
      "learning_rate": 4.4840457555689346e-05,
      "loss": 1.5902,
      "step": 26800
    },
    {
      "epoch": 2.351962158374212,
      "grad_norm": 0.8670059442520142,
      "learning_rate": 4.4539434075857915e-05,
      "loss": 1.5403,
      "step": 26850
    },
    {
      "epoch": 2.356341976173791,
      "grad_norm": 0.8006023168563843,
      "learning_rate": 4.423841059602649e-05,
      "loss": 1.5185,
      "step": 26900
    },
    {
      "epoch": 2.360721793973371,
      "grad_norm": 0.9813383221626282,
      "learning_rate": 4.3937387116195067e-05,
      "loss": 1.5924,
      "step": 26950
    },
    {
      "epoch": 2.36510161177295,
      "grad_norm": 0.9746068716049194,
      "learning_rate": 4.3636363636363636e-05,
      "loss": 1.607,
      "step": 27000
    },
    {
      "epoch": 2.36948142957253,
      "grad_norm": 0.717689037322998,
      "learning_rate": 4.333534015653221e-05,
      "loss": 1.4289,
      "step": 27050
    },
    {
      "epoch": 2.3738612473721092,
      "grad_norm": 0.7223944067955017,
      "learning_rate": 4.303431667670079e-05,
      "loss": 1.5579,
      "step": 27100
    },
    {
      "epoch": 2.378241065171689,
      "grad_norm": 0.9212134480476379,
      "learning_rate": 4.2733293196869356e-05,
      "loss": 1.58,
      "step": 27150
    },
    {
      "epoch": 2.3826208829712683,
      "grad_norm": 0.8057319521903992,
      "learning_rate": 4.243226971703793e-05,
      "loss": 1.5762,
      "step": 27200
    },
    {
      "epoch": 2.387000700770848,
      "grad_norm": 0.9927838444709778,
      "learning_rate": 4.213124623720651e-05,
      "loss": 1.4902,
      "step": 27250
    },
    {
      "epoch": 2.3913805185704273,
      "grad_norm": 0.8093670010566711,
      "learning_rate": 4.183022275737508e-05,
      "loss": 1.5408,
      "step": 27300
    },
    {
      "epoch": 2.395760336370007,
      "grad_norm": 0.9168815612792969,
      "learning_rate": 4.152919927754365e-05,
      "loss": 1.552,
      "step": 27350
    },
    {
      "epoch": 2.4001401541695864,
      "grad_norm": 0.9516090154647827,
      "learning_rate": 4.122817579771223e-05,
      "loss": 1.4907,
      "step": 27400
    },
    {
      "epoch": 2.404519971969166,
      "grad_norm": 0.6936975717544556,
      "learning_rate": 4.09271523178808e-05,
      "loss": 1.5476,
      "step": 27450
    },
    {
      "epoch": 2.4088997897687454,
      "grad_norm": 0.7073224782943726,
      "learning_rate": 4.0626128838049374e-05,
      "loss": 1.4688,
      "step": 27500
    },
    {
      "epoch": 2.413279607568325,
      "grad_norm": 0.9737555980682373,
      "learning_rate": 4.032510535821794e-05,
      "loss": 1.6683,
      "step": 27550
    },
    {
      "epoch": 2.417659425367905,
      "grad_norm": 0.9320167899131775,
      "learning_rate": 4.002408187838652e-05,
      "loss": 1.5558,
      "step": 27600
    },
    {
      "epoch": 2.422039243167484,
      "grad_norm": 0.723536491394043,
      "learning_rate": 3.972305839855509e-05,
      "loss": 1.5686,
      "step": 27650
    },
    {
      "epoch": 2.426419060967064,
      "grad_norm": 0.8827565312385559,
      "learning_rate": 3.9422034918723664e-05,
      "loss": 1.5456,
      "step": 27700
    },
    {
      "epoch": 2.4307988787666432,
      "grad_norm": 0.9820276498794556,
      "learning_rate": 3.912101143889223e-05,
      "loss": 1.4896,
      "step": 27750
    },
    {
      "epoch": 2.435178696566223,
      "grad_norm": 0.8826998472213745,
      "learning_rate": 3.881998795906081e-05,
      "loss": 1.5332,
      "step": 27800
    },
    {
      "epoch": 2.4395585143658023,
      "grad_norm": 0.8572302460670471,
      "learning_rate": 3.851896447922938e-05,
      "loss": 1.5497,
      "step": 27850
    },
    {
      "epoch": 2.443938332165382,
      "grad_norm": 0.8431278467178345,
      "learning_rate": 3.8217940999397954e-05,
      "loss": 1.4965,
      "step": 27900
    },
    {
      "epoch": 2.4483181499649613,
      "grad_norm": 0.6237239241600037,
      "learning_rate": 3.791691751956653e-05,
      "loss": 1.5773,
      "step": 27950
    },
    {
      "epoch": 2.452697967764541,
      "grad_norm": 0.9707909822463989,
      "learning_rate": 3.76158940397351e-05,
      "loss": 1.5475,
      "step": 28000
    },
    {
      "epoch": 2.4570777855641204,
      "grad_norm": 0.8868750333786011,
      "learning_rate": 3.7314870559903674e-05,
      "loss": 1.5205,
      "step": 28050
    },
    {
      "epoch": 2.4614576033637,
      "grad_norm": 1.0419565439224243,
      "learning_rate": 3.7013847080072244e-05,
      "loss": 1.5369,
      "step": 28100
    },
    {
      "epoch": 2.4658374211632794,
      "grad_norm": 0.747904896736145,
      "learning_rate": 3.671282360024082e-05,
      "loss": 1.5698,
      "step": 28150
    },
    {
      "epoch": 2.470217238962859,
      "grad_norm": 0.8715327978134155,
      "learning_rate": 3.6411800120409395e-05,
      "loss": 1.6054,
      "step": 28200
    },
    {
      "epoch": 2.4745970567624385,
      "grad_norm": 0.8231952786445618,
      "learning_rate": 3.6110776640577964e-05,
      "loss": 1.5649,
      "step": 28250
    },
    {
      "epoch": 2.478976874562018,
      "grad_norm": 0.7892783880233765,
      "learning_rate": 3.580975316074654e-05,
      "loss": 1.5405,
      "step": 28300
    },
    {
      "epoch": 2.483356692361598,
      "grad_norm": 0.8011445999145508,
      "learning_rate": 3.5508729680915116e-05,
      "loss": 1.5077,
      "step": 28350
    },
    {
      "epoch": 2.4877365101611773,
      "grad_norm": 0.9125519394874573,
      "learning_rate": 3.5207706201083685e-05,
      "loss": 1.5089,
      "step": 28400
    },
    {
      "epoch": 2.492116327960757,
      "grad_norm": 0.804273247718811,
      "learning_rate": 3.490668272125226e-05,
      "loss": 1.5582,
      "step": 28450
    },
    {
      "epoch": 2.4964961457603363,
      "grad_norm": 0.7739760279655457,
      "learning_rate": 3.460565924142084e-05,
      "loss": 1.5388,
      "step": 28500
    },
    {
      "epoch": 2.500875963559916,
      "grad_norm": 0.7530128955841064,
      "learning_rate": 3.4304635761589406e-05,
      "loss": 1.4529,
      "step": 28550
    },
    {
      "epoch": 2.5052557813594953,
      "grad_norm": 1.0110408067703247,
      "learning_rate": 3.400361228175798e-05,
      "loss": 1.5787,
      "step": 28600
    },
    {
      "epoch": 2.509635599159075,
      "grad_norm": 0.7674755454063416,
      "learning_rate": 3.370258880192656e-05,
      "loss": 1.5518,
      "step": 28650
    },
    {
      "epoch": 2.5140154169586544,
      "grad_norm": 0.9778756499290466,
      "learning_rate": 3.340156532209513e-05,
      "loss": 1.4895,
      "step": 28700
    },
    {
      "epoch": 2.518395234758234,
      "grad_norm": 0.849465548992157,
      "learning_rate": 3.31005418422637e-05,
      "loss": 1.5809,
      "step": 28750
    },
    {
      "epoch": 2.5227750525578134,
      "grad_norm": 0.7039919495582581,
      "learning_rate": 3.279951836243227e-05,
      "loss": 1.5639,
      "step": 28800
    },
    {
      "epoch": 2.527154870357393,
      "grad_norm": 0.942156970500946,
      "learning_rate": 3.249849488260084e-05,
      "loss": 1.5031,
      "step": 28850
    },
    {
      "epoch": 2.531534688156973,
      "grad_norm": 1.0150485038757324,
      "learning_rate": 3.219747140276942e-05,
      "loss": 1.5694,
      "step": 28900
    },
    {
      "epoch": 2.535914505956552,
      "grad_norm": 0.7685465812683105,
      "learning_rate": 3.1896447922937986e-05,
      "loss": 1.5552,
      "step": 28950
    },
    {
      "epoch": 2.5402943237561315,
      "grad_norm": 0.9015955328941345,
      "learning_rate": 3.159542444310656e-05,
      "loss": 1.5159,
      "step": 29000
    },
    {
      "epoch": 2.5446741415557113,
      "grad_norm": 0.7310477495193481,
      "learning_rate": 3.129440096327514e-05,
      "loss": 1.5305,
      "step": 29050
    },
    {
      "epoch": 2.549053959355291,
      "grad_norm": 0.7430494427680969,
      "learning_rate": 3.0993377483443707e-05,
      "loss": 1.5786,
      "step": 29100
    },
    {
      "epoch": 2.5534337771548703,
      "grad_norm": 0.8182600736618042,
      "learning_rate": 3.069235400361228e-05,
      "loss": 1.4999,
      "step": 29150
    },
    {
      "epoch": 2.55781359495445,
      "grad_norm": 0.8126798868179321,
      "learning_rate": 3.0391330523780858e-05,
      "loss": 1.5487,
      "step": 29200
    },
    {
      "epoch": 2.5621934127540293,
      "grad_norm": 0.7919415831565857,
      "learning_rate": 3.0090307043949427e-05,
      "loss": 1.5072,
      "step": 29250
    },
    {
      "epoch": 2.566573230553609,
      "grad_norm": 0.7960209250450134,
      "learning_rate": 2.9789283564118003e-05,
      "loss": 1.5767,
      "step": 29300
    },
    {
      "epoch": 2.5709530483531884,
      "grad_norm": 0.8889176845550537,
      "learning_rate": 2.9488260084286572e-05,
      "loss": 1.5278,
      "step": 29350
    },
    {
      "epoch": 2.575332866152768,
      "grad_norm": 0.6278218030929565,
      "learning_rate": 2.9187236604455148e-05,
      "loss": 1.4961,
      "step": 29400
    },
    {
      "epoch": 2.5797126839523474,
      "grad_norm": 0.9181122779846191,
      "learning_rate": 2.8886213124623724e-05,
      "loss": 1.5521,
      "step": 29450
    },
    {
      "epoch": 2.584092501751927,
      "grad_norm": 0.6661888957023621,
      "learning_rate": 2.8585189644792293e-05,
      "loss": 1.5142,
      "step": 29500
    },
    {
      "epoch": 2.5884723195515065,
      "grad_norm": 0.6860754489898682,
      "learning_rate": 2.828416616496087e-05,
      "loss": 1.4628,
      "step": 29550
    },
    {
      "epoch": 2.592852137351086,
      "grad_norm": 0.7974706888198853,
      "learning_rate": 2.7983142685129445e-05,
      "loss": 1.5667,
      "step": 29600
    },
    {
      "epoch": 2.597231955150666,
      "grad_norm": 0.7813949584960938,
      "learning_rate": 2.7682119205298014e-05,
      "loss": 1.5392,
      "step": 29650
    },
    {
      "epoch": 2.6016117729502453,
      "grad_norm": 0.8064683675765991,
      "learning_rate": 2.7381095725466586e-05,
      "loss": 1.4976,
      "step": 29700
    },
    {
      "epoch": 2.6059915907498246,
      "grad_norm": 0.9136999845504761,
      "learning_rate": 2.7080072245635162e-05,
      "loss": 1.525,
      "step": 29750
    },
    {
      "epoch": 2.6103714085494043,
      "grad_norm": 0.9969817996025085,
      "learning_rate": 2.677904876580373e-05,
      "loss": 1.4659,
      "step": 29800
    },
    {
      "epoch": 2.614751226348984,
      "grad_norm": 0.7980899214744568,
      "learning_rate": 2.6478025285972307e-05,
      "loss": 1.488,
      "step": 29850
    },
    {
      "epoch": 2.6191310441485633,
      "grad_norm": 0.7273786067962646,
      "learning_rate": 2.6177001806140883e-05,
      "loss": 1.4907,
      "step": 29900
    },
    {
      "epoch": 2.623510861948143,
      "grad_norm": 0.7667307257652283,
      "learning_rate": 2.5875978326309452e-05,
      "loss": 1.5424,
      "step": 29950
    },
    {
      "epoch": 2.6278906797477224,
      "grad_norm": 0.8535183072090149,
      "learning_rate": 2.5574954846478028e-05,
      "loss": 1.4624,
      "step": 30000
    },
    {
      "epoch": 2.632270497547302,
      "grad_norm": 0.8695995211601257,
      "learning_rate": 2.5273931366646597e-05,
      "loss": 1.5998,
      "step": 30050
    },
    {
      "epoch": 2.6366503153468814,
      "grad_norm": 1.0942203998565674,
      "learning_rate": 2.4972907886815173e-05,
      "loss": 1.4829,
      "step": 30100
    },
    {
      "epoch": 2.641030133146461,
      "grad_norm": 0.8830613493919373,
      "learning_rate": 2.4671884406983745e-05,
      "loss": 1.5872,
      "step": 30150
    },
    {
      "epoch": 2.6454099509460405,
      "grad_norm": 0.9996356964111328,
      "learning_rate": 2.437086092715232e-05,
      "loss": 1.5289,
      "step": 30200
    },
    {
      "epoch": 2.64978976874562,
      "grad_norm": 0.7489883899688721,
      "learning_rate": 2.4069837447320894e-05,
      "loss": 1.5848,
      "step": 30250
    },
    {
      "epoch": 2.6541695865451995,
      "grad_norm": 1.016396164894104,
      "learning_rate": 2.3768813967489463e-05,
      "loss": 1.5127,
      "step": 30300
    },
    {
      "epoch": 2.6585494043447793,
      "grad_norm": 0.8125839829444885,
      "learning_rate": 2.346779048765804e-05,
      "loss": 1.5009,
      "step": 30350
    },
    {
      "epoch": 2.662929222144359,
      "grad_norm": 0.7866261005401611,
      "learning_rate": 2.316676700782661e-05,
      "loss": 1.4976,
      "step": 30400
    },
    {
      "epoch": 2.6673090399439383,
      "grad_norm": 0.895408034324646,
      "learning_rate": 2.2865743527995184e-05,
      "loss": 1.5327,
      "step": 30450
    },
    {
      "epoch": 2.6716888577435176,
      "grad_norm": 0.9947616457939148,
      "learning_rate": 2.2564720048163756e-05,
      "loss": 1.5105,
      "step": 30500
    },
    {
      "epoch": 2.6760686755430974,
      "grad_norm": 0.6692536473274231,
      "learning_rate": 2.2263696568332332e-05,
      "loss": 1.4844,
      "step": 30550
    },
    {
      "epoch": 2.680448493342677,
      "grad_norm": 1.0088728666305542,
      "learning_rate": 2.1962673088500904e-05,
      "loss": 1.5822,
      "step": 30600
    },
    {
      "epoch": 2.6848283111422564,
      "grad_norm": 0.9449158906936646,
      "learning_rate": 2.1661649608669477e-05,
      "loss": 1.5588,
      "step": 30650
    },
    {
      "epoch": 2.689208128941836,
      "grad_norm": 0.7517828941345215,
      "learning_rate": 2.1360626128838053e-05,
      "loss": 1.551,
      "step": 30700
    },
    {
      "epoch": 2.6935879467414154,
      "grad_norm": 0.9196260571479797,
      "learning_rate": 2.1059602649006625e-05,
      "loss": 1.5457,
      "step": 30750
    },
    {
      "epoch": 2.697967764540995,
      "grad_norm": 0.9735723733901978,
      "learning_rate": 2.0758579169175198e-05,
      "loss": 1.532,
      "step": 30800
    },
    {
      "epoch": 2.7023475823405745,
      "grad_norm": 0.9304119944572449,
      "learning_rate": 2.045755568934377e-05,
      "loss": 1.4382,
      "step": 30850
    },
    {
      "epoch": 2.7067274001401542,
      "grad_norm": 1.108038306236267,
      "learning_rate": 2.0156532209512343e-05,
      "loss": 1.5241,
      "step": 30900
    },
    {
      "epoch": 2.711107217939734,
      "grad_norm": 0.8824107050895691,
      "learning_rate": 1.9855508729680915e-05,
      "loss": 1.5268,
      "step": 30950
    },
    {
      "epoch": 2.7154870357393133,
      "grad_norm": 0.8823485970497131,
      "learning_rate": 1.9554485249849488e-05,
      "loss": 1.4768,
      "step": 31000
    },
    {
      "epoch": 2.7198668535388926,
      "grad_norm": 0.877219021320343,
      "learning_rate": 1.9253461770018063e-05,
      "loss": 1.6183,
      "step": 31050
    },
    {
      "epoch": 2.7242466713384723,
      "grad_norm": 0.8455535769462585,
      "learning_rate": 1.8952438290186636e-05,
      "loss": 1.4962,
      "step": 31100
    },
    {
      "epoch": 2.728626489138052,
      "grad_norm": 0.8332453966140747,
      "learning_rate": 1.865141481035521e-05,
      "loss": 1.4679,
      "step": 31150
    },
    {
      "epoch": 2.7330063069376314,
      "grad_norm": 0.9950475692749023,
      "learning_rate": 1.835039133052378e-05,
      "loss": 1.5477,
      "step": 31200
    },
    {
      "epoch": 2.7373861247372107,
      "grad_norm": 0.8283076286315918,
      "learning_rate": 1.8049367850692357e-05,
      "loss": 1.4975,
      "step": 31250
    },
    {
      "epoch": 2.7417659425367904,
      "grad_norm": 0.9993900060653687,
      "learning_rate": 1.774834437086093e-05,
      "loss": 1.5164,
      "step": 31300
    },
    {
      "epoch": 2.74614576033637,
      "grad_norm": 0.7961745262145996,
      "learning_rate": 1.74473208910295e-05,
      "loss": 1.5564,
      "step": 31350
    },
    {
      "epoch": 2.7505255781359494,
      "grad_norm": 0.8122552633285522,
      "learning_rate": 1.7146297411198074e-05,
      "loss": 1.528,
      "step": 31400
    },
    {
      "epoch": 2.754905395935529,
      "grad_norm": 0.822014570236206,
      "learning_rate": 1.6845273931366647e-05,
      "loss": 1.5441,
      "step": 31450
    },
    {
      "epoch": 2.7592852137351085,
      "grad_norm": 0.7021247744560242,
      "learning_rate": 1.654425045153522e-05,
      "loss": 1.5448,
      "step": 31500
    },
    {
      "epoch": 2.7636650315346882,
      "grad_norm": 0.9469395875930786,
      "learning_rate": 1.624322697170379e-05,
      "loss": 1.5394,
      "step": 31550
    },
    {
      "epoch": 2.7680448493342675,
      "grad_norm": 0.7110187411308289,
      "learning_rate": 1.5942203491872367e-05,
      "loss": 1.4805,
      "step": 31600
    },
    {
      "epoch": 2.7724246671338473,
      "grad_norm": 0.7249006032943726,
      "learning_rate": 1.564118001204094e-05,
      "loss": 1.5117,
      "step": 31650
    },
    {
      "epoch": 2.776804484933427,
      "grad_norm": 0.8113796710968018,
      "learning_rate": 1.5340156532209512e-05,
      "loss": 1.512,
      "step": 31700
    },
    {
      "epoch": 2.7811843027330063,
      "grad_norm": 0.6442748308181763,
      "learning_rate": 1.5039133052378085e-05,
      "loss": 1.5678,
      "step": 31750
    },
    {
      "epoch": 2.7855641205325856,
      "grad_norm": 0.9174409508705139,
      "learning_rate": 1.473810957254666e-05,
      "loss": 1.4985,
      "step": 31800
    },
    {
      "epoch": 2.7899439383321654,
      "grad_norm": 0.7587726712226868,
      "learning_rate": 1.4437086092715233e-05,
      "loss": 1.532,
      "step": 31850
    },
    {
      "epoch": 2.794323756131745,
      "grad_norm": 0.8877901434898376,
      "learning_rate": 1.4136062612883806e-05,
      "loss": 1.4862,
      "step": 31900
    },
    {
      "epoch": 2.7987035739313244,
      "grad_norm": 0.9808672070503235,
      "learning_rate": 1.383503913305238e-05,
      "loss": 1.5717,
      "step": 31950
    },
    {
      "epoch": 2.803083391730904,
      "grad_norm": 0.9753283262252808,
      "learning_rate": 1.3534015653220952e-05,
      "loss": 1.5186,
      "step": 32000
    },
    {
      "epoch": 2.8074632095304835,
      "grad_norm": 0.8343231081962585,
      "learning_rate": 1.3232992173389525e-05,
      "loss": 1.5743,
      "step": 32050
    },
    {
      "epoch": 2.811843027330063,
      "grad_norm": 1.1490893363952637,
      "learning_rate": 1.2931968693558097e-05,
      "loss": 1.4912,
      "step": 32100
    },
    {
      "epoch": 2.8162228451296425,
      "grad_norm": 0.9827654361724854,
      "learning_rate": 1.2630945213726671e-05,
      "loss": 1.5677,
      "step": 32150
    },
    {
      "epoch": 2.8206026629292222,
      "grad_norm": 0.752692461013794,
      "learning_rate": 1.2329921733895244e-05,
      "loss": 1.4599,
      "step": 32200
    },
    {
      "epoch": 2.8249824807288015,
      "grad_norm": 0.8205243349075317,
      "learning_rate": 1.2028898254063818e-05,
      "loss": 1.5691,
      "step": 32250
    },
    {
      "epoch": 2.8293622985283813,
      "grad_norm": 0.9030959010124207,
      "learning_rate": 1.172787477423239e-05,
      "loss": 1.6001,
      "step": 32300
    },
    {
      "epoch": 2.8337421163279606,
      "grad_norm": 0.7431808114051819,
      "learning_rate": 1.1426851294400965e-05,
      "loss": 1.5244,
      "step": 32350
    },
    {
      "epoch": 2.8381219341275403,
      "grad_norm": 0.799920916557312,
      "learning_rate": 1.1125827814569537e-05,
      "loss": 1.5293,
      "step": 32400
    },
    {
      "epoch": 2.84250175192712,
      "grad_norm": 0.7988287210464478,
      "learning_rate": 1.082480433473811e-05,
      "loss": 1.5568,
      "step": 32450
    },
    {
      "epoch": 2.8468815697266994,
      "grad_norm": 0.9720121026039124,
      "learning_rate": 1.0523780854906682e-05,
      "loss": 1.5828,
      "step": 32500
    },
    {
      "epoch": 2.8512613875262787,
      "grad_norm": 0.8332740664482117,
      "learning_rate": 1.0222757375075256e-05,
      "loss": 1.5103,
      "step": 32550
    },
    {
      "epoch": 2.8556412053258584,
      "grad_norm": 0.8462110161781311,
      "learning_rate": 9.92173389524383e-06,
      "loss": 1.6078,
      "step": 32600
    },
    {
      "epoch": 2.860021023125438,
      "grad_norm": 0.92964106798172,
      "learning_rate": 9.620710415412403e-06,
      "loss": 1.5296,
      "step": 32650
    },
    {
      "epoch": 2.8644008409250175,
      "grad_norm": 0.791251540184021,
      "learning_rate": 9.319686935580977e-06,
      "loss": 1.4978,
      "step": 32700
    },
    {
      "epoch": 2.868780658724597,
      "grad_norm": 0.870695173740387,
      "learning_rate": 9.01866345574955e-06,
      "loss": 1.5191,
      "step": 32750
    },
    {
      "epoch": 2.8731604765241765,
      "grad_norm": 0.8854502439498901,
      "learning_rate": 8.717639975918122e-06,
      "loss": 1.5196,
      "step": 32800
    },
    {
      "epoch": 2.8775402943237562,
      "grad_norm": 0.8851457834243774,
      "learning_rate": 8.416616496086694e-06,
      "loss": 1.5408,
      "step": 32850
    },
    {
      "epoch": 2.8819201121233355,
      "grad_norm": 0.9248100519180298,
      "learning_rate": 8.115593016255269e-06,
      "loss": 1.544,
      "step": 32900
    },
    {
      "epoch": 2.8862999299229153,
      "grad_norm": 0.6879705190658569,
      "learning_rate": 7.814569536423841e-06,
      "loss": 1.5203,
      "step": 32950
    },
    {
      "epoch": 2.8906797477224946,
      "grad_norm": 1.1539993286132812,
      "learning_rate": 7.513546056592414e-06,
      "loss": 1.6188,
      "step": 33000
    },
    {
      "epoch": 2.8950595655220743,
      "grad_norm": 1.0641118288040161,
      "learning_rate": 7.2125225767609885e-06,
      "loss": 1.5924,
      "step": 33050
    },
    {
      "epoch": 2.8994393833216536,
      "grad_norm": 0.9418721199035645,
      "learning_rate": 6.911499096929561e-06,
      "loss": 1.5248,
      "step": 33100
    },
    {
      "epoch": 2.9038192011212334,
      "grad_norm": 0.6873894333839417,
      "learning_rate": 6.610475617098134e-06,
      "loss": 1.5413,
      "step": 33150
    },
    {
      "epoch": 2.908199018920813,
      "grad_norm": 1.1307708024978638,
      "learning_rate": 6.309452137266707e-06,
      "loss": 1.5138,
      "step": 33200
    },
    {
      "epoch": 2.9125788367203924,
      "grad_norm": 0.9447811245918274,
      "learning_rate": 6.00842865743528e-06,
      "loss": 1.4919,
      "step": 33250
    },
    {
      "epoch": 2.9169586545199717,
      "grad_norm": 0.796899676322937,
      "learning_rate": 5.707405177603853e-06,
      "loss": 1.5442,
      "step": 33300
    },
    {
      "epoch": 2.9213384723195515,
      "grad_norm": 1.0009230375289917,
      "learning_rate": 5.406381697772427e-06,
      "loss": 1.4947,
      "step": 33350
    },
    {
      "epoch": 2.925718290119131,
      "grad_norm": 0.9836390018463135,
      "learning_rate": 5.105358217941e-06,
      "loss": 1.5569,
      "step": 33400
    },
    {
      "epoch": 2.9300981079187105,
      "grad_norm": 0.8325369954109192,
      "learning_rate": 4.8043347381095725e-06,
      "loss": 1.5611,
      "step": 33450
    },
    {
      "epoch": 2.9344779257182902,
      "grad_norm": 1.0863940715789795,
      "learning_rate": 4.503311258278146e-06,
      "loss": 1.5492,
      "step": 33500
    },
    {
      "epoch": 2.9388577435178695,
      "grad_norm": 1.0628741979599,
      "learning_rate": 4.202287778446719e-06,
      "loss": 1.5024,
      "step": 33550
    },
    {
      "epoch": 2.9432375613174493,
      "grad_norm": 0.8859097957611084,
      "learning_rate": 3.9012642986152925e-06,
      "loss": 1.5524,
      "step": 33600
    },
    {
      "epoch": 2.9476173791170286,
      "grad_norm": 0.9259510636329651,
      "learning_rate": 3.6002408187838654e-06,
      "loss": 1.4995,
      "step": 33650
    },
    {
      "epoch": 2.9519971969166083,
      "grad_norm": 0.827971339225769,
      "learning_rate": 3.2992173389524387e-06,
      "loss": 1.5107,
      "step": 33700
    },
    {
      "epoch": 2.956377014716188,
      "grad_norm": 0.7811101675033569,
      "learning_rate": 2.9981938591210116e-06,
      "loss": 1.5626,
      "step": 33750
    },
    {
      "epoch": 2.9607568325157674,
      "grad_norm": 0.790309488773346,
      "learning_rate": 2.697170379289585e-06,
      "loss": 1.5152,
      "step": 33800
    },
    {
      "epoch": 2.9651366503153467,
      "grad_norm": 0.9010661244392395,
      "learning_rate": 2.396146899458158e-06,
      "loss": 1.49,
      "step": 33850
    },
    {
      "epoch": 2.9695164681149264,
      "grad_norm": 0.8239333629608154,
      "learning_rate": 2.095123419626731e-06,
      "loss": 1.5124,
      "step": 33900
    },
    {
      "epoch": 2.973896285914506,
      "grad_norm": 0.767280638217926,
      "learning_rate": 1.7940999397953042e-06,
      "loss": 1.5857,
      "step": 33950
    },
    {
      "epoch": 2.9782761037140855,
      "grad_norm": 1.055349588394165,
      "learning_rate": 1.4930764599638773e-06,
      "loss": 1.5484,
      "step": 34000
    },
    {
      "epoch": 2.9826559215136648,
      "grad_norm": 1.0495212078094482,
      "learning_rate": 1.1920529801324504e-06,
      "loss": 1.4805,
      "step": 34050
    },
    {
      "epoch": 2.9870357393132445,
      "grad_norm": 0.9639238119125366,
      "learning_rate": 8.910295003010235e-07,
      "loss": 1.5885,
      "step": 34100
    },
    {
      "epoch": 2.9914155571128243,
      "grad_norm": 0.804885983467102,
      "learning_rate": 5.900060204695966e-07,
      "loss": 1.5625,
      "step": 34150
    },
    {
      "epoch": 2.9957953749124036,
      "grad_norm": 0.7779174447059631,
      "learning_rate": 2.889825406381698e-07,
      "loss": 1.5403,
      "step": 34200
    }
  ],
  "logging_steps": 50,
  "max_steps": 34248,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 30,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.708237976216961e+18,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
